{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load_data\n",
    "#explore_data\n",
    "#vectorize_data\n",
    "#build_model\n",
    "#Tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection,preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = open('/home/ankush/Github/Machine Learning/Text Classification/AnalyticsVidya/data/corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'],test_size=0.1)\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Module to explore data.\n",
    "Contains functions to help study, visualize and understand datasets.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_classes(labels):\n",
    "    \"\"\"Gets the total number of classes.\n",
    "    # Arguments\n",
    "        labels: list, label values.\n",
    "            There should be at lease one sample for values in the\n",
    "            range (0, num_classes -1)\n",
    "    # Returns\n",
    "        int, total number of classes.\n",
    "    # Raises\n",
    "        ValueError: if any label value in the range(0, num_classes - 1)\n",
    "            is missing or if number of classes is <= 1.\n",
    "    \"\"\"\n",
    "    num_classes = max(labels) + 1\n",
    "    missing_classes = [i for i in range(num_classes) if i not in labels]\n",
    "    if len(missing_classes):\n",
    "        raise ValueError('Missing samples with label value(s) '\n",
    "                         '{missing_classes}. Please make sure you have '\n",
    "                         'at least one sample for every label value '\n",
    "                         'in the range(0, {max_class})'.format(\n",
    "                            missing_classes=missing_classes,\n",
    "                            max_class=num_classes - 1))\n",
    "\n",
    "    if num_classes <= 1:\n",
    "        raise ValueError('Invalid number of labels: {num_classes}.'\n",
    "                         'Please make sure there are at least two classes '\n",
    "                         'of samples'.format(num_classes=num_classes))\n",
    "    return num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_words_per_sample(sample_texts):\n",
    "    \"\"\"Gets the median number of words per sample given corpus.\n",
    "    # Arguments\n",
    "        sample_texts: list, sample texts.\n",
    "    # Returns\n",
    "        int, median number of words per sample.\n",
    "    \"\"\"\n",
    "    num_words = [len(s.split()) for s in sample_texts]\n",
    "    return np.median(num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_frequency_distribution_of_ngrams(sample_texts,\n",
    "                                          ngram_range=(1, 2),\n",
    "                                          num_ngrams=50):\n",
    "    \"\"\"Plots the frequency distribution of n-grams.\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n",
    "            Min and mplt are the lower and upper bound values for the range.\n",
    "        num_ngrams: int, number of n-grams to plot.\n",
    "            Top `num_ngrams` frequent n-grams will be plotted.\n",
    "    \"\"\"\n",
    "    # Create args required for vectorizing.\n",
    "    kwargs = {\n",
    "            'ngram_range': (1, 1),\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "    }\n",
    "    vectorizer = CountVectorizer(**kwargs)\n",
    "\n",
    "    # This creates a vocabulary (dict, where keys are n-grams and values are\n",
    "    # idxices). This also converts every text to an array the length of\n",
    "    # vocabulary, where every element idxicates the count of the n-gram\n",
    "    # corresponding at that idxex in vocabulary.\n",
    "    vectorized_texts = vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "    # This is the list of all n-grams in the index order from the vocabulary.\n",
    "    all_ngrams = list(vectorizer.get_feature_names())\n",
    "    num_ngrams = min(num_ngrams, len(all_ngrams))\n",
    "    # ngrams = all_ngrams[:num_ngrams]\n",
    "\n",
    "    # Add up the counts per n-gram ie. column-wise\n",
    "    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n",
    "\n",
    "    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n",
    "    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n",
    "        zip(all_counts, all_ngrams), reverse=True)])\n",
    "    ngrams = list(all_ngrams)[:num_ngrams]\n",
    "    counts = list(all_counts)[:num_ngrams]\n",
    "\n",
    "    idx = np.arange(num_ngrams)\n",
    "    plt.bar(idx, counts, width=0.8, color='b')\n",
    "    plt.xlabel('N-grams')\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.title('Frequency distribution of n-grams')\n",
    "    plt.xticks(idx, ngrams, rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_sample_length_distribution(sample_texts):\n",
    "    \"\"\"Plots the sample length distribution.\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "    \"\"\"\n",
    "    plt.hist([len(s) for s in sample_texts], 50)\n",
    "    plt.xlabel('Length of a sample')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Sample length distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_class_distribution(labels):\n",
    "    \"\"\"Plots the class distribution.\n",
    "    # Arguments\n",
    "        labels: list, label values.\n",
    "            There should be at lease one sample for values in the\n",
    "            range (0, num_classes -1)\n",
    "    \"\"\"\n",
    "    num_classes = get_num_classes(labels)\n",
    "    count_map = Counter(labels)\n",
    "    counts = [count_map[i] for i in range(num_classes)]\n",
    "    idx = np.arange(num_classes)\n",
    "    plt.bar(idx, counts, width=0.8, color='b')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Class distribution')\n",
    "    plt.xticks(idx, idx)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_classes(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_words_per_sample(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEpCAYAAABbU781AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVIz/h9knmGyhfJfjgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c75469cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frequency_distribution_of_ngrams(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkBOEmZkVcoIwM7NC/x+ZyMtpkh+g/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c75e62cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample_length_distribution(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c752fdda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_class_distribution(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Module to vectorize data.\n",
    "Converts the given training and validation texts into numerical tensors.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorization parameters\n",
    "\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as ngram vectors.\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of uni-grams + bi-grams.\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train)\n",
    "    x_val = selector.transform(x_val)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Module to create model.\n",
    "Helper functions to create a multi-layer perceptron model and a separable CNN\n",
    "model. These functions take the model hyper-parameters as input. This will\n",
    "allow us to create model instances with slightly varying architectures.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import SeparableConv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sepcnn_model(blocks,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 embedding_dim,\n",
    "                 dropout_rate,\n",
    "                 pool_size,\n",
    "                 input_shape,\n",
    "                 num_classes,\n",
    "                 num_features,\n",
    "                 use_pretrained_embedding=False,\n",
    "                 is_embedding_trainable=False,\n",
    "                 embedding_matrix=None):\n",
    "    \"\"\"Creates an instance of a separable CNN model.\n",
    "    # Arguments\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of the layers.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "        num_features: int, number of words (embedding input dimension).\n",
    "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
    "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
    "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
    "    # Returns\n",
    "        A sepCNN model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    if use_pretrained_embedding:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0]))\n",
    "\n",
    "    for _ in range(blocks-1):\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Module to train n-gram model.\n",
    "Vectorizes training and validation texts into n-grams and uses that for\n",
    "training a n-gram model - a simple multi-layer perceptron model. We use n-gram\n",
    "model for text classification when the ratio of number of samples to number of\n",
    "words per sample for the given dataset is very small (<~1500).\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#import load_data\n",
    "#import explore_data\n",
    "#import vectorize_data\n",
    "#import build_model\n",
    "\n",
    "FLAGS = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_ngram_model(data,\n",
    "                      learning_rate=1e-3,\n",
    "                      epochs=1000,\n",
    "                      batch_size=128,\n",
    "                      layers=2,\n",
    "                      units=64,\n",
    "                      dropout_rate=0.2):\n",
    "    \"\"\"Trains n-gram model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of Dense layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val = ngram_vectorize(\n",
    "        train_texts, train_labels, val_texts)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = mlp_model(layers=layers,\n",
    "                                  units=units,\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  input_shape=x_train.shape[1:],\n",
    "                                  num_classes=num_classes)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('imdb_mlp_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ((train_x, train_y), (valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 7s - loss: 0.6204 - acc: 0.8142 - val_loss: 0.5268 - val_acc: 0.8730\n",
      "Epoch 2/1000\n",
      " - 5s - loss: 0.4058 - acc: 0.9252 - val_loss: 0.3745 - val_acc: 0.8860\n",
      "Epoch 3/1000\n",
      " - 5s - loss: 0.2593 - acc: 0.9433 - val_loss: 0.3039 - val_acc: 0.8930\n",
      "Epoch 4/1000\n",
      " - 5s - loss: 0.1847 - acc: 0.9580 - val_loss: 0.2732 - val_acc: 0.8950\n",
      "Epoch 5/1000\n",
      " - 6s - loss: 0.1401 - acc: 0.9664 - val_loss: 0.2575 - val_acc: 0.8970\n",
      "Epoch 6/1000\n",
      " - 5s - loss: 0.1116 - acc: 0.9743 - val_loss: 0.2504 - val_acc: 0.8990\n",
      "Epoch 7/1000\n",
      " - 5s - loss: 0.0909 - acc: 0.9819 - val_loss: 0.2469 - val_acc: 0.9030\n",
      "Epoch 8/1000\n",
      " - 6s - loss: 0.0749 - acc: 0.9850 - val_loss: 0.2458 - val_acc: 0.9060\n",
      "Epoch 9/1000\n",
      " - 5s - loss: 0.0641 - acc: 0.9870 - val_loss: 0.2468 - val_acc: 0.9060\n",
      "Epoch 10/1000\n",
      " - 5s - loss: 0.0553 - acc: 0.9892 - val_loss: 0.2486 - val_acc: 0.9050\n",
      "Validation accuracy: 0.9049999990463257, loss: 0.24855739521980286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9049999990463257, 0.24855739521980286)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ngram_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Module to demonstrate hyper-parameter tuning.\n",
    "Trains n-gram model with different combination of hyper-parameters and finds\n",
    "the one that works best.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "#import load_data\n",
    "#import train_ngram_model\n",
    "\n",
    "FLAGS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tune_ngram_model(data):\n",
    "    \"\"\"Tunes n-gram model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "    \"\"\"\n",
    "    # Select parameter values to try.\n",
    "    num_layers = [1, 2, 3]\n",
    "    num_units = [8, 16, 32, 64, 128]\n",
    "\n",
    "    # Save parameter combination and results.\n",
    "    params = {\n",
    "        'layers': [],\n",
    "        'units': [],\n",
    "        'accuracy': [],\n",
    "    }\n",
    "\n",
    "    # Iterate over all parameter combinations.\n",
    "    for layers in num_layers:\n",
    "        for units in num_units:\n",
    "                params['layers'].append(layers)\n",
    "                params['units'].append(units)\n",
    "\n",
    "                accuracy, _ = train_ngram_model(\n",
    "                    data=data,\n",
    "                    layers=layers,\n",
    "                    units=units)\n",
    "                print(('Accuracy: {accuracy}, Parameters: (layers={layers}, '\n",
    "                       'units={units})').format(accuracy=accuracy,\n",
    "                                                layers=layers,\n",
    "                                                units=units))\n",
    "                params['accuracy'].append(accuracy)\n",
    "    _plot_parameters(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _plot_parameters(params):\n",
    "    \"\"\"Creates a 3D surface plot of given parameters.\n",
    "    # Arguments\n",
    "        params: dict, contains layers, units and accuracy value combinations.\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_trisurf(params['layers'],\n",
    "                    params['units'],\n",
    "                    params['accuracy'],\n",
    "                    cmap=cm.coolwarm,\n",
    "                    antialiased=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 4s - loss: 0.6846 - acc: 0.6874 - val_loss: 0.6761 - val_acc: 0.7920\n",
      "Epoch 2/1000\n",
      " - 3s - loss: 0.6650 - acc: 0.8320 - val_loss: 0.6603 - val_acc: 0.8230\n",
      "Epoch 3/1000\n",
      " - 3s - loss: 0.6464 - acc: 0.8794 - val_loss: 0.6454 - val_acc: 0.8450\n",
      "Epoch 4/1000\n",
      " - 3s - loss: 0.6286 - acc: 0.9026 - val_loss: 0.6312 - val_acc: 0.8520\n",
      "Epoch 5/1000\n",
      " - 3s - loss: 0.6116 - acc: 0.9119 - val_loss: 0.6177 - val_acc: 0.8550\n",
      "Epoch 6/1000\n",
      " - 3s - loss: 0.5957 - acc: 0.9208 - val_loss: 0.6049 - val_acc: 0.8650\n",
      "Epoch 7/1000\n",
      " - 3s - loss: 0.5803 - acc: 0.9210 - val_loss: 0.5928 - val_acc: 0.8660\n",
      "Epoch 8/1000\n",
      " - 3s - loss: 0.5657 - acc: 0.9247 - val_loss: 0.5813 - val_acc: 0.8690\n",
      "Epoch 9/1000\n",
      " - 3s - loss: 0.5514 - acc: 0.9292 - val_loss: 0.5702 - val_acc: 0.8690\n",
      "Epoch 10/1000\n",
      " - 3s - loss: 0.5379 - acc: 0.9293 - val_loss: 0.5597 - val_acc: 0.8700\n",
      "Epoch 11/1000\n",
      " - 3s - loss: 0.5254 - acc: 0.9318 - val_loss: 0.5497 - val_acc: 0.8700\n",
      "Epoch 12/1000\n",
      " - 3s - loss: 0.5137 - acc: 0.9321 - val_loss: 0.5403 - val_acc: 0.8690\n",
      "Epoch 13/1000\n",
      " - 3s - loss: 0.5021 - acc: 0.9327 - val_loss: 0.5311 - val_acc: 0.8720\n",
      "Epoch 14/1000\n",
      " - 3s - loss: 0.4903 - acc: 0.9332 - val_loss: 0.5224 - val_acc: 0.8730\n",
      "Epoch 15/1000\n",
      " - 3s - loss: 0.4797 - acc: 0.9363 - val_loss: 0.5141 - val_acc: 0.8740\n",
      "Epoch 16/1000\n",
      " - 3s - loss: 0.4689 - acc: 0.9372 - val_loss: 0.5062 - val_acc: 0.8730\n",
      "Epoch 17/1000\n",
      " - 3s - loss: 0.4597 - acc: 0.9376 - val_loss: 0.4986 - val_acc: 0.8730\n",
      "Epoch 18/1000\n",
      " - 3s - loss: 0.4503 - acc: 0.9390 - val_loss: 0.4913 - val_acc: 0.8750\n",
      "Epoch 19/1000\n",
      " - 3s - loss: 0.4413 - acc: 0.9390 - val_loss: 0.4843 - val_acc: 0.8760\n",
      "Epoch 20/1000\n",
      " - 3s - loss: 0.4320 - acc: 0.9400 - val_loss: 0.4776 - val_acc: 0.8770\n",
      "Epoch 21/1000\n",
      " - 3s - loss: 0.4247 - acc: 0.9399 - val_loss: 0.4712 - val_acc: 0.8780\n",
      "Epoch 22/1000\n",
      " - 3s - loss: 0.4163 - acc: 0.9397 - val_loss: 0.4651 - val_acc: 0.8770\n",
      "Epoch 23/1000\n",
      " - 4s - loss: 0.4087 - acc: 0.9421 - val_loss: 0.4592 - val_acc: 0.8790\n",
      "Epoch 24/1000\n",
      " - 4s - loss: 0.4016 - acc: 0.9432 - val_loss: 0.4535 - val_acc: 0.8810\n",
      "Epoch 25/1000\n",
      " - 3s - loss: 0.3935 - acc: 0.9459 - val_loss: 0.4480 - val_acc: 0.8790\n",
      "Epoch 26/1000\n",
      " - 3s - loss: 0.3870 - acc: 0.9441 - val_loss: 0.4428 - val_acc: 0.8800\n",
      "Epoch 27/1000\n",
      " - 3s - loss: 0.3805 - acc: 0.9444 - val_loss: 0.4378 - val_acc: 0.8800\n",
      "Epoch 28/1000\n",
      " - 3s - loss: 0.3736 - acc: 0.9466 - val_loss: 0.4330 - val_acc: 0.8790\n",
      "Epoch 29/1000\n",
      " - 3s - loss: 0.3677 - acc: 0.9458 - val_loss: 0.4283 - val_acc: 0.8800\n",
      "Epoch 30/1000\n",
      " - 3s - loss: 0.3618 - acc: 0.9464 - val_loss: 0.4237 - val_acc: 0.8810\n",
      "Epoch 31/1000\n",
      " - 3s - loss: 0.3559 - acc: 0.9467 - val_loss: 0.4194 - val_acc: 0.8810\n",
      "Epoch 32/1000\n",
      " - 3s - loss: 0.3490 - acc: 0.9484 - val_loss: 0.4151 - val_acc: 0.8810\n",
      "Epoch 33/1000\n",
      " - 3s - loss: 0.3450 - acc: 0.9492 - val_loss: 0.4111 - val_acc: 0.8810\n",
      "Epoch 34/1000\n",
      " - 3s - loss: 0.3390 - acc: 0.9503 - val_loss: 0.4071 - val_acc: 0.8810\n",
      "Epoch 35/1000\n",
      " - 3s - loss: 0.3335 - acc: 0.9508 - val_loss: 0.4033 - val_acc: 0.8820\n",
      "Epoch 36/1000\n",
      " - 3s - loss: 0.3286 - acc: 0.9534 - val_loss: 0.3995 - val_acc: 0.8830\n",
      "Epoch 37/1000\n",
      " - 3s - loss: 0.3228 - acc: 0.9523 - val_loss: 0.3959 - val_acc: 0.8820\n",
      "Epoch 38/1000\n",
      " - 3s - loss: 0.3194 - acc: 0.9524 - val_loss: 0.3925 - val_acc: 0.8830\n",
      "Epoch 39/1000\n",
      " - 3s - loss: 0.3142 - acc: 0.9519 - val_loss: 0.3891 - val_acc: 0.8830\n",
      "Epoch 40/1000\n",
      " - 3s - loss: 0.3110 - acc: 0.9529 - val_loss: 0.3859 - val_acc: 0.8840\n",
      "Epoch 41/1000\n",
      " - 3s - loss: 0.3045 - acc: 0.9538 - val_loss: 0.3827 - val_acc: 0.8830\n",
      "Epoch 42/1000\n",
      " - 3s - loss: 0.3019 - acc: 0.9532 - val_loss: 0.3796 - val_acc: 0.8840\n",
      "Epoch 43/1000\n",
      " - 3s - loss: 0.2967 - acc: 0.9547 - val_loss: 0.3766 - val_acc: 0.8840\n",
      "Epoch 44/1000\n",
      " - 3s - loss: 0.2944 - acc: 0.9548 - val_loss: 0.3737 - val_acc: 0.8830\n",
      "Epoch 45/1000\n",
      " - 3s - loss: 0.2897 - acc: 0.9571 - val_loss: 0.3709 - val_acc: 0.8830\n",
      "Epoch 46/1000\n",
      " - 3s - loss: 0.2860 - acc: 0.9540 - val_loss: 0.3681 - val_acc: 0.8810\n",
      "Epoch 47/1000\n",
      " - 3s - loss: 0.2820 - acc: 0.9574 - val_loss: 0.3655 - val_acc: 0.8810\n",
      "Epoch 48/1000\n",
      " - 3s - loss: 0.2782 - acc: 0.9562 - val_loss: 0.3629 - val_acc: 0.8810\n",
      "Epoch 49/1000\n",
      " - 3s - loss: 0.2745 - acc: 0.9591 - val_loss: 0.3604 - val_acc: 0.8800\n",
      "Epoch 50/1000\n",
      " - 3s - loss: 0.2710 - acc: 0.9591 - val_loss: 0.3579 - val_acc: 0.8810\n",
      "Epoch 51/1000\n",
      " - 3s - loss: 0.2671 - acc: 0.9603 - val_loss: 0.3556 - val_acc: 0.8810\n",
      "Epoch 52/1000\n",
      " - 3s - loss: 0.2644 - acc: 0.9591 - val_loss: 0.3532 - val_acc: 0.8830\n",
      "Epoch 53/1000\n",
      " - 3s - loss: 0.2601 - acc: 0.9624 - val_loss: 0.3509 - val_acc: 0.8840\n",
      "Epoch 54/1000\n",
      " - 3s - loss: 0.2583 - acc: 0.9591 - val_loss: 0.3488 - val_acc: 0.8870\n",
      "Epoch 55/1000\n",
      " - 3s - loss: 0.2552 - acc: 0.9610 - val_loss: 0.3466 - val_acc: 0.8870\n",
      "Epoch 56/1000\n",
      " - 3s - loss: 0.2524 - acc: 0.9597 - val_loss: 0.3446 - val_acc: 0.8880\n",
      "Epoch 57/1000\n",
      " - 3s - loss: 0.2488 - acc: 0.9618 - val_loss: 0.3425 - val_acc: 0.8900\n",
      "Epoch 58/1000\n",
      " - 3s - loss: 0.2462 - acc: 0.9592 - val_loss: 0.3405 - val_acc: 0.8900\n",
      "Epoch 59/1000\n",
      " - 3s - loss: 0.2427 - acc: 0.9606 - val_loss: 0.3386 - val_acc: 0.8910\n",
      "Epoch 60/1000\n",
      " - 3s - loss: 0.2402 - acc: 0.9631 - val_loss: 0.3367 - val_acc: 0.8910\n",
      "Epoch 61/1000\n",
      " - 3s - loss: 0.2371 - acc: 0.9613 - val_loss: 0.3348 - val_acc: 0.8910\n",
      "Epoch 62/1000\n",
      " - 3s - loss: 0.2350 - acc: 0.9630 - val_loss: 0.3330 - val_acc: 0.8920\n",
      "Epoch 63/1000\n",
      " - 3s - loss: 0.2323 - acc: 0.9631 - val_loss: 0.3312 - val_acc: 0.8920\n",
      "Epoch 64/1000\n",
      " - 3s - loss: 0.2300 - acc: 0.9624 - val_loss: 0.3295 - val_acc: 0.8910\n",
      "Epoch 65/1000\n",
      " - 3s - loss: 0.2260 - acc: 0.9649 - val_loss: 0.3278 - val_acc: 0.8910\n",
      "Epoch 66/1000\n",
      " - 3s - loss: 0.2239 - acc: 0.9630 - val_loss: 0.3262 - val_acc: 0.8920\n",
      "Epoch 67/1000\n",
      " - 3s - loss: 0.2223 - acc: 0.9633 - val_loss: 0.3246 - val_acc: 0.8940\n",
      "Epoch 68/1000\n",
      " - 3s - loss: 0.2199 - acc: 0.9634 - val_loss: 0.3230 - val_acc: 0.8940\n",
      "Epoch 69/1000\n",
      " - 3s - loss: 0.2172 - acc: 0.9660 - val_loss: 0.3214 - val_acc: 0.8920\n",
      "Epoch 70/1000\n",
      " - 3s - loss: 0.2148 - acc: 0.9651 - val_loss: 0.3200 - val_acc: 0.8930\n",
      "Epoch 71/1000\n",
      " - 3s - loss: 0.2130 - acc: 0.9646 - val_loss: 0.3185 - val_acc: 0.8940\n",
      "Epoch 72/1000\n",
      " - 3s - loss: 0.2105 - acc: 0.9672 - val_loss: 0.3170 - val_acc: 0.8940\n",
      "Epoch 73/1000\n",
      " - 3s - loss: 0.2102 - acc: 0.9653 - val_loss: 0.3156 - val_acc: 0.8940\n",
      "Epoch 74/1000\n",
      " - 3s - loss: 0.2067 - acc: 0.9670 - val_loss: 0.3142 - val_acc: 0.8940\n",
      "Epoch 75/1000\n",
      " - 3s - loss: 0.2051 - acc: 0.9676 - val_loss: 0.3130 - val_acc: 0.8940\n",
      "Epoch 76/1000\n",
      " - 3s - loss: 0.2011 - acc: 0.9690 - val_loss: 0.3117 - val_acc: 0.8950\n",
      "Epoch 77/1000\n",
      " - 3s - loss: 0.2006 - acc: 0.9668 - val_loss: 0.3104 - val_acc: 0.8940\n",
      "Epoch 78/1000\n",
      " - 3s - loss: 0.1988 - acc: 0.9657 - val_loss: 0.3092 - val_acc: 0.8940\n",
      "Epoch 79/1000\n",
      " - 3s - loss: 0.1975 - acc: 0.9650 - val_loss: 0.3079 - val_acc: 0.8940\n",
      "Epoch 80/1000\n",
      " - 3s - loss: 0.1938 - acc: 0.9691 - val_loss: 0.3067 - val_acc: 0.8940\n",
      "Epoch 81/1000\n",
      " - 3s - loss: 0.1927 - acc: 0.9681 - val_loss: 0.3055 - val_acc: 0.8950\n",
      "Epoch 82/1000\n",
      " - 3s - loss: 0.1914 - acc: 0.9692 - val_loss: 0.3044 - val_acc: 0.8950\n",
      "Epoch 83/1000\n",
      " - 3s - loss: 0.1895 - acc: 0.9694 - val_loss: 0.3032 - val_acc: 0.8950\n",
      "Epoch 84/1000\n",
      " - 3s - loss: 0.1887 - acc: 0.9701 - val_loss: 0.3021 - val_acc: 0.8960\n",
      "Epoch 85/1000\n",
      " - 3s - loss: 0.1857 - acc: 0.9699 - val_loss: 0.3010 - val_acc: 0.8950\n",
      "Epoch 86/1000\n",
      " - 3s - loss: 0.1836 - acc: 0.9707 - val_loss: 0.2999 - val_acc: 0.8950\n",
      "Epoch 87/1000\n",
      " - 3s - loss: 0.1818 - acc: 0.9704 - val_loss: 0.2988 - val_acc: 0.8960\n",
      "Epoch 88/1000\n",
      " - 3s - loss: 0.1803 - acc: 0.9694 - val_loss: 0.2978 - val_acc: 0.8950\n",
      "Epoch 89/1000\n",
      " - 3s - loss: 0.1799 - acc: 0.9698 - val_loss: 0.2968 - val_acc: 0.8950\n",
      "Epoch 90/1000\n",
      " - 3s - loss: 0.1773 - acc: 0.9716 - val_loss: 0.2959 - val_acc: 0.8950\n",
      "Epoch 91/1000\n",
      " - 3s - loss: 0.1760 - acc: 0.9702 - val_loss: 0.2949 - val_acc: 0.8950\n",
      "Epoch 92/1000\n",
      " - 3s - loss: 0.1743 - acc: 0.9727 - val_loss: 0.2940 - val_acc: 0.8970\n",
      "Epoch 93/1000\n",
      " - 3s - loss: 0.1716 - acc: 0.9710 - val_loss: 0.2930 - val_acc: 0.8970\n",
      "Epoch 94/1000\n",
      " - 3s - loss: 0.1717 - acc: 0.9714 - val_loss: 0.2921 - val_acc: 0.8960\n",
      "Epoch 95/1000\n",
      " - 3s - loss: 0.1693 - acc: 0.9721 - val_loss: 0.2912 - val_acc: 0.8970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/1000\n",
      " - 3s - loss: 0.1693 - acc: 0.9722 - val_loss: 0.2904 - val_acc: 0.8960\n",
      "Epoch 97/1000\n",
      " - 3s - loss: 0.1668 - acc: 0.9737 - val_loss: 0.2895 - val_acc: 0.8970\n",
      "Epoch 98/1000\n",
      " - 3s - loss: 0.1654 - acc: 0.9722 - val_loss: 0.2887 - val_acc: 0.8970\n",
      "Epoch 99/1000\n",
      " - 3s - loss: 0.1633 - acc: 0.9731 - val_loss: 0.2879 - val_acc: 0.8960\n",
      "Epoch 100/1000\n",
      " - 3s - loss: 0.1623 - acc: 0.9753 - val_loss: 0.2871 - val_acc: 0.8960\n",
      "Epoch 101/1000\n",
      " - 3s - loss: 0.1609 - acc: 0.9736 - val_loss: 0.2863 - val_acc: 0.8960\n",
      "Epoch 102/1000\n",
      " - 3s - loss: 0.1605 - acc: 0.9730 - val_loss: 0.2855 - val_acc: 0.8960\n",
      "Epoch 103/1000\n",
      " - 3s - loss: 0.1588 - acc: 0.9737 - val_loss: 0.2847 - val_acc: 0.8960\n",
      "Epoch 104/1000\n",
      " - 3s - loss: 0.1563 - acc: 0.9747 - val_loss: 0.2840 - val_acc: 0.8960\n",
      "Epoch 105/1000\n",
      " - 3s - loss: 0.1555 - acc: 0.9746 - val_loss: 0.2834 - val_acc: 0.8960\n",
      "Epoch 106/1000\n",
      " - 3s - loss: 0.1536 - acc: 0.9757 - val_loss: 0.2827 - val_acc: 0.8960\n",
      "Epoch 107/1000\n",
      " - 3s - loss: 0.1527 - acc: 0.9748 - val_loss: 0.2819 - val_acc: 0.8960\n",
      "Epoch 108/1000\n",
      " - 3s - loss: 0.1508 - acc: 0.9756 - val_loss: 0.2812 - val_acc: 0.8970\n",
      "Epoch 109/1000\n",
      " - 3s - loss: 0.1521 - acc: 0.9732 - val_loss: 0.2805 - val_acc: 0.8970\n",
      "Epoch 110/1000\n",
      " - 3s - loss: 0.1494 - acc: 0.9750 - val_loss: 0.2799 - val_acc: 0.8970\n",
      "Epoch 111/1000\n",
      " - 3s - loss: 0.1484 - acc: 0.9761 - val_loss: 0.2792 - val_acc: 0.8990\n",
      "Epoch 112/1000\n",
      " - 3s - loss: 0.1467 - acc: 0.9767 - val_loss: 0.2785 - val_acc: 0.8980\n",
      "Epoch 113/1000\n",
      " - 3s - loss: 0.1461 - acc: 0.9771 - val_loss: 0.2779 - val_acc: 0.8990\n",
      "Epoch 114/1000\n",
      " - 3s - loss: 0.1453 - acc: 0.9742 - val_loss: 0.2773 - val_acc: 0.8970\n",
      "Epoch 115/1000\n",
      " - 3s - loss: 0.1449 - acc: 0.9749 - val_loss: 0.2767 - val_acc: 0.8980\n",
      "Epoch 116/1000\n",
      " - 3s - loss: 0.1424 - acc: 0.9772 - val_loss: 0.2761 - val_acc: 0.8960\n",
      "Epoch 117/1000\n",
      " - 3s - loss: 0.1415 - acc: 0.9777 - val_loss: 0.2756 - val_acc: 0.8950\n",
      "Epoch 118/1000\n",
      " - 3s - loss: 0.1415 - acc: 0.9768 - val_loss: 0.2750 - val_acc: 0.8960\n",
      "Epoch 119/1000\n",
      " - 3s - loss: 0.1399 - acc: 0.9779 - val_loss: 0.2745 - val_acc: 0.8960\n",
      "Epoch 120/1000\n",
      " - 3s - loss: 0.1382 - acc: 0.9787 - val_loss: 0.2739 - val_acc: 0.8970\n",
      "Epoch 121/1000\n",
      " - 3s - loss: 0.1378 - acc: 0.9792 - val_loss: 0.2734 - val_acc: 0.8970\n",
      "Epoch 122/1000\n",
      " - 3s - loss: 0.1360 - acc: 0.9771 - val_loss: 0.2729 - val_acc: 0.8960\n",
      "Epoch 123/1000\n",
      " - 3s - loss: 0.1352 - acc: 0.9782 - val_loss: 0.2724 - val_acc: 0.8970\n",
      "Epoch 124/1000\n",
      " - 3s - loss: 0.1339 - acc: 0.9786 - val_loss: 0.2719 - val_acc: 0.8970\n",
      "Epoch 125/1000\n",
      " - 3s - loss: 0.1328 - acc: 0.9776 - val_loss: 0.2714 - val_acc: 0.8970\n",
      "Epoch 126/1000\n",
      " - 3s - loss: 0.1339 - acc: 0.9777 - val_loss: 0.2709 - val_acc: 0.8970\n",
      "Epoch 127/1000\n",
      " - 3s - loss: 0.1311 - acc: 0.9790 - val_loss: 0.2704 - val_acc: 0.8970\n",
      "Epoch 128/1000\n",
      " - 3s - loss: 0.1295 - acc: 0.9804 - val_loss: 0.2699 - val_acc: 0.8970\n",
      "Epoch 129/1000\n",
      " - 3s - loss: 0.1301 - acc: 0.9797 - val_loss: 0.2695 - val_acc: 0.8970\n",
      "Epoch 130/1000\n",
      " - 3s - loss: 0.1285 - acc: 0.9800 - val_loss: 0.2691 - val_acc: 0.8970\n",
      "Epoch 131/1000\n",
      " - 3s - loss: 0.1274 - acc: 0.9797 - val_loss: 0.2686 - val_acc: 0.8970\n",
      "Epoch 132/1000\n",
      " - 3s - loss: 0.1271 - acc: 0.9790 - val_loss: 0.2682 - val_acc: 0.8980\n",
      "Epoch 133/1000\n",
      " - 3s - loss: 0.1270 - acc: 0.9790 - val_loss: 0.2677 - val_acc: 0.8990\n",
      "Epoch 134/1000\n",
      " - 3s - loss: 0.1248 - acc: 0.9798 - val_loss: 0.2673 - val_acc: 0.8980\n",
      "Epoch 135/1000\n",
      " - 3s - loss: 0.1229 - acc: 0.9809 - val_loss: 0.2668 - val_acc: 0.8990\n",
      "Epoch 136/1000\n",
      " - 3s - loss: 0.1220 - acc: 0.9809 - val_loss: 0.2664 - val_acc: 0.8990\n",
      "Epoch 137/1000\n",
      " - 3s - loss: 0.1226 - acc: 0.9809 - val_loss: 0.2660 - val_acc: 0.8990\n",
      "Epoch 138/1000\n",
      " - 3s - loss: 0.1208 - acc: 0.9810 - val_loss: 0.2656 - val_acc: 0.8980\n",
      "Epoch 139/1000\n",
      " - 3s - loss: 0.1197 - acc: 0.9807 - val_loss: 0.2652 - val_acc: 0.8990\n",
      "Epoch 140/1000\n",
      " - 3s - loss: 0.1206 - acc: 0.9793 - val_loss: 0.2649 - val_acc: 0.8970\n",
      "Epoch 141/1000\n",
      " - 3s - loss: 0.1173 - acc: 0.9821 - val_loss: 0.2644 - val_acc: 0.8980\n",
      "Epoch 142/1000\n",
      " - 3s - loss: 0.1181 - acc: 0.9812 - val_loss: 0.2641 - val_acc: 0.8990\n",
      "Epoch 143/1000\n",
      " - 3s - loss: 0.1186 - acc: 0.9806 - val_loss: 0.2637 - val_acc: 0.8980\n",
      "Epoch 144/1000\n",
      " - 3s - loss: 0.1164 - acc: 0.9808 - val_loss: 0.2634 - val_acc: 0.8970\n",
      "Epoch 145/1000\n",
      " - 3s - loss: 0.1154 - acc: 0.9821 - val_loss: 0.2629 - val_acc: 0.8980\n",
      "Epoch 146/1000\n",
      " - 3s - loss: 0.1150 - acc: 0.9799 - val_loss: 0.2626 - val_acc: 0.8980\n",
      "Epoch 147/1000\n",
      " - 3s - loss: 0.1140 - acc: 0.9817 - val_loss: 0.2623 - val_acc: 0.8970\n",
      "Epoch 148/1000\n",
      " - 3s - loss: 0.1135 - acc: 0.9818 - val_loss: 0.2620 - val_acc: 0.8970\n",
      "Epoch 149/1000\n",
      " - 3s - loss: 0.1131 - acc: 0.9823 - val_loss: 0.2617 - val_acc: 0.8970\n",
      "Epoch 150/1000\n",
      " - 3s - loss: 0.1119 - acc: 0.9813 - val_loss: 0.2613 - val_acc: 0.8970\n",
      "Epoch 151/1000\n",
      " - 3s - loss: 0.1114 - acc: 0.9803 - val_loss: 0.2610 - val_acc: 0.8980\n",
      "Epoch 152/1000\n",
      " - 3s - loss: 0.1096 - acc: 0.9824 - val_loss: 0.2607 - val_acc: 0.8970\n",
      "Epoch 153/1000\n",
      " - 3s - loss: 0.1112 - acc: 0.9827 - val_loss: 0.2605 - val_acc: 0.8980\n",
      "Epoch 154/1000\n",
      " - 3s - loss: 0.1109 - acc: 0.9823 - val_loss: 0.2602 - val_acc: 0.8980\n",
      "Epoch 155/1000\n",
      " - 3s - loss: 0.1079 - acc: 0.9839 - val_loss: 0.2599 - val_acc: 0.8980\n",
      "Epoch 156/1000\n",
      " - 3s - loss: 0.1073 - acc: 0.9824 - val_loss: 0.2596 - val_acc: 0.8980\n",
      "Epoch 157/1000\n",
      " - 4s - loss: 0.1068 - acc: 0.9838 - val_loss: 0.2594 - val_acc: 0.8980\n",
      "Epoch 158/1000\n",
      " - 3s - loss: 0.1054 - acc: 0.9839 - val_loss: 0.2591 - val_acc: 0.8980\n",
      "Epoch 159/1000\n",
      " - 3s - loss: 0.1069 - acc: 0.9823 - val_loss: 0.2588 - val_acc: 0.8980\n",
      "Epoch 160/1000\n",
      " - 3s - loss: 0.1049 - acc: 0.9840 - val_loss: 0.2586 - val_acc: 0.9000\n",
      "Epoch 161/1000\n",
      " - 3s - loss: 0.1040 - acc: 0.9840 - val_loss: 0.2583 - val_acc: 0.8990\n",
      "Epoch 162/1000\n",
      " - 3s - loss: 0.1032 - acc: 0.9839 - val_loss: 0.2580 - val_acc: 0.9000\n",
      "Epoch 163/1000\n",
      " - 3s - loss: 0.1029 - acc: 0.9830 - val_loss: 0.2577 - val_acc: 0.9000\n",
      "Epoch 164/1000\n",
      " - 3s - loss: 0.1017 - acc: 0.9842 - val_loss: 0.2575 - val_acc: 0.9000\n",
      "Epoch 165/1000\n",
      " - 3s - loss: 0.1004 - acc: 0.9844 - val_loss: 0.2572 - val_acc: 0.9000\n",
      "Epoch 166/1000\n",
      " - 3s - loss: 0.1010 - acc: 0.9859 - val_loss: 0.2570 - val_acc: 0.9010\n",
      "Epoch 167/1000\n",
      " - 3s - loss: 0.1003 - acc: 0.9850 - val_loss: 0.2568 - val_acc: 0.9000\n",
      "Epoch 168/1000\n",
      " - 3s - loss: 0.0998 - acc: 0.9851 - val_loss: 0.2565 - val_acc: 0.9010\n",
      "Epoch 169/1000\n",
      " - 3s - loss: 0.0996 - acc: 0.9846 - val_loss: 0.2563 - val_acc: 0.9010\n",
      "Epoch 170/1000\n",
      " - 3s - loss: 0.0988 - acc: 0.9836 - val_loss: 0.2561 - val_acc: 0.9010\n",
      "Epoch 171/1000\n",
      " - 3s - loss: 0.0966 - acc: 0.9858 - val_loss: 0.2559 - val_acc: 0.9010\n",
      "Epoch 172/1000\n",
      " - 3s - loss: 0.0963 - acc: 0.9848 - val_loss: 0.2557 - val_acc: 0.9010\n",
      "Epoch 173/1000\n",
      " - 4s - loss: 0.0973 - acc: 0.9832 - val_loss: 0.2555 - val_acc: 0.9010\n",
      "Epoch 174/1000\n",
      " - 3s - loss: 0.0964 - acc: 0.9837 - val_loss: 0.2553 - val_acc: 0.9010\n",
      "Epoch 175/1000\n",
      " - 3s - loss: 0.0949 - acc: 0.9843 - val_loss: 0.2550 - val_acc: 0.9010\n",
      "Epoch 176/1000\n",
      " - 3s - loss: 0.0950 - acc: 0.9852 - val_loss: 0.2549 - val_acc: 0.9010\n",
      "Epoch 177/1000\n",
      " - 3s - loss: 0.0949 - acc: 0.9849 - val_loss: 0.2547 - val_acc: 0.9010\n",
      "Epoch 178/1000\n",
      " - 3s - loss: 0.0931 - acc: 0.9862 - val_loss: 0.2545 - val_acc: 0.9020\n",
      "Epoch 179/1000\n",
      " - 3s - loss: 0.0920 - acc: 0.9854 - val_loss: 0.2544 - val_acc: 0.9010\n",
      "Epoch 180/1000\n",
      " - 3s - loss: 0.0923 - acc: 0.9860 - val_loss: 0.2542 - val_acc: 0.9010\n",
      "Epoch 181/1000\n",
      " - 3s - loss: 0.0936 - acc: 0.9841 - val_loss: 0.2540 - val_acc: 0.9010\n",
      "Epoch 182/1000\n",
      " - 3s - loss: 0.0907 - acc: 0.9862 - val_loss: 0.2539 - val_acc: 0.9010\n",
      "Epoch 183/1000\n",
      " - 4s - loss: 0.0908 - acc: 0.9861 - val_loss: 0.2537 - val_acc: 0.9010\n",
      "Epoch 184/1000\n",
      " - 3s - loss: 0.0901 - acc: 0.9854 - val_loss: 0.2535 - val_acc: 0.9000\n",
      "Epoch 185/1000\n",
      " - 4s - loss: 0.0901 - acc: 0.9860 - val_loss: 0.2534 - val_acc: 0.9010\n",
      "Epoch 186/1000\n",
      " - 3s - loss: 0.0888 - acc: 0.9862 - val_loss: 0.2532 - val_acc: 0.9010\n",
      "Epoch 187/1000\n",
      " - 3s - loss: 0.0893 - acc: 0.9861 - val_loss: 0.2530 - val_acc: 0.9010\n",
      "Epoch 188/1000\n",
      " - 3s - loss: 0.0887 - acc: 0.9861 - val_loss: 0.2529 - val_acc: 0.9000\n",
      "Epoch 189/1000\n",
      " - 3s - loss: 0.0883 - acc: 0.9862 - val_loss: 0.2528 - val_acc: 0.9000\n",
      "Epoch 190/1000\n",
      " - 3s - loss: 0.0876 - acc: 0.9857 - val_loss: 0.2527 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/1000\n",
      " - 3s - loss: 0.0876 - acc: 0.9864 - val_loss: 0.2526 - val_acc: 0.9000\n",
      "Epoch 192/1000\n",
      " - 3s - loss: 0.0874 - acc: 0.9861 - val_loss: 0.2525 - val_acc: 0.9000\n",
      "Epoch 193/1000\n",
      " - 3s - loss: 0.0863 - acc: 0.9868 - val_loss: 0.2523 - val_acc: 0.9000\n",
      "Epoch 194/1000\n",
      " - 3s - loss: 0.0856 - acc: 0.9877 - val_loss: 0.2522 - val_acc: 0.9000\n",
      "Epoch 195/1000\n",
      " - 4s - loss: 0.0845 - acc: 0.9870 - val_loss: 0.2521 - val_acc: 0.9000\n",
      "Epoch 196/1000\n",
      " - 4s - loss: 0.0853 - acc: 0.9863 - val_loss: 0.2520 - val_acc: 0.9010\n",
      "Epoch 197/1000\n",
      " - 3s - loss: 0.0833 - acc: 0.9870 - val_loss: 0.2519 - val_acc: 0.9010\n",
      "Epoch 198/1000\n",
      " - 3s - loss: 0.0838 - acc: 0.9863 - val_loss: 0.2518 - val_acc: 0.9000\n",
      "Epoch 199/1000\n",
      " - 3s - loss: 0.0842 - acc: 0.9863 - val_loss: 0.2516 - val_acc: 0.9010\n",
      "Epoch 200/1000\n",
      " - 3s - loss: 0.0805 - acc: 0.9882 - val_loss: 0.2515 - val_acc: 0.9020\n",
      "Epoch 201/1000\n",
      " - 3s - loss: 0.0817 - acc: 0.9878 - val_loss: 0.2515 - val_acc: 0.9010\n",
      "Epoch 202/1000\n",
      " - 3s - loss: 0.0830 - acc: 0.9864 - val_loss: 0.2513 - val_acc: 0.9010\n",
      "Epoch 203/1000\n",
      " - 3s - loss: 0.0823 - acc: 0.9867 - val_loss: 0.2512 - val_acc: 0.9020\n",
      "Epoch 204/1000\n",
      " - 3s - loss: 0.0817 - acc: 0.9873 - val_loss: 0.2511 - val_acc: 0.9020\n",
      "Epoch 205/1000\n",
      " - 3s - loss: 0.0807 - acc: 0.9881 - val_loss: 0.2510 - val_acc: 0.9010\n",
      "Epoch 206/1000\n",
      " - 3s - loss: 0.0796 - acc: 0.9881 - val_loss: 0.2509 - val_acc: 0.9010\n",
      "Epoch 207/1000\n",
      " - 3s - loss: 0.0795 - acc: 0.9893 - val_loss: 0.2508 - val_acc: 0.9010\n",
      "Epoch 208/1000\n",
      " - 4s - loss: 0.0789 - acc: 0.9881 - val_loss: 0.2508 - val_acc: 0.9020\n",
      "Epoch 209/1000\n",
      " - 3s - loss: 0.0789 - acc: 0.9876 - val_loss: 0.2508 - val_acc: 0.9010\n",
      "Epoch 210/1000\n",
      " - 3s - loss: 0.0767 - acc: 0.9883 - val_loss: 0.2506 - val_acc: 0.9010\n",
      "Epoch 211/1000\n",
      " - 3s - loss: 0.0782 - acc: 0.9874 - val_loss: 0.2505 - val_acc: 0.9020\n",
      "Epoch 212/1000\n",
      " - 3s - loss: 0.0771 - acc: 0.9882 - val_loss: 0.2505 - val_acc: 0.9020\n",
      "Epoch 213/1000\n",
      " - 3s - loss: 0.0761 - acc: 0.9887 - val_loss: 0.2504 - val_acc: 0.9030\n",
      "Epoch 214/1000\n",
      " - 3s - loss: 0.0769 - acc: 0.9881 - val_loss: 0.2503 - val_acc: 0.9030\n",
      "Epoch 215/1000\n",
      " - 3s - loss: 0.0768 - acc: 0.9889 - val_loss: 0.2501 - val_acc: 0.9030\n",
      "Epoch 216/1000\n",
      " - 3s - loss: 0.0756 - acc: 0.9882 - val_loss: 0.2501 - val_acc: 0.9030\n",
      "Epoch 217/1000\n",
      " - 3s - loss: 0.0755 - acc: 0.9877 - val_loss: 0.2500 - val_acc: 0.9030\n",
      "Epoch 218/1000\n",
      " - 3s - loss: 0.0747 - acc: 0.9892 - val_loss: 0.2499 - val_acc: 0.9040\n",
      "Epoch 219/1000\n",
      " - 3s - loss: 0.0735 - acc: 0.9894 - val_loss: 0.2498 - val_acc: 0.9040\n",
      "Epoch 220/1000\n",
      " - 3s - loss: 0.0731 - acc: 0.9886 - val_loss: 0.2498 - val_acc: 0.9040\n",
      "Epoch 221/1000\n",
      " - 3s - loss: 0.0756 - acc: 0.9878 - val_loss: 0.2498 - val_acc: 0.9040\n",
      "Epoch 222/1000\n",
      " - 3s - loss: 0.0742 - acc: 0.9879 - val_loss: 0.2497 - val_acc: 0.9040\n",
      "Epoch 223/1000\n",
      " - 3s - loss: 0.0733 - acc: 0.9886 - val_loss: 0.2496 - val_acc: 0.9040\n",
      "Epoch 224/1000\n",
      " - 3s - loss: 0.0735 - acc: 0.9879 - val_loss: 0.2496 - val_acc: 0.9030\n",
      "Epoch 225/1000\n",
      " - 3s - loss: 0.0721 - acc: 0.9900 - val_loss: 0.2496 - val_acc: 0.9030\n",
      "Epoch 226/1000\n",
      " - 3s - loss: 0.0716 - acc: 0.9899 - val_loss: 0.2495 - val_acc: 0.9030\n",
      "Epoch 227/1000\n",
      " - 3s - loss: 0.0719 - acc: 0.9884 - val_loss: 0.2495 - val_acc: 0.9040\n",
      "Epoch 228/1000\n",
      " - 3s - loss: 0.0708 - acc: 0.9892 - val_loss: 0.2495 - val_acc: 0.9040\n",
      "Epoch 229/1000\n",
      " - 3s - loss: 0.0707 - acc: 0.9889 - val_loss: 0.2495 - val_acc: 0.9030\n",
      "Validation accuracy: 0.903, loss: 0.24951123142242432\n",
      "Accuracy: 0.903, Parameters: (layers=1, units=8)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 3s - loss: 0.6848 - acc: 0.6923 - val_loss: 0.6766 - val_acc: 0.7820\n",
      "Epoch 2/1000\n",
      " - 3s - loss: 0.6651 - acc: 0.8668 - val_loss: 0.6606 - val_acc: 0.8270\n",
      "Epoch 3/1000\n",
      " - 3s - loss: 0.6464 - acc: 0.8941 - val_loss: 0.6454 - val_acc: 0.8350\n",
      "Epoch 4/1000\n",
      " - 3s - loss: 0.6287 - acc: 0.9014 - val_loss: 0.6312 - val_acc: 0.8530\n",
      "Epoch 5/1000\n",
      " - 3s - loss: 0.6116 - acc: 0.9097 - val_loss: 0.6177 - val_acc: 0.8560\n",
      "Epoch 6/1000\n",
      " - 3s - loss: 0.5955 - acc: 0.9172 - val_loss: 0.6049 - val_acc: 0.8600\n",
      "Epoch 7/1000\n",
      " - 3s - loss: 0.5800 - acc: 0.9228 - val_loss: 0.5927 - val_acc: 0.8680\n",
      "Epoch 8/1000\n",
      " - 3s - loss: 0.5657 - acc: 0.9264 - val_loss: 0.5811 - val_acc: 0.8710\n",
      "Epoch 9/1000\n",
      " - 3s - loss: 0.5517 - acc: 0.9289 - val_loss: 0.5701 - val_acc: 0.8720\n",
      "Epoch 10/1000\n",
      " - 3s - loss: 0.5378 - acc: 0.9296 - val_loss: 0.5596 - val_acc: 0.8720\n",
      "Epoch 11/1000\n",
      " - 3s - loss: 0.5255 - acc: 0.9307 - val_loss: 0.5496 - val_acc: 0.8740\n",
      "Epoch 12/1000\n",
      " - 3s - loss: 0.5130 - acc: 0.9332 - val_loss: 0.5400 - val_acc: 0.8720\n",
      "Epoch 13/1000\n",
      " - 3s - loss: 0.5015 - acc: 0.9327 - val_loss: 0.5309 - val_acc: 0.8750\n",
      "Epoch 14/1000\n",
      " - 3s - loss: 0.4903 - acc: 0.9352 - val_loss: 0.5222 - val_acc: 0.8700\n",
      "Epoch 15/1000\n",
      " - 3s - loss: 0.4795 - acc: 0.9366 - val_loss: 0.5139 - val_acc: 0.8700\n",
      "Epoch 16/1000\n",
      " - 3s - loss: 0.4701 - acc: 0.9356 - val_loss: 0.5060 - val_acc: 0.8710\n",
      "Epoch 17/1000\n",
      " - 3s - loss: 0.4591 - acc: 0.9393 - val_loss: 0.4984 - val_acc: 0.8730\n",
      "Epoch 18/1000\n",
      " - 3s - loss: 0.4503 - acc: 0.9368 - val_loss: 0.4911 - val_acc: 0.8720\n",
      "Epoch 19/1000\n",
      " - 3s - loss: 0.4412 - acc: 0.9381 - val_loss: 0.4841 - val_acc: 0.8750\n",
      "Epoch 20/1000\n",
      " - 3s - loss: 0.4327 - acc: 0.9392 - val_loss: 0.4775 - val_acc: 0.8740\n",
      "Epoch 21/1000\n",
      " - 3s - loss: 0.4242 - acc: 0.9418 - val_loss: 0.4711 - val_acc: 0.8730\n",
      "Epoch 22/1000\n",
      " - 3s - loss: 0.4150 - acc: 0.9458 - val_loss: 0.4649 - val_acc: 0.8740\n",
      "Epoch 23/1000\n",
      " - 3s - loss: 0.4077 - acc: 0.9441 - val_loss: 0.4590 - val_acc: 0.8760\n",
      "Epoch 24/1000\n",
      " - 3s - loss: 0.4019 - acc: 0.9423 - val_loss: 0.4534 - val_acc: 0.8760\n",
      "Epoch 25/1000\n",
      " - 3s - loss: 0.3935 - acc: 0.9448 - val_loss: 0.4479 - val_acc: 0.8770\n",
      "Epoch 26/1000\n",
      " - 3s - loss: 0.3873 - acc: 0.9428 - val_loss: 0.4426 - val_acc: 0.8780\n",
      "Epoch 27/1000\n",
      " - 3s - loss: 0.3804 - acc: 0.9440 - val_loss: 0.4376 - val_acc: 0.8770\n",
      "Epoch 28/1000\n",
      " - 3s - loss: 0.3729 - acc: 0.9480 - val_loss: 0.4327 - val_acc: 0.8770\n",
      "Epoch 29/1000\n",
      " - 3s - loss: 0.3669 - acc: 0.9486 - val_loss: 0.4280 - val_acc: 0.8790\n",
      "Epoch 30/1000\n",
      " - 3s - loss: 0.3614 - acc: 0.9489 - val_loss: 0.4235 - val_acc: 0.8790\n",
      "Epoch 31/1000\n",
      " - 3s - loss: 0.3555 - acc: 0.9479 - val_loss: 0.4191 - val_acc: 0.8800\n",
      "Epoch 32/1000\n",
      " - 3s - loss: 0.3501 - acc: 0.9482 - val_loss: 0.4149 - val_acc: 0.8820\n",
      "Epoch 33/1000\n",
      " - 3s - loss: 0.3440 - acc: 0.9484 - val_loss: 0.4108 - val_acc: 0.8830\n",
      "Epoch 34/1000\n",
      " - 3s - loss: 0.3384 - acc: 0.9513 - val_loss: 0.4068 - val_acc: 0.8840\n",
      "Epoch 35/1000\n",
      " - 3s - loss: 0.3335 - acc: 0.9488 - val_loss: 0.4031 - val_acc: 0.8840\n",
      "Epoch 36/1000\n",
      " - 3s - loss: 0.3303 - acc: 0.9473 - val_loss: 0.3994 - val_acc: 0.8830\n",
      "Epoch 37/1000\n",
      " - 3s - loss: 0.3238 - acc: 0.9527 - val_loss: 0.3959 - val_acc: 0.8840\n",
      "Epoch 38/1000\n",
      " - 3s - loss: 0.3190 - acc: 0.9527 - val_loss: 0.3924 - val_acc: 0.8860\n",
      "Epoch 39/1000\n",
      " - 3s - loss: 0.3134 - acc: 0.9522 - val_loss: 0.3891 - val_acc: 0.8850\n",
      "Epoch 40/1000\n",
      " - 3s - loss: 0.3094 - acc: 0.9560 - val_loss: 0.3858 - val_acc: 0.8850\n",
      "Epoch 41/1000\n",
      " - 3s - loss: 0.3051 - acc: 0.9537 - val_loss: 0.3826 - val_acc: 0.8860\n",
      "Epoch 42/1000\n",
      " - 3s - loss: 0.3019 - acc: 0.9546 - val_loss: 0.3796 - val_acc: 0.8870\n",
      "Epoch 43/1000\n",
      " - 3s - loss: 0.2980 - acc: 0.9537 - val_loss: 0.3766 - val_acc: 0.8860\n",
      "Epoch 44/1000\n",
      " - 3s - loss: 0.2937 - acc: 0.9532 - val_loss: 0.3737 - val_acc: 0.8850\n",
      "Epoch 45/1000\n",
      " - 3s - loss: 0.2900 - acc: 0.9557 - val_loss: 0.3709 - val_acc: 0.8840\n",
      "Epoch 46/1000\n",
      " - 3s - loss: 0.2852 - acc: 0.9568 - val_loss: 0.3682 - val_acc: 0.8840\n",
      "Epoch 47/1000\n",
      " - 3s - loss: 0.2824 - acc: 0.9562 - val_loss: 0.3655 - val_acc: 0.8840\n",
      "Epoch 48/1000\n",
      " - 4s - loss: 0.2781 - acc: 0.9574 - val_loss: 0.3629 - val_acc: 0.8840\n",
      "Epoch 49/1000\n",
      " - 3s - loss: 0.2751 - acc: 0.9577 - val_loss: 0.3604 - val_acc: 0.8860\n",
      "Epoch 50/1000\n",
      " - 3s - loss: 0.2704 - acc: 0.9584 - val_loss: 0.3579 - val_acc: 0.8870\n",
      "Epoch 51/1000\n",
      " - 3s - loss: 0.2676 - acc: 0.9572 - val_loss: 0.3555 - val_acc: 0.8880\n",
      "Epoch 52/1000\n",
      " - 3s - loss: 0.2642 - acc: 0.9586 - val_loss: 0.3532 - val_acc: 0.8880\n",
      "Epoch 53/1000\n",
      " - 4s - loss: 0.2608 - acc: 0.9594 - val_loss: 0.3510 - val_acc: 0.8880\n",
      "Epoch 54/1000\n",
      " - 3s - loss: 0.2578 - acc: 0.9606 - val_loss: 0.3487 - val_acc: 0.8870\n",
      "Epoch 55/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.2542 - acc: 0.9604 - val_loss: 0.3465 - val_acc: 0.8880\n",
      "Epoch 56/1000\n",
      " - 3s - loss: 0.2518 - acc: 0.9590 - val_loss: 0.3444 - val_acc: 0.8900\n",
      "Epoch 57/1000\n",
      " - 4s - loss: 0.2487 - acc: 0.9610 - val_loss: 0.3423 - val_acc: 0.8890\n",
      "Epoch 58/1000\n",
      " - 3s - loss: 0.2450 - acc: 0.9623 - val_loss: 0.3403 - val_acc: 0.8900\n",
      "Epoch 59/1000\n",
      " - 3s - loss: 0.2424 - acc: 0.9613 - val_loss: 0.3383 - val_acc: 0.8900\n",
      "Epoch 60/1000\n",
      " - 3s - loss: 0.2416 - acc: 0.9602 - val_loss: 0.3365 - val_acc: 0.8910\n",
      "Epoch 61/1000\n",
      " - 3s - loss: 0.2375 - acc: 0.9604 - val_loss: 0.3347 - val_acc: 0.8910\n",
      "Epoch 62/1000\n",
      " - 3s - loss: 0.2348 - acc: 0.9649 - val_loss: 0.3328 - val_acc: 0.8920\n",
      "Epoch 63/1000\n",
      " - 3s - loss: 0.2319 - acc: 0.9623 - val_loss: 0.3311 - val_acc: 0.8920\n",
      "Epoch 64/1000\n",
      " - 3s - loss: 0.2296 - acc: 0.9624 - val_loss: 0.3294 - val_acc: 0.8920\n",
      "Epoch 65/1000\n",
      " - 3s - loss: 0.2273 - acc: 0.9647 - val_loss: 0.3277 - val_acc: 0.8930\n",
      "Epoch 66/1000\n",
      " - 3s - loss: 0.2251 - acc: 0.9636 - val_loss: 0.3260 - val_acc: 0.8930\n",
      "Epoch 67/1000\n",
      " - 3s - loss: 0.2227 - acc: 0.9646 - val_loss: 0.3244 - val_acc: 0.8930\n",
      "Epoch 68/1000\n",
      " - 3s - loss: 0.2199 - acc: 0.9647 - val_loss: 0.3228 - val_acc: 0.8930\n",
      "Epoch 69/1000\n",
      " - 3s - loss: 0.2168 - acc: 0.9657 - val_loss: 0.3213 - val_acc: 0.8950\n",
      "Epoch 70/1000\n",
      " - 3s - loss: 0.2156 - acc: 0.9660 - val_loss: 0.3197 - val_acc: 0.8950\n",
      "Epoch 71/1000\n",
      " - 3s - loss: 0.2142 - acc: 0.9649 - val_loss: 0.3183 - val_acc: 0.8950\n",
      "Epoch 72/1000\n",
      " - 3s - loss: 0.2107 - acc: 0.9676 - val_loss: 0.3169 - val_acc: 0.8940\n",
      "Epoch 73/1000\n",
      " - 3s - loss: 0.2081 - acc: 0.9668 - val_loss: 0.3155 - val_acc: 0.8940\n",
      "Epoch 74/1000\n",
      " - 3s - loss: 0.2064 - acc: 0.9664 - val_loss: 0.3141 - val_acc: 0.8940\n",
      "Epoch 75/1000\n",
      " - 3s - loss: 0.2052 - acc: 0.9679 - val_loss: 0.3128 - val_acc: 0.8940\n",
      "Epoch 76/1000\n",
      " - 3s - loss: 0.2027 - acc: 0.9672 - val_loss: 0.3115 - val_acc: 0.8960\n",
      "Epoch 77/1000\n",
      " - 3s - loss: 0.1997 - acc: 0.9678 - val_loss: 0.3102 - val_acc: 0.8950\n",
      "Epoch 78/1000\n",
      " - 4s - loss: 0.1992 - acc: 0.9684 - val_loss: 0.3089 - val_acc: 0.8950\n",
      "Epoch 79/1000\n",
      " - 4s - loss: 0.1961 - acc: 0.9681 - val_loss: 0.3077 - val_acc: 0.8950\n",
      "Epoch 80/1000\n",
      " - 4s - loss: 0.1932 - acc: 0.9683 - val_loss: 0.3064 - val_acc: 0.8950\n",
      "Epoch 81/1000\n",
      " - 3s - loss: 0.1922 - acc: 0.9694 - val_loss: 0.3053 - val_acc: 0.8950\n",
      "Epoch 82/1000\n",
      " - 3s - loss: 0.1909 - acc: 0.9693 - val_loss: 0.3042 - val_acc: 0.8950\n",
      "Epoch 83/1000\n",
      " - 3s - loss: 0.1895 - acc: 0.9680 - val_loss: 0.3030 - val_acc: 0.8950\n",
      "Epoch 84/1000\n",
      " - 3s - loss: 0.1871 - acc: 0.9706 - val_loss: 0.3019 - val_acc: 0.8960\n",
      "Epoch 85/1000\n",
      " - 3s - loss: 0.1862 - acc: 0.9699 - val_loss: 0.3008 - val_acc: 0.8970\n",
      "Epoch 86/1000\n",
      " - 3s - loss: 0.1854 - acc: 0.9687 - val_loss: 0.2998 - val_acc: 0.8980\n",
      "Epoch 87/1000\n",
      " - 3s - loss: 0.1833 - acc: 0.9701 - val_loss: 0.2987 - val_acc: 0.8980\n",
      "Epoch 88/1000\n",
      " - 3s - loss: 0.1815 - acc: 0.9697 - val_loss: 0.2977 - val_acc: 0.8980\n",
      "Epoch 89/1000\n",
      " - 3s - loss: 0.1783 - acc: 0.9709 - val_loss: 0.2967 - val_acc: 0.8980\n",
      "Epoch 90/1000\n",
      " - 3s - loss: 0.1776 - acc: 0.9708 - val_loss: 0.2957 - val_acc: 0.8980\n",
      "Epoch 91/1000\n",
      " - 3s - loss: 0.1754 - acc: 0.9687 - val_loss: 0.2948 - val_acc: 0.8980\n",
      "Epoch 92/1000\n",
      " - 3s - loss: 0.1738 - acc: 0.9709 - val_loss: 0.2938 - val_acc: 0.8980\n",
      "Epoch 93/1000\n",
      " - 3s - loss: 0.1733 - acc: 0.9711 - val_loss: 0.2929 - val_acc: 0.8980\n",
      "Epoch 94/1000\n",
      " - 3s - loss: 0.1717 - acc: 0.9728 - val_loss: 0.2920 - val_acc: 0.8980\n",
      "Epoch 95/1000\n",
      " - 3s - loss: 0.1703 - acc: 0.9706 - val_loss: 0.2911 - val_acc: 0.8990\n",
      "Epoch 96/1000\n",
      " - 3s - loss: 0.1683 - acc: 0.9722 - val_loss: 0.2903 - val_acc: 0.8980\n",
      "Epoch 97/1000\n",
      " - 3s - loss: 0.1663 - acc: 0.9719 - val_loss: 0.2894 - val_acc: 0.8980\n",
      "Epoch 98/1000\n",
      " - 3s - loss: 0.1654 - acc: 0.9730 - val_loss: 0.2886 - val_acc: 0.8980\n",
      "Epoch 99/1000\n",
      " - 3s - loss: 0.1641 - acc: 0.9737 - val_loss: 0.2877 - val_acc: 0.8990\n",
      "Epoch 100/1000\n",
      " - 3s - loss: 0.1613 - acc: 0.9737 - val_loss: 0.2869 - val_acc: 0.8990\n",
      "Epoch 101/1000\n",
      " - 3s - loss: 0.1612 - acc: 0.9720 - val_loss: 0.2862 - val_acc: 0.8970\n",
      "Epoch 102/1000\n",
      " - 3s - loss: 0.1595 - acc: 0.9742 - val_loss: 0.2854 - val_acc: 0.8970\n",
      "Epoch 103/1000\n",
      " - 4s - loss: 0.1583 - acc: 0.9736 - val_loss: 0.2846 - val_acc: 0.8980\n",
      "Epoch 104/1000\n",
      " - 4s - loss: 0.1571 - acc: 0.9727 - val_loss: 0.2839 - val_acc: 0.8980\n",
      "Epoch 105/1000\n",
      " - 4s - loss: 0.1560 - acc: 0.9749 - val_loss: 0.2831 - val_acc: 0.8980\n",
      "Epoch 106/1000\n",
      " - 3s - loss: 0.1543 - acc: 0.9750 - val_loss: 0.2824 - val_acc: 0.8980\n",
      "Epoch 107/1000\n",
      " - 3s - loss: 0.1521 - acc: 0.9754 - val_loss: 0.2818 - val_acc: 0.8970\n",
      "Epoch 108/1000\n",
      " - 3s - loss: 0.1528 - acc: 0.9752 - val_loss: 0.2811 - val_acc: 0.8970\n",
      "Epoch 109/1000\n",
      " - 3s - loss: 0.1506 - acc: 0.9749 - val_loss: 0.2804 - val_acc: 0.8980\n",
      "Epoch 110/1000\n",
      " - 3s - loss: 0.1496 - acc: 0.9739 - val_loss: 0.2798 - val_acc: 0.9000\n",
      "Epoch 111/1000\n",
      " - 3s - loss: 0.1487 - acc: 0.9761 - val_loss: 0.2791 - val_acc: 0.8980\n",
      "Epoch 112/1000\n",
      " - 3s - loss: 0.1469 - acc: 0.9771 - val_loss: 0.2785 - val_acc: 0.8970\n",
      "Epoch 113/1000\n",
      " - 3s - loss: 0.1459 - acc: 0.9774 - val_loss: 0.2779 - val_acc: 0.8970\n",
      "Epoch 114/1000\n",
      " - 4s - loss: 0.1446 - acc: 0.9774 - val_loss: 0.2773 - val_acc: 0.8970\n",
      "Epoch 115/1000\n",
      " - 4s - loss: 0.1453 - acc: 0.9754 - val_loss: 0.2767 - val_acc: 0.8980\n",
      "Epoch 116/1000\n",
      " - 3s - loss: 0.1438 - acc: 0.9754 - val_loss: 0.2761 - val_acc: 0.8980\n",
      "Epoch 117/1000\n",
      " - 3s - loss: 0.1422 - acc: 0.9750 - val_loss: 0.2755 - val_acc: 0.9000\n",
      "Epoch 118/1000\n",
      " - 3s - loss: 0.1388 - acc: 0.9783 - val_loss: 0.2750 - val_acc: 0.8990\n",
      "Epoch 119/1000\n",
      " - 3s - loss: 0.1386 - acc: 0.9792 - val_loss: 0.2744 - val_acc: 0.8990\n",
      "Epoch 120/1000\n",
      " - 3s - loss: 0.1379 - acc: 0.9778 - val_loss: 0.2738 - val_acc: 0.8990\n",
      "Epoch 121/1000\n",
      " - 3s - loss: 0.1376 - acc: 0.9766 - val_loss: 0.2733 - val_acc: 0.8980\n",
      "Epoch 122/1000\n",
      " - 3s - loss: 0.1359 - acc: 0.9780 - val_loss: 0.2728 - val_acc: 0.8990\n",
      "Epoch 123/1000\n",
      " - 3s - loss: 0.1351 - acc: 0.9792 - val_loss: 0.2723 - val_acc: 0.8980\n",
      "Epoch 124/1000\n",
      " - 3s - loss: 0.1350 - acc: 0.9784 - val_loss: 0.2718 - val_acc: 0.8970\n",
      "Epoch 125/1000\n",
      " - 3s - loss: 0.1338 - acc: 0.9772 - val_loss: 0.2713 - val_acc: 0.8970\n",
      "Epoch 126/1000\n",
      " - 3s - loss: 0.1323 - acc: 0.9773 - val_loss: 0.2708 - val_acc: 0.8980\n",
      "Epoch 127/1000\n",
      " - 3s - loss: 0.1319 - acc: 0.9790 - val_loss: 0.2704 - val_acc: 0.8980\n",
      "Epoch 128/1000\n",
      " - 3s - loss: 0.1305 - acc: 0.9807 - val_loss: 0.2699 - val_acc: 0.8970\n",
      "Epoch 129/1000\n",
      " - 3s - loss: 0.1294 - acc: 0.9794 - val_loss: 0.2695 - val_acc: 0.8970\n",
      "Epoch 130/1000\n",
      " - 3s - loss: 0.1281 - acc: 0.9789 - val_loss: 0.2690 - val_acc: 0.8970\n",
      "Epoch 131/1000\n",
      " - 3s - loss: 0.1284 - acc: 0.9788 - val_loss: 0.2686 - val_acc: 0.8980\n",
      "Epoch 132/1000\n",
      " - 3s - loss: 0.1278 - acc: 0.9780 - val_loss: 0.2681 - val_acc: 0.8980\n",
      "Epoch 133/1000\n",
      " - 3s - loss: 0.1256 - acc: 0.9786 - val_loss: 0.2677 - val_acc: 0.8970\n",
      "Epoch 134/1000\n",
      " - 3s - loss: 0.1248 - acc: 0.9814 - val_loss: 0.2673 - val_acc: 0.8970\n",
      "Epoch 135/1000\n",
      " - 3s - loss: 0.1229 - acc: 0.9797 - val_loss: 0.2669 - val_acc: 0.8970\n",
      "Epoch 136/1000\n",
      " - 3s - loss: 0.1232 - acc: 0.9798 - val_loss: 0.2664 - val_acc: 0.8980\n",
      "Epoch 137/1000\n",
      " - 3s - loss: 0.1233 - acc: 0.9798 - val_loss: 0.2661 - val_acc: 0.8970\n",
      "Epoch 138/1000\n",
      " - 3s - loss: 0.1212 - acc: 0.9792 - val_loss: 0.2657 - val_acc: 0.8970\n",
      "Epoch 139/1000\n",
      " - 3s - loss: 0.1208 - acc: 0.9790 - val_loss: 0.2653 - val_acc: 0.8970\n",
      "Epoch 140/1000\n",
      " - 3s - loss: 0.1206 - acc: 0.9789 - val_loss: 0.2649 - val_acc: 0.8970\n",
      "Epoch 141/1000\n",
      " - 3s - loss: 0.1186 - acc: 0.9802 - val_loss: 0.2646 - val_acc: 0.8970\n",
      "Epoch 142/1000\n",
      " - 3s - loss: 0.1171 - acc: 0.9818 - val_loss: 0.2642 - val_acc: 0.8970\n",
      "Epoch 143/1000\n",
      " - 3s - loss: 0.1167 - acc: 0.9814 - val_loss: 0.2638 - val_acc: 0.8980\n",
      "Epoch 144/1000\n",
      " - 3s - loss: 0.1159 - acc: 0.9818 - val_loss: 0.2634 - val_acc: 0.8980\n",
      "Epoch 145/1000\n",
      " - 3s - loss: 0.1145 - acc: 0.9823 - val_loss: 0.2631 - val_acc: 0.8980\n",
      "Epoch 146/1000\n",
      " - 3s - loss: 0.1133 - acc: 0.9829 - val_loss: 0.2627 - val_acc: 0.8960\n",
      "Epoch 147/1000\n",
      " - 3s - loss: 0.1131 - acc: 0.9813 - val_loss: 0.2625 - val_acc: 0.8960\n",
      "Epoch 148/1000\n",
      " - 3s - loss: 0.1141 - acc: 0.9813 - val_loss: 0.2621 - val_acc: 0.8970\n",
      "Epoch 149/1000\n",
      " - 3s - loss: 0.1121 - acc: 0.9818 - val_loss: 0.2618 - val_acc: 0.8960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/1000\n",
      " - 3s - loss: 0.1109 - acc: 0.9814 - val_loss: 0.2615 - val_acc: 0.8970\n",
      "Epoch 151/1000\n",
      " - 3s - loss: 0.1108 - acc: 0.9830 - val_loss: 0.2611 - val_acc: 0.8970\n",
      "Epoch 152/1000\n",
      " - 3s - loss: 0.1106 - acc: 0.9821 - val_loss: 0.2609 - val_acc: 0.8970\n",
      "Epoch 153/1000\n",
      " - 3s - loss: 0.1106 - acc: 0.9811 - val_loss: 0.2605 - val_acc: 0.8970\n",
      "Epoch 154/1000\n",
      " - 3s - loss: 0.1087 - acc: 0.9823 - val_loss: 0.2603 - val_acc: 0.8970\n",
      "Epoch 155/1000\n",
      " - 3s - loss: 0.1096 - acc: 0.9811 - val_loss: 0.2600 - val_acc: 0.8970\n",
      "Epoch 156/1000\n",
      " - 3s - loss: 0.1082 - acc: 0.9827 - val_loss: 0.2597 - val_acc: 0.8970\n",
      "Epoch 157/1000\n",
      " - 3s - loss: 0.1069 - acc: 0.9819 - val_loss: 0.2594 - val_acc: 0.8980\n",
      "Epoch 158/1000\n",
      " - 3s - loss: 0.1058 - acc: 0.9844 - val_loss: 0.2591 - val_acc: 0.8970\n",
      "Epoch 159/1000\n",
      " - 3s - loss: 0.1062 - acc: 0.9829 - val_loss: 0.2589 - val_acc: 0.8970\n",
      "Epoch 160/1000\n",
      " - 3s - loss: 0.1042 - acc: 0.9836 - val_loss: 0.2586 - val_acc: 0.8970\n",
      "Epoch 161/1000\n",
      " - 3s - loss: 0.1043 - acc: 0.9838 - val_loss: 0.2583 - val_acc: 0.8980\n",
      "Epoch 162/1000\n",
      " - 3s - loss: 0.1039 - acc: 0.9828 - val_loss: 0.2581 - val_acc: 0.8980\n",
      "Epoch 163/1000\n",
      " - 3s - loss: 0.1037 - acc: 0.9826 - val_loss: 0.2579 - val_acc: 0.8980\n",
      "Epoch 164/1000\n",
      " - 3s - loss: 0.1025 - acc: 0.9837 - val_loss: 0.2577 - val_acc: 0.9000\n",
      "Epoch 165/1000\n",
      " - 3s - loss: 0.1019 - acc: 0.9828 - val_loss: 0.2575 - val_acc: 0.9000\n",
      "Epoch 166/1000\n",
      " - 3s - loss: 0.1008 - acc: 0.9834 - val_loss: 0.2572 - val_acc: 0.9000\n",
      "Epoch 167/1000\n",
      " - 3s - loss: 0.1000 - acc: 0.9848 - val_loss: 0.2569 - val_acc: 0.9000\n",
      "Epoch 168/1000\n",
      " - 3s - loss: 0.1001 - acc: 0.9841 - val_loss: 0.2568 - val_acc: 0.9000\n",
      "Epoch 169/1000\n",
      " - 3s - loss: 0.0990 - acc: 0.9838 - val_loss: 0.2565 - val_acc: 0.9000\n",
      "Epoch 170/1000\n",
      " - 3s - loss: 0.0981 - acc: 0.9838 - val_loss: 0.2563 - val_acc: 0.9000\n",
      "Epoch 171/1000\n",
      " - 3s - loss: 0.0972 - acc: 0.9839 - val_loss: 0.2561 - val_acc: 0.9010\n",
      "Epoch 172/1000\n",
      " - 3s - loss: 0.0953 - acc: 0.9858 - val_loss: 0.2559 - val_acc: 0.9010\n",
      "Epoch 173/1000\n",
      " - 4s - loss: 0.0969 - acc: 0.9851 - val_loss: 0.2557 - val_acc: 0.9000\n",
      "Epoch 174/1000\n",
      " - 3s - loss: 0.0954 - acc: 0.9844 - val_loss: 0.2555 - val_acc: 0.9010\n",
      "Epoch 175/1000\n",
      " - 3s - loss: 0.0955 - acc: 0.9833 - val_loss: 0.2553 - val_acc: 0.9020\n",
      "Epoch 176/1000\n",
      " - 3s - loss: 0.0946 - acc: 0.9871 - val_loss: 0.2552 - val_acc: 0.9020\n",
      "Epoch 177/1000\n",
      " - 3s - loss: 0.0943 - acc: 0.9858 - val_loss: 0.2550 - val_acc: 0.9010\n",
      "Epoch 178/1000\n",
      " - 3s - loss: 0.0926 - acc: 0.9853 - val_loss: 0.2549 - val_acc: 0.9010\n",
      "Epoch 179/1000\n",
      " - 3s - loss: 0.0938 - acc: 0.9839 - val_loss: 0.2546 - val_acc: 0.9010\n",
      "Epoch 180/1000\n",
      " - 3s - loss: 0.0931 - acc: 0.9851 - val_loss: 0.2545 - val_acc: 0.9010\n",
      "Epoch 181/1000\n",
      " - 3s - loss: 0.0916 - acc: 0.9854 - val_loss: 0.2543 - val_acc: 0.9020\n",
      "Epoch 182/1000\n",
      " - 3s - loss: 0.0914 - acc: 0.9860 - val_loss: 0.2541 - val_acc: 0.9030\n",
      "Epoch 183/1000\n",
      " - 3s - loss: 0.0923 - acc: 0.9856 - val_loss: 0.2539 - val_acc: 0.9020\n",
      "Epoch 184/1000\n",
      " - 3s - loss: 0.0914 - acc: 0.9842 - val_loss: 0.2538 - val_acc: 0.9020\n",
      "Epoch 185/1000\n",
      " - 3s - loss: 0.0905 - acc: 0.9857 - val_loss: 0.2536 - val_acc: 0.9020\n",
      "Epoch 186/1000\n",
      " - 3s - loss: 0.0903 - acc: 0.9864 - val_loss: 0.2535 - val_acc: 0.9020\n",
      "Epoch 187/1000\n",
      " - 3s - loss: 0.0889 - acc: 0.9862 - val_loss: 0.2534 - val_acc: 0.9000\n",
      "Epoch 188/1000\n",
      " - 3s - loss: 0.0887 - acc: 0.9860 - val_loss: 0.2533 - val_acc: 0.9010\n",
      "Epoch 189/1000\n",
      " - 3s - loss: 0.0881 - acc: 0.9874 - val_loss: 0.2532 - val_acc: 0.9000\n",
      "Epoch 190/1000\n",
      " - 3s - loss: 0.0865 - acc: 0.9862 - val_loss: 0.2530 - val_acc: 0.9000\n",
      "Epoch 191/1000\n",
      " - 3s - loss: 0.0872 - acc: 0.9868 - val_loss: 0.2528 - val_acc: 0.9010\n",
      "Epoch 192/1000\n",
      " - 3s - loss: 0.0871 - acc: 0.9867 - val_loss: 0.2528 - val_acc: 0.9000\n",
      "Epoch 193/1000\n",
      " - 3s - loss: 0.0851 - acc: 0.9864 - val_loss: 0.2526 - val_acc: 0.9010\n",
      "Epoch 194/1000\n",
      " - 3s - loss: 0.0845 - acc: 0.9869 - val_loss: 0.2525 - val_acc: 0.9020\n",
      "Epoch 195/1000\n",
      " - 3s - loss: 0.0848 - acc: 0.9869 - val_loss: 0.2524 - val_acc: 0.9000\n",
      "Epoch 196/1000\n",
      " - 3s - loss: 0.0853 - acc: 0.9874 - val_loss: 0.2523 - val_acc: 0.9020\n",
      "Epoch 197/1000\n",
      " - 3s - loss: 0.0847 - acc: 0.9863 - val_loss: 0.2522 - val_acc: 0.9010\n",
      "Epoch 198/1000\n",
      " - 3s - loss: 0.0838 - acc: 0.9869 - val_loss: 0.2521 - val_acc: 0.9020\n",
      "Epoch 199/1000\n",
      " - 3s - loss: 0.0831 - acc: 0.9858 - val_loss: 0.2520 - val_acc: 0.9020\n",
      "Epoch 200/1000\n",
      " - 3s - loss: 0.0845 - acc: 0.9854 - val_loss: 0.2518 - val_acc: 0.9010\n",
      "Epoch 201/1000\n",
      " - 3s - loss: 0.0808 - acc: 0.9880 - val_loss: 0.2517 - val_acc: 0.9000\n",
      "Epoch 202/1000\n",
      " - 3s - loss: 0.0813 - acc: 0.9889 - val_loss: 0.2516 - val_acc: 0.9000\n",
      "Epoch 203/1000\n",
      " - 3s - loss: 0.0826 - acc: 0.9873 - val_loss: 0.2515 - val_acc: 0.9010\n",
      "Epoch 204/1000\n",
      " - 3s - loss: 0.0805 - acc: 0.9883 - val_loss: 0.2514 - val_acc: 0.9010\n",
      "Epoch 205/1000\n",
      " - 3s - loss: 0.0802 - acc: 0.9879 - val_loss: 0.2513 - val_acc: 0.9020\n",
      "Epoch 206/1000\n",
      " - 3s - loss: 0.0801 - acc: 0.9870 - val_loss: 0.2513 - val_acc: 0.9020\n",
      "Epoch 207/1000\n",
      " - 3s - loss: 0.0795 - acc: 0.9882 - val_loss: 0.2511 - val_acc: 0.9020\n",
      "Epoch 208/1000\n",
      " - 3s - loss: 0.0791 - acc: 0.9879 - val_loss: 0.2510 - val_acc: 0.9020\n",
      "Epoch 209/1000\n",
      " - 3s - loss: 0.0785 - acc: 0.9888 - val_loss: 0.2510 - val_acc: 0.9020\n",
      "Epoch 210/1000\n",
      " - 3s - loss: 0.0785 - acc: 0.9867 - val_loss: 0.2509 - val_acc: 0.9020\n",
      "Epoch 211/1000\n",
      " - 3s - loss: 0.0780 - acc: 0.9878 - val_loss: 0.2509 - val_acc: 0.9020\n",
      "Epoch 212/1000\n",
      " - 3s - loss: 0.0772 - acc: 0.9890 - val_loss: 0.2509 - val_acc: 0.9010\n",
      "Validation accuracy: 0.901, loss: 0.25087419390678406\n",
      "Accuracy: 0.901, Parameters: (layers=1, units=16)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 4s - loss: 0.6845 - acc: 0.7811 - val_loss: 0.6760 - val_acc: 0.7950\n",
      "Epoch 2/1000\n",
      " - 3s - loss: 0.6647 - acc: 0.8241 - val_loss: 0.6601 - val_acc: 0.8260\n",
      "Epoch 3/1000\n",
      " - 4s - loss: 0.6460 - acc: 0.8901 - val_loss: 0.6451 - val_acc: 0.8460\n",
      "Epoch 4/1000\n",
      " - 3s - loss: 0.6283 - acc: 0.8952 - val_loss: 0.6308 - val_acc: 0.8520\n",
      "Epoch 5/1000\n",
      " - 3s - loss: 0.6112 - acc: 0.9112 - val_loss: 0.6173 - val_acc: 0.8610\n",
      "Epoch 6/1000\n",
      " - 3s - loss: 0.5953 - acc: 0.9176 - val_loss: 0.6045 - val_acc: 0.8680\n",
      "Epoch 7/1000\n",
      " - 3s - loss: 0.5800 - acc: 0.9244 - val_loss: 0.5924 - val_acc: 0.8690\n",
      "Epoch 8/1000\n",
      " - 3s - loss: 0.5650 - acc: 0.9236 - val_loss: 0.5808 - val_acc: 0.8690\n",
      "Epoch 9/1000\n",
      " - 3s - loss: 0.5508 - acc: 0.9268 - val_loss: 0.5697 - val_acc: 0.8700\n",
      "Epoch 10/1000\n",
      " - 3s - loss: 0.5379 - acc: 0.9290 - val_loss: 0.5592 - val_acc: 0.8710\n",
      "Epoch 11/1000\n",
      " - 3s - loss: 0.5246 - acc: 0.9294 - val_loss: 0.5492 - val_acc: 0.8720\n",
      "Epoch 12/1000\n",
      " - 3s - loss: 0.5132 - acc: 0.9306 - val_loss: 0.5398 - val_acc: 0.8710\n",
      "Epoch 13/1000\n",
      " - 3s - loss: 0.5008 - acc: 0.9323 - val_loss: 0.5307 - val_acc: 0.8720\n",
      "Epoch 14/1000\n",
      " - 3s - loss: 0.4899 - acc: 0.9319 - val_loss: 0.5220 - val_acc: 0.8700\n",
      "Epoch 15/1000\n",
      " - 3s - loss: 0.4798 - acc: 0.9366 - val_loss: 0.5137 - val_acc: 0.8710\n",
      "Epoch 16/1000\n",
      " - 3s - loss: 0.4685 - acc: 0.9347 - val_loss: 0.5058 - val_acc: 0.8740\n",
      "Epoch 17/1000\n",
      " - 3s - loss: 0.4597 - acc: 0.9373 - val_loss: 0.4982 - val_acc: 0.8730\n",
      "Epoch 18/1000\n",
      " - 3s - loss: 0.4494 - acc: 0.9373 - val_loss: 0.4909 - val_acc: 0.8740\n",
      "Epoch 19/1000\n",
      " - 3s - loss: 0.4415 - acc: 0.9372 - val_loss: 0.4839 - val_acc: 0.8760\n",
      "Epoch 20/1000\n",
      " - 3s - loss: 0.4328 - acc: 0.9404 - val_loss: 0.4773 - val_acc: 0.8760\n",
      "Epoch 21/1000\n",
      " - 3s - loss: 0.4246 - acc: 0.9406 - val_loss: 0.4709 - val_acc: 0.8760\n",
      "Epoch 22/1000\n",
      " - 3s - loss: 0.4163 - acc: 0.9409 - val_loss: 0.4648 - val_acc: 0.8770\n",
      "Epoch 23/1000\n",
      " - 3s - loss: 0.4082 - acc: 0.9432 - val_loss: 0.4589 - val_acc: 0.8780\n",
      "Epoch 24/1000\n",
      " - 3s - loss: 0.4014 - acc: 0.9431 - val_loss: 0.4532 - val_acc: 0.8790\n",
      "Epoch 25/1000\n",
      " - 3s - loss: 0.3937 - acc: 0.9448 - val_loss: 0.4478 - val_acc: 0.8790\n",
      "Epoch 26/1000\n",
      " - 3s - loss: 0.3862 - acc: 0.9446 - val_loss: 0.4425 - val_acc: 0.8780\n",
      "Epoch 27/1000\n",
      " - 3s - loss: 0.3802 - acc: 0.9482 - val_loss: 0.4374 - val_acc: 0.8780\n",
      "Epoch 28/1000\n",
      " - 3s - loss: 0.3735 - acc: 0.9478 - val_loss: 0.4325 - val_acc: 0.8780\n",
      "Epoch 29/1000\n",
      " - 3s - loss: 0.3682 - acc: 0.9467 - val_loss: 0.4278 - val_acc: 0.8800\n",
      "Epoch 30/1000\n",
      " - 3s - loss: 0.3611 - acc: 0.9480 - val_loss: 0.4233 - val_acc: 0.8790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000\n",
      " - 3s - loss: 0.3548 - acc: 0.9479 - val_loss: 0.4189 - val_acc: 0.8790\n",
      "Epoch 32/1000\n",
      " - 3s - loss: 0.3494 - acc: 0.9479 - val_loss: 0.4147 - val_acc: 0.8810\n",
      "Epoch 33/1000\n",
      " - 3s - loss: 0.3439 - acc: 0.9504 - val_loss: 0.4106 - val_acc: 0.8820\n",
      "Epoch 34/1000\n",
      " - 3s - loss: 0.3389 - acc: 0.9487 - val_loss: 0.4067 - val_acc: 0.8810\n",
      "Epoch 35/1000\n",
      " - 3s - loss: 0.3332 - acc: 0.9522 - val_loss: 0.4028 - val_acc: 0.8820\n",
      "Epoch 36/1000\n",
      " - 4s - loss: 0.3280 - acc: 0.9492 - val_loss: 0.3991 - val_acc: 0.8820\n",
      "Epoch 37/1000\n",
      " - 3s - loss: 0.3235 - acc: 0.9537 - val_loss: 0.3955 - val_acc: 0.8820\n",
      "Epoch 38/1000\n",
      " - 3s - loss: 0.3187 - acc: 0.9504 - val_loss: 0.3921 - val_acc: 0.8810\n",
      "Epoch 39/1000\n",
      " - 3s - loss: 0.3155 - acc: 0.9528 - val_loss: 0.3887 - val_acc: 0.8820\n",
      "Epoch 40/1000\n",
      " - 3s - loss: 0.3099 - acc: 0.9519 - val_loss: 0.3855 - val_acc: 0.8800\n",
      "Epoch 41/1000\n",
      " - 3s - loss: 0.3060 - acc: 0.9537 - val_loss: 0.3823 - val_acc: 0.8820\n",
      "Epoch 42/1000\n",
      " - 3s - loss: 0.3022 - acc: 0.9527 - val_loss: 0.3793 - val_acc: 0.8850\n",
      "Epoch 43/1000\n",
      " - 3s - loss: 0.2974 - acc: 0.9533 - val_loss: 0.3763 - val_acc: 0.8840\n",
      "Epoch 44/1000\n",
      " - 3s - loss: 0.2926 - acc: 0.9560 - val_loss: 0.3734 - val_acc: 0.8830\n",
      "Epoch 45/1000\n",
      " - 4s - loss: 0.2889 - acc: 0.9548 - val_loss: 0.3706 - val_acc: 0.8850\n",
      "Epoch 46/1000\n",
      " - 3s - loss: 0.2842 - acc: 0.9581 - val_loss: 0.3678 - val_acc: 0.8850\n",
      "Epoch 47/1000\n",
      " - 3s - loss: 0.2816 - acc: 0.9563 - val_loss: 0.3652 - val_acc: 0.8850\n",
      "Epoch 48/1000\n",
      " - 3s - loss: 0.2780 - acc: 0.9579 - val_loss: 0.3626 - val_acc: 0.8850\n",
      "Epoch 49/1000\n",
      " - 3s - loss: 0.2728 - acc: 0.9587 - val_loss: 0.3600 - val_acc: 0.8850\n",
      "Epoch 50/1000\n",
      " - 3s - loss: 0.2713 - acc: 0.9582 - val_loss: 0.3576 - val_acc: 0.8850\n",
      "Epoch 51/1000\n",
      " - 3s - loss: 0.2676 - acc: 0.9587 - val_loss: 0.3552 - val_acc: 0.8840\n",
      "Epoch 52/1000\n",
      " - 3s - loss: 0.2644 - acc: 0.9580 - val_loss: 0.3529 - val_acc: 0.8850\n",
      "Epoch 53/1000\n",
      " - 3s - loss: 0.2614 - acc: 0.9589 - val_loss: 0.3507 - val_acc: 0.8870\n",
      "Epoch 54/1000\n",
      " - 3s - loss: 0.2566 - acc: 0.9587 - val_loss: 0.3484 - val_acc: 0.8870\n",
      "Epoch 55/1000\n",
      " - 3s - loss: 0.2543 - acc: 0.9601 - val_loss: 0.3463 - val_acc: 0.8880\n",
      "Epoch 56/1000\n",
      " - 3s - loss: 0.2524 - acc: 0.9596 - val_loss: 0.3442 - val_acc: 0.8890\n",
      "Epoch 57/1000\n",
      " - 4s - loss: 0.2494 - acc: 0.9612 - val_loss: 0.3422 - val_acc: 0.8890\n",
      "Epoch 58/1000\n",
      " - 3s - loss: 0.2465 - acc: 0.9608 - val_loss: 0.3402 - val_acc: 0.8910\n",
      "Epoch 59/1000\n",
      " - 3s - loss: 0.2436 - acc: 0.9622 - val_loss: 0.3383 - val_acc: 0.8910\n",
      "Epoch 60/1000\n",
      " - 3s - loss: 0.2400 - acc: 0.9611 - val_loss: 0.3364 - val_acc: 0.8920\n",
      "Epoch 61/1000\n",
      " - 3s - loss: 0.2370 - acc: 0.9608 - val_loss: 0.3345 - val_acc: 0.8910\n",
      "Epoch 62/1000\n",
      " - 3s - loss: 0.2358 - acc: 0.9620 - val_loss: 0.3327 - val_acc: 0.8920\n",
      "Epoch 63/1000\n",
      " - 3s - loss: 0.2318 - acc: 0.9636 - val_loss: 0.3309 - val_acc: 0.8920\n",
      "Epoch 64/1000\n",
      " - 3s - loss: 0.2303 - acc: 0.9628 - val_loss: 0.3292 - val_acc: 0.8920\n",
      "Epoch 65/1000\n",
      " - 3s - loss: 0.2273 - acc: 0.9637 - val_loss: 0.3276 - val_acc: 0.8920\n",
      "Epoch 66/1000\n",
      " - 3s - loss: 0.2244 - acc: 0.9632 - val_loss: 0.3259 - val_acc: 0.8920\n",
      "Epoch 67/1000\n",
      " - 3s - loss: 0.2227 - acc: 0.9632 - val_loss: 0.3243 - val_acc: 0.8920\n",
      "Epoch 68/1000\n",
      " - 3s - loss: 0.2191 - acc: 0.9650 - val_loss: 0.3227 - val_acc: 0.8920\n",
      "Epoch 69/1000\n",
      " - 3s - loss: 0.2171 - acc: 0.9660 - val_loss: 0.3212 - val_acc: 0.8930\n",
      "Epoch 70/1000\n",
      " - 4s - loss: 0.2159 - acc: 0.9642 - val_loss: 0.3197 - val_acc: 0.8930\n",
      "Epoch 71/1000\n",
      " - 3s - loss: 0.2124 - acc: 0.9669 - val_loss: 0.3182 - val_acc: 0.8930\n",
      "Epoch 72/1000\n",
      " - 3s - loss: 0.2115 - acc: 0.9669 - val_loss: 0.3168 - val_acc: 0.8940\n",
      "Epoch 73/1000\n",
      " - 3s - loss: 0.2090 - acc: 0.9664 - val_loss: 0.3154 - val_acc: 0.8960\n",
      "Epoch 74/1000\n",
      " - 3s - loss: 0.2057 - acc: 0.9681 - val_loss: 0.3141 - val_acc: 0.8960\n",
      "Epoch 75/1000\n",
      " - 3s - loss: 0.2054 - acc: 0.9668 - val_loss: 0.3127 - val_acc: 0.8970\n",
      "Epoch 76/1000\n",
      " - 3s - loss: 0.2018 - acc: 0.9682 - val_loss: 0.3114 - val_acc: 0.8970\n",
      "Epoch 77/1000\n",
      " - 3s - loss: 0.2010 - acc: 0.9674 - val_loss: 0.3101 - val_acc: 0.8970\n",
      "Epoch 78/1000\n",
      " - 3s - loss: 0.1989 - acc: 0.9679 - val_loss: 0.3088 - val_acc: 0.8970\n",
      "Epoch 79/1000\n",
      " - 3s - loss: 0.1971 - acc: 0.9679 - val_loss: 0.3076 - val_acc: 0.8990\n",
      "Epoch 80/1000\n",
      " - 3s - loss: 0.1948 - acc: 0.9679 - val_loss: 0.3064 - val_acc: 0.8990\n",
      "Epoch 81/1000\n",
      " - 3s - loss: 0.1928 - acc: 0.9679 - val_loss: 0.3053 - val_acc: 0.9000\n",
      "Epoch 82/1000\n",
      " - 3s - loss: 0.1911 - acc: 0.9677 - val_loss: 0.3041 - val_acc: 0.9010\n",
      "Epoch 83/1000\n",
      " - 3s - loss: 0.1903 - acc: 0.9680 - val_loss: 0.3029 - val_acc: 0.9000\n",
      "Epoch 84/1000\n",
      " - 3s - loss: 0.1877 - acc: 0.9679 - val_loss: 0.3018 - val_acc: 0.8990\n",
      "Epoch 85/1000\n",
      " - 3s - loss: 0.1856 - acc: 0.9687 - val_loss: 0.3008 - val_acc: 0.8980\n",
      "Epoch 86/1000\n",
      " - 3s - loss: 0.1831 - acc: 0.9708 - val_loss: 0.2997 - val_acc: 0.8990\n",
      "Epoch 87/1000\n",
      " - 3s - loss: 0.1825 - acc: 0.9717 - val_loss: 0.2986 - val_acc: 0.8990\n",
      "Epoch 88/1000\n",
      " - 3s - loss: 0.1810 - acc: 0.9704 - val_loss: 0.2976 - val_acc: 0.8990\n",
      "Epoch 89/1000\n",
      " - 3s - loss: 0.1792 - acc: 0.9711 - val_loss: 0.2966 - val_acc: 0.8990\n",
      "Epoch 90/1000\n",
      " - 4s - loss: 0.1765 - acc: 0.9711 - val_loss: 0.2956 - val_acc: 0.8990\n",
      "Epoch 91/1000\n",
      " - 4s - loss: 0.1750 - acc: 0.9728 - val_loss: 0.2947 - val_acc: 0.8990\n",
      "Epoch 92/1000\n",
      " - 3s - loss: 0.1739 - acc: 0.9721 - val_loss: 0.2937 - val_acc: 0.8990\n",
      "Epoch 93/1000\n",
      " - 3s - loss: 0.1730 - acc: 0.9714 - val_loss: 0.2928 - val_acc: 0.9000\n",
      "Epoch 94/1000\n",
      " - 3s - loss: 0.1721 - acc: 0.9717 - val_loss: 0.2919 - val_acc: 0.8990\n",
      "Epoch 95/1000\n",
      " - 3s - loss: 0.1697 - acc: 0.9723 - val_loss: 0.2910 - val_acc: 0.8990\n",
      "Epoch 96/1000\n",
      " - 4s - loss: 0.1701 - acc: 0.9694 - val_loss: 0.2902 - val_acc: 0.8980\n",
      "Epoch 97/1000\n",
      " - 4s - loss: 0.1664 - acc: 0.9729 - val_loss: 0.2893 - val_acc: 0.8980\n",
      "Epoch 98/1000\n",
      " - 3s - loss: 0.1649 - acc: 0.9746 - val_loss: 0.2884 - val_acc: 0.8980\n",
      "Epoch 99/1000\n",
      " - 4s - loss: 0.1641 - acc: 0.9730 - val_loss: 0.2876 - val_acc: 0.8990\n",
      "Epoch 100/1000\n",
      " - 4s - loss: 0.1631 - acc: 0.9727 - val_loss: 0.2868 - val_acc: 0.8980\n",
      "Epoch 101/1000\n",
      " - 3s - loss: 0.1620 - acc: 0.9743 - val_loss: 0.2861 - val_acc: 0.8980\n",
      "Epoch 102/1000\n",
      " - 3s - loss: 0.1604 - acc: 0.9722 - val_loss: 0.2853 - val_acc: 0.8990\n",
      "Epoch 103/1000\n",
      " - 3s - loss: 0.1588 - acc: 0.9747 - val_loss: 0.2845 - val_acc: 0.9000\n",
      "Epoch 104/1000\n",
      " - 3s - loss: 0.1564 - acc: 0.9741 - val_loss: 0.2838 - val_acc: 0.8990\n",
      "Epoch 105/1000\n",
      " - 3s - loss: 0.1560 - acc: 0.9728 - val_loss: 0.2831 - val_acc: 0.8990\n",
      "Epoch 106/1000\n",
      " - 3s - loss: 0.1556 - acc: 0.9739 - val_loss: 0.2824 - val_acc: 0.8990\n",
      "Epoch 107/1000\n",
      " - 3s - loss: 0.1535 - acc: 0.9749 - val_loss: 0.2817 - val_acc: 0.8980\n",
      "Epoch 108/1000\n",
      " - 3s - loss: 0.1531 - acc: 0.9749 - val_loss: 0.2810 - val_acc: 0.8980\n",
      "Epoch 109/1000\n",
      " - 3s - loss: 0.1503 - acc: 0.9771 - val_loss: 0.2804 - val_acc: 0.8980\n",
      "Epoch 110/1000\n",
      " - 3s - loss: 0.1490 - acc: 0.9761 - val_loss: 0.2797 - val_acc: 0.8990\n",
      "Epoch 111/1000\n",
      " - 3s - loss: 0.1494 - acc: 0.9749 - val_loss: 0.2790 - val_acc: 0.9000\n",
      "Epoch 112/1000\n",
      " - 3s - loss: 0.1476 - acc: 0.9749 - val_loss: 0.2784 - val_acc: 0.9000\n",
      "Epoch 113/1000\n",
      " - 3s - loss: 0.1451 - acc: 0.9761 - val_loss: 0.2778 - val_acc: 0.9000\n",
      "Epoch 114/1000\n",
      " - 3s - loss: 0.1456 - acc: 0.9759 - val_loss: 0.2772 - val_acc: 0.8990\n",
      "Epoch 115/1000\n",
      " - 3s - loss: 0.1439 - acc: 0.9763 - val_loss: 0.2766 - val_acc: 0.8990\n",
      "Epoch 116/1000\n",
      " - 3s - loss: 0.1440 - acc: 0.9759 - val_loss: 0.2760 - val_acc: 0.9000\n",
      "Epoch 117/1000\n",
      " - 3s - loss: 0.1420 - acc: 0.9781 - val_loss: 0.2755 - val_acc: 0.8990\n",
      "Epoch 118/1000\n",
      " - 3s - loss: 0.1420 - acc: 0.9772 - val_loss: 0.2749 - val_acc: 0.8990\n",
      "Epoch 119/1000\n",
      " - 3s - loss: 0.1400 - acc: 0.9772 - val_loss: 0.2743 - val_acc: 0.9000\n",
      "Epoch 120/1000\n",
      " - 3s - loss: 0.1371 - acc: 0.9778 - val_loss: 0.2738 - val_acc: 0.8980\n",
      "Epoch 121/1000\n",
      " - 3s - loss: 0.1376 - acc: 0.9768 - val_loss: 0.2732 - val_acc: 0.8990\n",
      "Epoch 122/1000\n",
      " - 3s - loss: 0.1354 - acc: 0.9771 - val_loss: 0.2727 - val_acc: 0.9000\n",
      "Epoch 123/1000\n",
      " - 3s - loss: 0.1347 - acc: 0.9778 - val_loss: 0.2722 - val_acc: 0.8980\n",
      "Epoch 124/1000\n",
      " - 3s - loss: 0.1340 - acc: 0.9793 - val_loss: 0.2717 - val_acc: 0.9000\n",
      "Epoch 125/1000\n",
      " - 3s - loss: 0.1321 - acc: 0.9798 - val_loss: 0.2711 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/1000\n",
      " - 3s - loss: 0.1318 - acc: 0.9797 - val_loss: 0.2707 - val_acc: 0.8990\n",
      "Epoch 127/1000\n",
      " - 3s - loss: 0.1308 - acc: 0.9781 - val_loss: 0.2703 - val_acc: 0.8980\n",
      "Epoch 128/1000\n",
      " - 3s - loss: 0.1309 - acc: 0.9773 - val_loss: 0.2698 - val_acc: 0.8980\n",
      "Epoch 129/1000\n",
      " - 3s - loss: 0.1302 - acc: 0.9804 - val_loss: 0.2693 - val_acc: 0.8980\n",
      "Epoch 130/1000\n",
      " - 3s - loss: 0.1288 - acc: 0.9788 - val_loss: 0.2689 - val_acc: 0.8980\n",
      "Epoch 131/1000\n",
      " - 4s - loss: 0.1292 - acc: 0.9780 - val_loss: 0.2684 - val_acc: 0.8980\n",
      "Epoch 132/1000\n",
      " - 3s - loss: 0.1263 - acc: 0.9789 - val_loss: 0.2680 - val_acc: 0.8980\n",
      "Epoch 133/1000\n",
      " - 3s - loss: 0.1257 - acc: 0.9803 - val_loss: 0.2676 - val_acc: 0.8980\n",
      "Epoch 134/1000\n",
      " - 4s - loss: 0.1235 - acc: 0.9806 - val_loss: 0.2671 - val_acc: 0.8980\n",
      "Epoch 135/1000\n",
      " - 3s - loss: 0.1232 - acc: 0.9798 - val_loss: 0.2667 - val_acc: 0.8990\n",
      "Epoch 136/1000\n",
      " - 3s - loss: 0.1223 - acc: 0.9808 - val_loss: 0.2662 - val_acc: 0.8990\n",
      "Epoch 137/1000\n",
      " - 3s - loss: 0.1229 - acc: 0.9793 - val_loss: 0.2658 - val_acc: 0.8980\n",
      "Epoch 138/1000\n",
      " - 4s - loss: 0.1206 - acc: 0.9816 - val_loss: 0.2654 - val_acc: 0.8990\n",
      "Epoch 139/1000\n",
      " - 3s - loss: 0.1219 - acc: 0.9799 - val_loss: 0.2650 - val_acc: 0.8990\n",
      "Epoch 140/1000\n",
      " - 3s - loss: 0.1190 - acc: 0.9803 - val_loss: 0.2646 - val_acc: 0.8990\n",
      "Epoch 141/1000\n",
      " - 3s - loss: 0.1199 - acc: 0.9806 - val_loss: 0.2643 - val_acc: 0.8990\n",
      "Epoch 142/1000\n",
      " - 3s - loss: 0.1186 - acc: 0.9813 - val_loss: 0.2639 - val_acc: 0.8980\n",
      "Epoch 143/1000\n",
      " - 4s - loss: 0.1178 - acc: 0.9809 - val_loss: 0.2635 - val_acc: 0.8990\n",
      "Epoch 144/1000\n",
      " - 3s - loss: 0.1167 - acc: 0.9810 - val_loss: 0.2631 - val_acc: 0.8990\n",
      "Epoch 145/1000\n",
      " - 3s - loss: 0.1152 - acc: 0.9810 - val_loss: 0.2628 - val_acc: 0.8990\n",
      "Epoch 146/1000\n",
      " - 3s - loss: 0.1160 - acc: 0.9808 - val_loss: 0.2624 - val_acc: 0.8990\n",
      "Epoch 147/1000\n",
      " - 3s - loss: 0.1147 - acc: 0.9804 - val_loss: 0.2621 - val_acc: 0.8990\n",
      "Epoch 148/1000\n",
      " - 3s - loss: 0.1145 - acc: 0.9812 - val_loss: 0.2619 - val_acc: 0.8990\n",
      "Epoch 149/1000\n",
      " - 3s - loss: 0.1140 - acc: 0.9811 - val_loss: 0.2616 - val_acc: 0.8990\n",
      "Epoch 150/1000\n",
      " - 3s - loss: 0.1109 - acc: 0.9811 - val_loss: 0.2612 - val_acc: 0.8990\n",
      "Epoch 151/1000\n",
      " - 3s - loss: 0.1107 - acc: 0.9838 - val_loss: 0.2610 - val_acc: 0.8990\n",
      "Epoch 152/1000\n",
      " - 3s - loss: 0.1097 - acc: 0.9833 - val_loss: 0.2606 - val_acc: 0.9000\n",
      "Epoch 153/1000\n",
      " - 3s - loss: 0.1106 - acc: 0.9813 - val_loss: 0.2604 - val_acc: 0.9010\n",
      "Epoch 154/1000\n",
      " - 3s - loss: 0.1088 - acc: 0.9837 - val_loss: 0.2602 - val_acc: 0.9010\n",
      "Epoch 155/1000\n",
      " - 3s - loss: 0.1070 - acc: 0.9832 - val_loss: 0.2598 - val_acc: 0.9020\n",
      "Epoch 156/1000\n",
      " - 3s - loss: 0.1073 - acc: 0.9838 - val_loss: 0.2596 - val_acc: 0.9020\n",
      "Epoch 157/1000\n",
      " - 3s - loss: 0.1058 - acc: 0.9830 - val_loss: 0.2593 - val_acc: 0.9020\n",
      "Epoch 158/1000\n",
      " - 4s - loss: 0.1073 - acc: 0.9803 - val_loss: 0.2591 - val_acc: 0.9010\n",
      "Epoch 159/1000\n",
      " - 4s - loss: 0.1065 - acc: 0.9831 - val_loss: 0.2588 - val_acc: 0.9010\n",
      "Epoch 160/1000\n",
      " - 3s - loss: 0.1058 - acc: 0.9834 - val_loss: 0.2585 - val_acc: 0.9010\n",
      "Epoch 161/1000\n",
      " - 3s - loss: 0.1048 - acc: 0.9839 - val_loss: 0.2582 - val_acc: 0.9010\n",
      "Epoch 162/1000\n",
      " - 4s - loss: 0.1027 - acc: 0.9851 - val_loss: 0.2580 - val_acc: 0.9010\n",
      "Epoch 163/1000\n",
      " - 3s - loss: 0.1025 - acc: 0.9834 - val_loss: 0.2578 - val_acc: 0.9010\n",
      "Epoch 164/1000\n",
      " - 3s - loss: 0.1013 - acc: 0.9850 - val_loss: 0.2576 - val_acc: 0.9010\n",
      "Epoch 165/1000\n",
      " - 3s - loss: 0.1018 - acc: 0.9838 - val_loss: 0.2573 - val_acc: 0.9010\n",
      "Epoch 166/1000\n",
      " - 3s - loss: 0.1012 - acc: 0.9831 - val_loss: 0.2571 - val_acc: 0.9010\n",
      "Epoch 167/1000\n",
      " - 3s - loss: 0.1009 - acc: 0.9847 - val_loss: 0.2569 - val_acc: 0.9010\n",
      "Epoch 168/1000\n",
      " - 3s - loss: 0.0993 - acc: 0.9851 - val_loss: 0.2566 - val_acc: 0.9010\n",
      "Epoch 169/1000\n",
      " - 4s - loss: 0.0996 - acc: 0.9844 - val_loss: 0.2564 - val_acc: 0.9010\n",
      "Epoch 170/1000\n",
      " - 3s - loss: 0.0977 - acc: 0.9847 - val_loss: 0.2562 - val_acc: 0.9010\n",
      "Epoch 171/1000\n",
      " - 3s - loss: 0.0973 - acc: 0.9858 - val_loss: 0.2559 - val_acc: 0.9000\n",
      "Epoch 172/1000\n",
      " - 3s - loss: 0.0962 - acc: 0.9847 - val_loss: 0.2557 - val_acc: 0.9010\n",
      "Epoch 173/1000\n",
      " - 3s - loss: 0.0973 - acc: 0.9842 - val_loss: 0.2555 - val_acc: 0.9010\n",
      "Epoch 174/1000\n",
      " - 3s - loss: 0.0953 - acc: 0.9849 - val_loss: 0.2553 - val_acc: 0.9010\n",
      "Epoch 175/1000\n",
      " - 3s - loss: 0.0965 - acc: 0.9831 - val_loss: 0.2552 - val_acc: 0.9010\n",
      "Epoch 176/1000\n",
      " - 3s - loss: 0.0955 - acc: 0.9850 - val_loss: 0.2549 - val_acc: 0.9000\n",
      "Epoch 177/1000\n",
      " - 3s - loss: 0.0930 - acc: 0.9849 - val_loss: 0.2547 - val_acc: 0.9010\n",
      "Epoch 178/1000\n",
      " - 3s - loss: 0.0933 - acc: 0.9862 - val_loss: 0.2545 - val_acc: 0.9000\n",
      "Epoch 179/1000\n",
      " - 3s - loss: 0.0918 - acc: 0.9871 - val_loss: 0.2543 - val_acc: 0.9000\n",
      "Epoch 180/1000\n",
      " - 3s - loss: 0.0931 - acc: 0.9849 - val_loss: 0.2541 - val_acc: 0.9000\n",
      "Epoch 181/1000\n",
      " - 3s - loss: 0.0925 - acc: 0.9842 - val_loss: 0.2540 - val_acc: 0.9010\n",
      "Epoch 182/1000\n",
      " - 3s - loss: 0.0912 - acc: 0.9857 - val_loss: 0.2538 - val_acc: 0.9010\n",
      "Epoch 183/1000\n",
      " - 3s - loss: 0.0905 - acc: 0.9854 - val_loss: 0.2537 - val_acc: 0.9010\n",
      "Epoch 184/1000\n",
      " - 3s - loss: 0.0907 - acc: 0.9852 - val_loss: 0.2535 - val_acc: 0.9010\n",
      "Epoch 185/1000\n",
      " - 3s - loss: 0.0911 - acc: 0.9872 - val_loss: 0.2533 - val_acc: 0.9010\n",
      "Epoch 186/1000\n",
      " - 3s - loss: 0.0885 - acc: 0.9863 - val_loss: 0.2532 - val_acc: 0.9010\n",
      "Epoch 187/1000\n",
      " - 3s - loss: 0.0884 - acc: 0.9871 - val_loss: 0.2530 - val_acc: 0.9020\n",
      "Epoch 188/1000\n",
      " - 3s - loss: 0.0887 - acc: 0.9860 - val_loss: 0.2529 - val_acc: 0.9020\n",
      "Epoch 189/1000\n",
      " - 3s - loss: 0.0891 - acc: 0.9859 - val_loss: 0.2528 - val_acc: 0.9020\n",
      "Epoch 190/1000\n",
      " - 3s - loss: 0.0873 - acc: 0.9871 - val_loss: 0.2527 - val_acc: 0.9020\n",
      "Epoch 191/1000\n",
      " - 3s - loss: 0.0881 - acc: 0.9866 - val_loss: 0.2525 - val_acc: 0.9030\n",
      "Epoch 192/1000\n",
      " - 3s - loss: 0.0860 - acc: 0.9871 - val_loss: 0.2524 - val_acc: 0.9030\n",
      "Epoch 193/1000\n",
      " - 3s - loss: 0.0864 - acc: 0.9868 - val_loss: 0.2523 - val_acc: 0.9030\n",
      "Epoch 194/1000\n",
      " - 3s - loss: 0.0879 - acc: 0.9862 - val_loss: 0.2522 - val_acc: 0.9020\n",
      "Epoch 195/1000\n",
      " - 3s - loss: 0.0844 - acc: 0.9871 - val_loss: 0.2520 - val_acc: 0.9030\n",
      "Epoch 196/1000\n",
      " - 3s - loss: 0.0852 - acc: 0.9864 - val_loss: 0.2520 - val_acc: 0.9020\n",
      "Epoch 197/1000\n",
      " - 3s - loss: 0.0832 - acc: 0.9870 - val_loss: 0.2519 - val_acc: 0.9020\n",
      "Epoch 198/1000\n",
      " - 3s - loss: 0.0833 - acc: 0.9870 - val_loss: 0.2518 - val_acc: 0.9020\n",
      "Epoch 199/1000\n",
      " - 3s - loss: 0.0829 - acc: 0.9858 - val_loss: 0.2516 - val_acc: 0.9010\n",
      "Epoch 200/1000\n",
      " - 3s - loss: 0.0815 - acc: 0.9887 - val_loss: 0.2515 - val_acc: 0.9010\n",
      "Epoch 201/1000\n",
      " - 3s - loss: 0.0827 - acc: 0.9879 - val_loss: 0.2514 - val_acc: 0.9010\n",
      "Epoch 202/1000\n",
      " - 3s - loss: 0.0826 - acc: 0.9877 - val_loss: 0.2512 - val_acc: 0.9010\n",
      "Epoch 203/1000\n",
      " - 3s - loss: 0.0821 - acc: 0.9859 - val_loss: 0.2511 - val_acc: 0.9000\n",
      "Epoch 204/1000\n",
      " - 3s - loss: 0.0810 - acc: 0.9894 - val_loss: 0.2511 - val_acc: 0.9000\n",
      "Epoch 205/1000\n",
      " - 3s - loss: 0.0801 - acc: 0.9874 - val_loss: 0.2509 - val_acc: 0.9010\n",
      "Epoch 206/1000\n",
      " - 3s - loss: 0.0799 - acc: 0.9870 - val_loss: 0.2508 - val_acc: 0.9010\n",
      "Epoch 207/1000\n",
      " - 3s - loss: 0.0800 - acc: 0.9886 - val_loss: 0.2508 - val_acc: 0.9010\n",
      "Epoch 208/1000\n",
      " - 3s - loss: 0.0803 - acc: 0.9867 - val_loss: 0.2506 - val_acc: 0.9020\n",
      "Epoch 209/1000\n",
      " - 3s - loss: 0.0785 - acc: 0.9886 - val_loss: 0.2506 - val_acc: 0.9020\n",
      "Epoch 210/1000\n",
      " - 3s - loss: 0.0778 - acc: 0.9882 - val_loss: 0.2505 - val_acc: 0.9020\n",
      "Epoch 211/1000\n",
      " - 3s - loss: 0.0778 - acc: 0.9884 - val_loss: 0.2504 - val_acc: 0.9020\n",
      "Epoch 212/1000\n",
      " - 3s - loss: 0.0779 - acc: 0.9882 - val_loss: 0.2504 - val_acc: 0.9020\n",
      "Epoch 213/1000\n",
      " - 3s - loss: 0.0757 - acc: 0.9890 - val_loss: 0.2502 - val_acc: 0.9020\n",
      "Epoch 214/1000\n",
      " - 3s - loss: 0.0755 - acc: 0.9908 - val_loss: 0.2501 - val_acc: 0.9020\n",
      "Epoch 215/1000\n",
      " - 3s - loss: 0.0751 - acc: 0.9899 - val_loss: 0.2501 - val_acc: 0.9020\n",
      "Epoch 216/1000\n",
      " - 3s - loss: 0.0754 - acc: 0.9883 - val_loss: 0.2499 - val_acc: 0.9010\n",
      "Epoch 217/1000\n",
      " - 3s - loss: 0.0748 - acc: 0.9896 - val_loss: 0.2499 - val_acc: 0.9020\n",
      "Epoch 218/1000\n",
      " - 3s - loss: 0.0754 - acc: 0.9884 - val_loss: 0.2499 - val_acc: 0.9000\n",
      "Epoch 219/1000\n",
      " - 3s - loss: 0.0744 - acc: 0.9888 - val_loss: 0.2498 - val_acc: 0.9000\n",
      "Epoch 220/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.0734 - acc: 0.9893 - val_loss: 0.2498 - val_acc: 0.9010\n",
      "Epoch 221/1000\n",
      " - 3s - loss: 0.0736 - acc: 0.9892 - val_loss: 0.2498 - val_acc: 0.9010\n",
      "Epoch 222/1000\n",
      " - 3s - loss: 0.0729 - acc: 0.9893 - val_loss: 0.2497 - val_acc: 0.9010\n",
      "Epoch 223/1000\n",
      " - 3s - loss: 0.0719 - acc: 0.9891 - val_loss: 0.2497 - val_acc: 0.9010\n",
      "Epoch 224/1000\n",
      " - 3s - loss: 0.0729 - acc: 0.9894 - val_loss: 0.2496 - val_acc: 0.9000\n",
      "Epoch 225/1000\n",
      " - 3s - loss: 0.0718 - acc: 0.9894 - val_loss: 0.2495 - val_acc: 0.9000\n",
      "Epoch 226/1000\n",
      " - 3s - loss: 0.0727 - acc: 0.9887 - val_loss: 0.2495 - val_acc: 0.9000\n",
      "Epoch 227/1000\n",
      " - 3s - loss: 0.0711 - acc: 0.9891 - val_loss: 0.2495 - val_acc: 0.9000\n",
      "Epoch 228/1000\n",
      " - 3s - loss: 0.0704 - acc: 0.9901 - val_loss: 0.2495 - val_acc: 0.9010\n",
      "Validation accuracy: 0.901, loss: 0.24949954986572265\n",
      "Accuracy: 0.901, Parameters: (layers=1, units=32)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 3s - loss: 0.6846 - acc: 0.7143 - val_loss: 0.6763 - val_acc: 0.7640\n",
      "Epoch 2/1000\n",
      " - 3s - loss: 0.6648 - acc: 0.8386 - val_loss: 0.6603 - val_acc: 0.8140\n",
      "Epoch 3/1000\n",
      " - 3s - loss: 0.6463 - acc: 0.8813 - val_loss: 0.6453 - val_acc: 0.8400\n",
      "Epoch 4/1000\n",
      " - 3s - loss: 0.6286 - acc: 0.9008 - val_loss: 0.6312 - val_acc: 0.8540\n",
      "Epoch 5/1000\n",
      " - 3s - loss: 0.6116 - acc: 0.9126 - val_loss: 0.6177 - val_acc: 0.8580\n",
      "Epoch 6/1000\n",
      " - 3s - loss: 0.5951 - acc: 0.9181 - val_loss: 0.6049 - val_acc: 0.8660\n",
      "Epoch 7/1000\n",
      " - 3s - loss: 0.5800 - acc: 0.9211 - val_loss: 0.5927 - val_acc: 0.8680\n",
      "Epoch 8/1000\n",
      " - 3s - loss: 0.5649 - acc: 0.9254 - val_loss: 0.5810 - val_acc: 0.8710\n",
      "Epoch 9/1000\n",
      " - 3s - loss: 0.5508 - acc: 0.9288 - val_loss: 0.5700 - val_acc: 0.8700\n",
      "Epoch 10/1000\n",
      " - 3s - loss: 0.5380 - acc: 0.9287 - val_loss: 0.5595 - val_acc: 0.8730\n",
      "Epoch 11/1000\n",
      " - 3s - loss: 0.5252 - acc: 0.9299 - val_loss: 0.5495 - val_acc: 0.8750\n",
      "Epoch 12/1000\n",
      " - 3s - loss: 0.5131 - acc: 0.9338 - val_loss: 0.5400 - val_acc: 0.8720\n",
      "Epoch 13/1000\n",
      " - 3s - loss: 0.5012 - acc: 0.9352 - val_loss: 0.5309 - val_acc: 0.8700\n",
      "Epoch 14/1000\n",
      " - 3s - loss: 0.4896 - acc: 0.9351 - val_loss: 0.5222 - val_acc: 0.8720\n",
      "Epoch 15/1000\n",
      " - 3s - loss: 0.4790 - acc: 0.9356 - val_loss: 0.5139 - val_acc: 0.8730\n",
      "Epoch 16/1000\n",
      " - 3s - loss: 0.4692 - acc: 0.9361 - val_loss: 0.5060 - val_acc: 0.8730\n",
      "Epoch 17/1000\n",
      " - 3s - loss: 0.4595 - acc: 0.9383 - val_loss: 0.4984 - val_acc: 0.8740\n",
      "Epoch 18/1000\n",
      " - 3s - loss: 0.4507 - acc: 0.9382 - val_loss: 0.4912 - val_acc: 0.8770\n",
      "Epoch 19/1000\n",
      " - 3s - loss: 0.4407 - acc: 0.9390 - val_loss: 0.4842 - val_acc: 0.8760\n",
      "Epoch 20/1000\n",
      " - 3s - loss: 0.4324 - acc: 0.9411 - val_loss: 0.4775 - val_acc: 0.8770\n",
      "Epoch 21/1000\n",
      " - 3s - loss: 0.4242 - acc: 0.9406 - val_loss: 0.4711 - val_acc: 0.8780\n",
      "Epoch 22/1000\n",
      " - 3s - loss: 0.4161 - acc: 0.9423 - val_loss: 0.4650 - val_acc: 0.8750\n",
      "Epoch 23/1000\n",
      " - 3s - loss: 0.4082 - acc: 0.9426 - val_loss: 0.4591 - val_acc: 0.8770\n",
      "Epoch 24/1000\n",
      " - 3s - loss: 0.4009 - acc: 0.9434 - val_loss: 0.4534 - val_acc: 0.8750\n",
      "Epoch 25/1000\n",
      " - 3s - loss: 0.3941 - acc: 0.9433 - val_loss: 0.4479 - val_acc: 0.8780\n",
      "Epoch 26/1000\n",
      " - 3s - loss: 0.3864 - acc: 0.9436 - val_loss: 0.4427 - val_acc: 0.8790\n",
      "Epoch 27/1000\n",
      " - 3s - loss: 0.3800 - acc: 0.9457 - val_loss: 0.4377 - val_acc: 0.8790\n",
      "Epoch 28/1000\n",
      " - 3s - loss: 0.3740 - acc: 0.9460 - val_loss: 0.4328 - val_acc: 0.8790\n",
      "Epoch 29/1000\n",
      " - 3s - loss: 0.3668 - acc: 0.9483 - val_loss: 0.4281 - val_acc: 0.8790\n",
      "Epoch 30/1000\n",
      " - 3s - loss: 0.3611 - acc: 0.9472 - val_loss: 0.4236 - val_acc: 0.8790\n",
      "Epoch 31/1000\n",
      " - 3s - loss: 0.3552 - acc: 0.9486 - val_loss: 0.4193 - val_acc: 0.8800\n",
      "Epoch 32/1000\n",
      " - 3s - loss: 0.3493 - acc: 0.9493 - val_loss: 0.4150 - val_acc: 0.8810\n",
      "Epoch 33/1000\n",
      " - 3s - loss: 0.3437 - acc: 0.9489 - val_loss: 0.4109 - val_acc: 0.8820\n",
      "Epoch 34/1000\n",
      " - 3s - loss: 0.3382 - acc: 0.9504 - val_loss: 0.4069 - val_acc: 0.8820\n",
      "Epoch 35/1000\n",
      " - 3s - loss: 0.3341 - acc: 0.9514 - val_loss: 0.4032 - val_acc: 0.8820\n",
      "Epoch 36/1000\n",
      " - 3s - loss: 0.3284 - acc: 0.9504 - val_loss: 0.3995 - val_acc: 0.8820\n",
      "Epoch 37/1000\n",
      " - 3s - loss: 0.3230 - acc: 0.9508 - val_loss: 0.3959 - val_acc: 0.8830\n",
      "Epoch 38/1000\n",
      " - 3s - loss: 0.3191 - acc: 0.9522 - val_loss: 0.3925 - val_acc: 0.8840\n",
      "Epoch 39/1000\n",
      " - 3s - loss: 0.3139 - acc: 0.9536 - val_loss: 0.3891 - val_acc: 0.8860\n",
      "Epoch 40/1000\n",
      " - 3s - loss: 0.3110 - acc: 0.9524 - val_loss: 0.3858 - val_acc: 0.8850\n",
      "Epoch 41/1000\n",
      " - 3s - loss: 0.3062 - acc: 0.9511 - val_loss: 0.3827 - val_acc: 0.8850\n",
      "Epoch 42/1000\n",
      " - 3s - loss: 0.3011 - acc: 0.9546 - val_loss: 0.3796 - val_acc: 0.8860\n",
      "Epoch 43/1000\n",
      " - 3s - loss: 0.2984 - acc: 0.9538 - val_loss: 0.3766 - val_acc: 0.8850\n",
      "Epoch 44/1000\n",
      " - 3s - loss: 0.2936 - acc: 0.9540 - val_loss: 0.3738 - val_acc: 0.8830\n",
      "Epoch 45/1000\n",
      " - 3s - loss: 0.2900 - acc: 0.9561 - val_loss: 0.3709 - val_acc: 0.8830\n",
      "Epoch 46/1000\n",
      " - 3s - loss: 0.2857 - acc: 0.9556 - val_loss: 0.3682 - val_acc: 0.8850\n",
      "Epoch 47/1000\n",
      " - 3s - loss: 0.2812 - acc: 0.9562 - val_loss: 0.3655 - val_acc: 0.8830\n",
      "Epoch 48/1000\n",
      " - 3s - loss: 0.2779 - acc: 0.9560 - val_loss: 0.3629 - val_acc: 0.8840\n",
      "Epoch 49/1000\n",
      " - 3s - loss: 0.2755 - acc: 0.9561 - val_loss: 0.3604 - val_acc: 0.8840\n",
      "Epoch 50/1000\n",
      " - 3s - loss: 0.2702 - acc: 0.9584 - val_loss: 0.3580 - val_acc: 0.8840\n",
      "Epoch 51/1000\n",
      " - 3s - loss: 0.2680 - acc: 0.9564 - val_loss: 0.3556 - val_acc: 0.8840\n",
      "Epoch 52/1000\n",
      " - 3s - loss: 0.2650 - acc: 0.9559 - val_loss: 0.3533 - val_acc: 0.8860\n",
      "Epoch 53/1000\n",
      " - 3s - loss: 0.2613 - acc: 0.9608 - val_loss: 0.3511 - val_acc: 0.8860\n",
      "Epoch 54/1000\n",
      " - 3s - loss: 0.2576 - acc: 0.9591 - val_loss: 0.3489 - val_acc: 0.8890\n",
      "Epoch 55/1000\n",
      " - 3s - loss: 0.2544 - acc: 0.9609 - val_loss: 0.3467 - val_acc: 0.8900\n",
      "Epoch 56/1000\n",
      " - 3s - loss: 0.2510 - acc: 0.9626 - val_loss: 0.3446 - val_acc: 0.8890\n",
      "Epoch 57/1000\n",
      " - 3s - loss: 0.2489 - acc: 0.9598 - val_loss: 0.3426 - val_acc: 0.8900\n",
      "Epoch 58/1000\n",
      " - 3s - loss: 0.2449 - acc: 0.9624 - val_loss: 0.3406 - val_acc: 0.8900\n",
      "Epoch 59/1000\n",
      " - 3s - loss: 0.2431 - acc: 0.9613 - val_loss: 0.3387 - val_acc: 0.8900\n",
      "Epoch 60/1000\n",
      " - 3s - loss: 0.2416 - acc: 0.9617 - val_loss: 0.3368 - val_acc: 0.8910\n",
      "Epoch 61/1000\n",
      " - 3s - loss: 0.2387 - acc: 0.9602 - val_loss: 0.3350 - val_acc: 0.8910\n",
      "Epoch 62/1000\n",
      " - 3s - loss: 0.2337 - acc: 0.9634 - val_loss: 0.3331 - val_acc: 0.8910\n",
      "Epoch 63/1000\n",
      " - 3s - loss: 0.2325 - acc: 0.9618 - val_loss: 0.3314 - val_acc: 0.8930\n",
      "Epoch 64/1000\n",
      " - 3s - loss: 0.2308 - acc: 0.9611 - val_loss: 0.3297 - val_acc: 0.8930\n",
      "Epoch 65/1000\n",
      " - 3s - loss: 0.2275 - acc: 0.9642 - val_loss: 0.3280 - val_acc: 0.8920\n",
      "Epoch 66/1000\n",
      " - 3s - loss: 0.2241 - acc: 0.9654 - val_loss: 0.3264 - val_acc: 0.8930\n",
      "Epoch 67/1000\n",
      " - 3s - loss: 0.2229 - acc: 0.9640 - val_loss: 0.3248 - val_acc: 0.8930\n",
      "Epoch 68/1000\n",
      " - 3s - loss: 0.2194 - acc: 0.9652 - val_loss: 0.3232 - val_acc: 0.8930\n",
      "Epoch 69/1000\n",
      " - 3s - loss: 0.2176 - acc: 0.9652 - val_loss: 0.3216 - val_acc: 0.8920\n",
      "Epoch 70/1000\n",
      " - 3s - loss: 0.2153 - acc: 0.9642 - val_loss: 0.3202 - val_acc: 0.8950\n",
      "Epoch 71/1000\n",
      " - 3s - loss: 0.2130 - acc: 0.9647 - val_loss: 0.3187 - val_acc: 0.8960\n",
      "Epoch 72/1000\n",
      " - 3s - loss: 0.2106 - acc: 0.9669 - val_loss: 0.3173 - val_acc: 0.8950\n",
      "Epoch 73/1000\n",
      " - 3s - loss: 0.2075 - acc: 0.9674 - val_loss: 0.3159 - val_acc: 0.8940\n",
      "Epoch 74/1000\n",
      " - 3s - loss: 0.2067 - acc: 0.9671 - val_loss: 0.3145 - val_acc: 0.8950\n",
      "Epoch 75/1000\n",
      " - 3s - loss: 0.2051 - acc: 0.9647 - val_loss: 0.3132 - val_acc: 0.8940\n",
      "Epoch 76/1000\n",
      " - 3s - loss: 0.2017 - acc: 0.9686 - val_loss: 0.3118 - val_acc: 0.8950\n",
      "Epoch 77/1000\n",
      " - 3s - loss: 0.1992 - acc: 0.9676 - val_loss: 0.3105 - val_acc: 0.8940\n",
      "Epoch 78/1000\n",
      " - 3s - loss: 0.1974 - acc: 0.9690 - val_loss: 0.3092 - val_acc: 0.8940\n",
      "Epoch 79/1000\n",
      " - 3s - loss: 0.1948 - acc: 0.9707 - val_loss: 0.3080 - val_acc: 0.8950\n",
      "Epoch 80/1000\n",
      " - 3s - loss: 0.1952 - acc: 0.9682 - val_loss: 0.3068 - val_acc: 0.8960\n",
      "Epoch 81/1000\n",
      " - 3s - loss: 0.1923 - acc: 0.9690 - val_loss: 0.3056 - val_acc: 0.8960\n",
      "Epoch 82/1000\n",
      " - 3s - loss: 0.1919 - acc: 0.9682 - val_loss: 0.3044 - val_acc: 0.8960\n",
      "Epoch 83/1000\n",
      " - 3s - loss: 0.1906 - acc: 0.9677 - val_loss: 0.3033 - val_acc: 0.8960\n",
      "Epoch 84/1000\n",
      " - 3s - loss: 0.1874 - acc: 0.9681 - val_loss: 0.3022 - val_acc: 0.8970\n",
      "Epoch 85/1000\n",
      " - 3s - loss: 0.1850 - acc: 0.9702 - val_loss: 0.3011 - val_acc: 0.8980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/1000\n",
      " - 3s - loss: 0.1828 - acc: 0.9714 - val_loss: 0.3000 - val_acc: 0.8980\n",
      "Epoch 87/1000\n",
      " - 3s - loss: 0.1821 - acc: 0.9717 - val_loss: 0.2989 - val_acc: 0.8980\n",
      "Epoch 88/1000\n",
      " - 3s - loss: 0.1814 - acc: 0.9699 - val_loss: 0.2978 - val_acc: 0.8980\n",
      "Epoch 89/1000\n",
      " - 3s - loss: 0.1792 - acc: 0.9689 - val_loss: 0.2969 - val_acc: 0.8990\n",
      "Epoch 90/1000\n",
      " - 3s - loss: 0.1768 - acc: 0.9708 - val_loss: 0.2958 - val_acc: 0.8990\n",
      "Epoch 91/1000\n",
      " - 3s - loss: 0.1765 - acc: 0.9707 - val_loss: 0.2948 - val_acc: 0.8990\n",
      "Epoch 92/1000\n",
      " - 3s - loss: 0.1744 - acc: 0.9711 - val_loss: 0.2939 - val_acc: 0.8990\n",
      "Epoch 93/1000\n",
      " - 3s - loss: 0.1727 - acc: 0.9733 - val_loss: 0.2930 - val_acc: 0.8990\n",
      "Epoch 94/1000\n",
      " - 3s - loss: 0.1699 - acc: 0.9724 - val_loss: 0.2921 - val_acc: 0.8990\n",
      "Epoch 95/1000\n",
      " - 3s - loss: 0.1689 - acc: 0.9724 - val_loss: 0.2912 - val_acc: 0.8990\n",
      "Epoch 96/1000\n",
      " - 3s - loss: 0.1686 - acc: 0.9704 - val_loss: 0.2904 - val_acc: 0.8990\n",
      "Epoch 97/1000\n",
      " - 3s - loss: 0.1664 - acc: 0.9740 - val_loss: 0.2895 - val_acc: 0.8990\n",
      "Epoch 98/1000\n",
      " - 3s - loss: 0.1658 - acc: 0.9720 - val_loss: 0.2887 - val_acc: 0.8990\n",
      "Epoch 99/1000\n",
      " - 3s - loss: 0.1636 - acc: 0.9734 - val_loss: 0.2878 - val_acc: 0.8990\n",
      "Epoch 100/1000\n",
      " - 3s - loss: 0.1628 - acc: 0.9730 - val_loss: 0.2870 - val_acc: 0.8990\n",
      "Epoch 101/1000\n",
      " - 3s - loss: 0.1611 - acc: 0.9729 - val_loss: 0.2863 - val_acc: 0.8980\n",
      "Epoch 102/1000\n",
      " - 3s - loss: 0.1590 - acc: 0.9741 - val_loss: 0.2855 - val_acc: 0.8970\n",
      "Epoch 103/1000\n",
      " - 3s - loss: 0.1602 - acc: 0.9748 - val_loss: 0.2847 - val_acc: 0.8980\n",
      "Epoch 104/1000\n",
      " - 3s - loss: 0.1561 - acc: 0.9742 - val_loss: 0.2840 - val_acc: 0.8980\n",
      "Epoch 105/1000\n",
      " - 3s - loss: 0.1554 - acc: 0.9720 - val_loss: 0.2832 - val_acc: 0.8970\n",
      "Epoch 106/1000\n",
      " - 3s - loss: 0.1546 - acc: 0.9747 - val_loss: 0.2825 - val_acc: 0.8960\n",
      "Epoch 107/1000\n",
      " - 3s - loss: 0.1535 - acc: 0.9744 - val_loss: 0.2818 - val_acc: 0.8980\n",
      "Epoch 108/1000\n",
      " - 3s - loss: 0.1523 - acc: 0.9744 - val_loss: 0.2811 - val_acc: 0.8980\n",
      "Epoch 109/1000\n",
      " - 3s - loss: 0.1500 - acc: 0.9749 - val_loss: 0.2804 - val_acc: 0.8980\n",
      "Epoch 110/1000\n",
      " - 3s - loss: 0.1504 - acc: 0.9758 - val_loss: 0.2797 - val_acc: 0.8980\n",
      "Epoch 111/1000\n",
      " - 3s - loss: 0.1490 - acc: 0.9753 - val_loss: 0.2791 - val_acc: 0.8980\n",
      "Epoch 112/1000\n",
      " - 3s - loss: 0.1469 - acc: 0.9753 - val_loss: 0.2784 - val_acc: 0.8980\n",
      "Epoch 113/1000\n",
      " - 3s - loss: 0.1479 - acc: 0.9764 - val_loss: 0.2778 - val_acc: 0.8980\n",
      "Epoch 114/1000\n",
      " - 3s - loss: 0.1438 - acc: 0.9778 - val_loss: 0.2772 - val_acc: 0.8980\n",
      "Epoch 115/1000\n",
      " - 3s - loss: 0.1445 - acc: 0.9763 - val_loss: 0.2766 - val_acc: 0.8980\n",
      "Epoch 116/1000\n",
      " - 3s - loss: 0.1435 - acc: 0.9762 - val_loss: 0.2760 - val_acc: 0.8980\n",
      "Epoch 117/1000\n",
      " - 3s - loss: 0.1420 - acc: 0.9764 - val_loss: 0.2754 - val_acc: 0.8980\n",
      "Epoch 118/1000\n",
      " - 3s - loss: 0.1394 - acc: 0.9780 - val_loss: 0.2749 - val_acc: 0.8950\n",
      "Epoch 119/1000\n",
      " - 3s - loss: 0.1406 - acc: 0.9777 - val_loss: 0.2744 - val_acc: 0.8960\n",
      "Epoch 120/1000\n",
      " - 3s - loss: 0.1366 - acc: 0.9793 - val_loss: 0.2738 - val_acc: 0.8960\n",
      "Epoch 121/1000\n",
      " - 3s - loss: 0.1364 - acc: 0.9781 - val_loss: 0.2733 - val_acc: 0.8960\n",
      "Epoch 122/1000\n",
      " - 3s - loss: 0.1370 - acc: 0.9776 - val_loss: 0.2727 - val_acc: 0.8980\n",
      "Epoch 123/1000\n",
      " - 3s - loss: 0.1347 - acc: 0.9778 - val_loss: 0.2722 - val_acc: 0.8980\n",
      "Epoch 124/1000\n",
      " - 3s - loss: 0.1334 - acc: 0.9799 - val_loss: 0.2717 - val_acc: 0.8970\n",
      "Epoch 125/1000\n",
      " - 3s - loss: 0.1332 - acc: 0.9779 - val_loss: 0.2712 - val_acc: 0.8980\n",
      "Epoch 126/1000\n",
      " - 3s - loss: 0.1314 - acc: 0.9799 - val_loss: 0.2708 - val_acc: 0.8980\n",
      "Epoch 127/1000\n",
      " - 3s - loss: 0.1299 - acc: 0.9786 - val_loss: 0.2703 - val_acc: 0.8980\n",
      "Epoch 128/1000\n",
      " - 3s - loss: 0.1301 - acc: 0.9789 - val_loss: 0.2698 - val_acc: 0.8980\n",
      "Epoch 129/1000\n",
      " - 3s - loss: 0.1285 - acc: 0.9808 - val_loss: 0.2693 - val_acc: 0.8980\n",
      "Epoch 130/1000\n",
      " - 3s - loss: 0.1289 - acc: 0.9783 - val_loss: 0.2689 - val_acc: 0.8980\n",
      "Epoch 131/1000\n",
      " - 3s - loss: 0.1272 - acc: 0.9794 - val_loss: 0.2684 - val_acc: 0.8980\n",
      "Epoch 132/1000\n",
      " - 3s - loss: 0.1265 - acc: 0.9798 - val_loss: 0.2680 - val_acc: 0.8970\n",
      "Epoch 133/1000\n",
      " - 3s - loss: 0.1260 - acc: 0.9801 - val_loss: 0.2676 - val_acc: 0.8970\n",
      "Epoch 134/1000\n",
      " - 3s - loss: 0.1250 - acc: 0.9801 - val_loss: 0.2672 - val_acc: 0.8970\n",
      "Epoch 135/1000\n",
      " - 3s - loss: 0.1237 - acc: 0.9781 - val_loss: 0.2668 - val_acc: 0.8970\n",
      "Epoch 136/1000\n",
      " - 3s - loss: 0.1228 - acc: 0.9807 - val_loss: 0.2664 - val_acc: 0.8970\n",
      "Epoch 137/1000\n",
      " - 3s - loss: 0.1216 - acc: 0.9798 - val_loss: 0.2660 - val_acc: 0.8970\n",
      "Epoch 138/1000\n",
      " - 3s - loss: 0.1221 - acc: 0.9799 - val_loss: 0.2657 - val_acc: 0.8970\n",
      "Epoch 139/1000\n",
      " - 3s - loss: 0.1205 - acc: 0.9803 - val_loss: 0.2653 - val_acc: 0.8970\n",
      "Epoch 140/1000\n",
      " - 3s - loss: 0.1197 - acc: 0.9807 - val_loss: 0.2649 - val_acc: 0.8970\n",
      "Epoch 141/1000\n",
      " - 3s - loss: 0.1190 - acc: 0.9792 - val_loss: 0.2646 - val_acc: 0.8970\n",
      "Epoch 142/1000\n",
      " - 3s - loss: 0.1169 - acc: 0.9812 - val_loss: 0.2642 - val_acc: 0.8970\n",
      "Epoch 143/1000\n",
      " - 3s - loss: 0.1174 - acc: 0.9814 - val_loss: 0.2639 - val_acc: 0.8970\n",
      "Epoch 144/1000\n",
      " - 3s - loss: 0.1166 - acc: 0.9799 - val_loss: 0.2635 - val_acc: 0.8970\n",
      "Epoch 145/1000\n",
      " - 3s - loss: 0.1145 - acc: 0.9809 - val_loss: 0.2632 - val_acc: 0.8970\n",
      "Epoch 146/1000\n",
      " - 4s - loss: 0.1146 - acc: 0.9808 - val_loss: 0.2629 - val_acc: 0.8970\n",
      "Epoch 147/1000\n",
      " - 4s - loss: 0.1155 - acc: 0.9816 - val_loss: 0.2625 - val_acc: 0.8970\n",
      "Epoch 148/1000\n",
      " - 4s - loss: 0.1146 - acc: 0.9816 - val_loss: 0.2622 - val_acc: 0.8970\n",
      "Epoch 149/1000\n",
      " - 3s - loss: 0.1131 - acc: 0.9819 - val_loss: 0.2619 - val_acc: 0.8970\n",
      "Epoch 150/1000\n",
      " - 3s - loss: 0.1110 - acc: 0.9818 - val_loss: 0.2616 - val_acc: 0.8960\n",
      "Epoch 151/1000\n",
      " - 3s - loss: 0.1115 - acc: 0.9813 - val_loss: 0.2613 - val_acc: 0.8960\n",
      "Epoch 152/1000\n",
      " - 3s - loss: 0.1117 - acc: 0.9804 - val_loss: 0.2610 - val_acc: 0.8960\n",
      "Epoch 153/1000\n",
      " - 3s - loss: 0.1108 - acc: 0.9814 - val_loss: 0.2607 - val_acc: 0.8960\n",
      "Epoch 154/1000\n",
      " - 3s - loss: 0.1092 - acc: 0.9826 - val_loss: 0.2604 - val_acc: 0.8960\n",
      "Epoch 155/1000\n",
      " - 3s - loss: 0.1098 - acc: 0.9820 - val_loss: 0.2601 - val_acc: 0.8960\n",
      "Epoch 156/1000\n",
      " - 3s - loss: 0.1074 - acc: 0.9829 - val_loss: 0.2599 - val_acc: 0.8970\n",
      "Epoch 157/1000\n",
      " - 3s - loss: 0.1064 - acc: 0.9838 - val_loss: 0.2596 - val_acc: 0.8980\n",
      "Epoch 158/1000\n",
      " - 3s - loss: 0.1074 - acc: 0.9823 - val_loss: 0.2593 - val_acc: 0.8980\n",
      "Epoch 159/1000\n",
      " - 3s - loss: 0.1060 - acc: 0.9823 - val_loss: 0.2591 - val_acc: 0.8980\n",
      "Epoch 160/1000\n",
      " - 3s - loss: 0.1037 - acc: 0.9832 - val_loss: 0.2588 - val_acc: 0.8990\n",
      "Epoch 161/1000\n",
      " - 3s - loss: 0.1047 - acc: 0.9831 - val_loss: 0.2585 - val_acc: 0.8990\n",
      "Epoch 162/1000\n",
      " - 3s - loss: 0.1043 - acc: 0.9842 - val_loss: 0.2583 - val_acc: 0.8990\n",
      "Epoch 163/1000\n",
      " - 3s - loss: 0.1033 - acc: 0.9837 - val_loss: 0.2580 - val_acc: 0.8990\n",
      "Epoch 164/1000\n",
      " - 3s - loss: 0.1014 - acc: 0.9841 - val_loss: 0.2578 - val_acc: 0.8990\n",
      "Epoch 165/1000\n",
      " - 3s - loss: 0.1024 - acc: 0.9843 - val_loss: 0.2576 - val_acc: 0.9000\n",
      "Epoch 166/1000\n",
      " - 3s - loss: 0.1007 - acc: 0.9839 - val_loss: 0.2573 - val_acc: 0.9000\n",
      "Epoch 167/1000\n",
      " - 3s - loss: 0.0993 - acc: 0.9851 - val_loss: 0.2572 - val_acc: 0.9000\n",
      "Epoch 168/1000\n",
      " - 3s - loss: 0.1006 - acc: 0.9837 - val_loss: 0.2570 - val_acc: 0.9000\n",
      "Epoch 169/1000\n",
      " - 3s - loss: 0.1000 - acc: 0.9840 - val_loss: 0.2568 - val_acc: 0.9000\n",
      "Epoch 170/1000\n",
      " - 3s - loss: 0.0980 - acc: 0.9841 - val_loss: 0.2565 - val_acc: 0.9000\n",
      "Epoch 171/1000\n",
      " - 3s - loss: 0.0966 - acc: 0.9858 - val_loss: 0.2563 - val_acc: 0.9000\n",
      "Epoch 172/1000\n",
      " - 3s - loss: 0.0983 - acc: 0.9838 - val_loss: 0.2561 - val_acc: 0.9000\n",
      "Epoch 173/1000\n",
      " - 3s - loss: 0.0966 - acc: 0.9842 - val_loss: 0.2559 - val_acc: 0.9010\n",
      "Epoch 174/1000\n",
      " - 3s - loss: 0.0964 - acc: 0.9853 - val_loss: 0.2558 - val_acc: 0.9010\n",
      "Epoch 175/1000\n",
      " - 3s - loss: 0.0956 - acc: 0.9843 - val_loss: 0.2556 - val_acc: 0.9010\n",
      "Epoch 176/1000\n",
      " - 3s - loss: 0.0962 - acc: 0.9836 - val_loss: 0.2554 - val_acc: 0.9010\n",
      "Epoch 177/1000\n",
      " - 3s - loss: 0.0934 - acc: 0.9863 - val_loss: 0.2551 - val_acc: 0.9020\n",
      "Epoch 178/1000\n",
      " - 3s - loss: 0.0937 - acc: 0.9839 - val_loss: 0.2549 - val_acc: 0.9020\n",
      "Epoch 179/1000\n",
      " - 3s - loss: 0.0937 - acc: 0.9853 - val_loss: 0.2548 - val_acc: 0.9020\n",
      "Epoch 180/1000\n",
      " - 3s - loss: 0.0926 - acc: 0.9861 - val_loss: 0.2546 - val_acc: 0.9020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/1000\n",
      " - 3s - loss: 0.0919 - acc: 0.9854 - val_loss: 0.2544 - val_acc: 0.9020\n",
      "Epoch 182/1000\n",
      " - 3s - loss: 0.0937 - acc: 0.9840 - val_loss: 0.2542 - val_acc: 0.9020\n",
      "Epoch 183/1000\n",
      " - 3s - loss: 0.0923 - acc: 0.9854 - val_loss: 0.2541 - val_acc: 0.9020\n",
      "Epoch 184/1000\n",
      " - 3s - loss: 0.0916 - acc: 0.9866 - val_loss: 0.2540 - val_acc: 0.9020\n",
      "Epoch 185/1000\n",
      " - 3s - loss: 0.0906 - acc: 0.9851 - val_loss: 0.2538 - val_acc: 0.9020\n",
      "Epoch 186/1000\n",
      " - 3s - loss: 0.0888 - acc: 0.9857 - val_loss: 0.2536 - val_acc: 0.9020\n",
      "Epoch 187/1000\n",
      " - 3s - loss: 0.0881 - acc: 0.9870 - val_loss: 0.2534 - val_acc: 0.9020\n",
      "Epoch 188/1000\n",
      " - 3s - loss: 0.0888 - acc: 0.9862 - val_loss: 0.2533 - val_acc: 0.9020\n",
      "Epoch 189/1000\n",
      " - 3s - loss: 0.0896 - acc: 0.9848 - val_loss: 0.2532 - val_acc: 0.9020\n",
      "Epoch 190/1000\n",
      " - 3s - loss: 0.0883 - acc: 0.9863 - val_loss: 0.2531 - val_acc: 0.9010\n",
      "Epoch 191/1000\n",
      " - 3s - loss: 0.0865 - acc: 0.9861 - val_loss: 0.2530 - val_acc: 0.9020\n",
      "Epoch 192/1000\n",
      " - 3s - loss: 0.0877 - acc: 0.9861 - val_loss: 0.2528 - val_acc: 0.9020\n",
      "Epoch 193/1000\n",
      " - 3s - loss: 0.0861 - acc: 0.9873 - val_loss: 0.2526 - val_acc: 0.9020\n",
      "Epoch 194/1000\n",
      " - 3s - loss: 0.0850 - acc: 0.9864 - val_loss: 0.2525 - val_acc: 0.9020\n",
      "Epoch 195/1000\n",
      " - 3s - loss: 0.0855 - acc: 0.9859 - val_loss: 0.2524 - val_acc: 0.9010\n",
      "Epoch 196/1000\n",
      " - 3s - loss: 0.0848 - acc: 0.9873 - val_loss: 0.2523 - val_acc: 0.9020\n",
      "Epoch 197/1000\n",
      " - 3s - loss: 0.0839 - acc: 0.9862 - val_loss: 0.2522 - val_acc: 0.9010\n",
      "Epoch 198/1000\n",
      " - 3s - loss: 0.0842 - acc: 0.9863 - val_loss: 0.2521 - val_acc: 0.9010\n",
      "Epoch 199/1000\n",
      " - 3s - loss: 0.0843 - acc: 0.9861 - val_loss: 0.2520 - val_acc: 0.9010\n",
      "Epoch 200/1000\n",
      " - 3s - loss: 0.0826 - acc: 0.9866 - val_loss: 0.2519 - val_acc: 0.9000\n",
      "Epoch 201/1000\n",
      " - 3s - loss: 0.0829 - acc: 0.9869 - val_loss: 0.2518 - val_acc: 0.9000\n",
      "Epoch 202/1000\n",
      " - 3s - loss: 0.0811 - acc: 0.9870 - val_loss: 0.2517 - val_acc: 0.9000\n",
      "Epoch 203/1000\n",
      " - 3s - loss: 0.0815 - acc: 0.9879 - val_loss: 0.2516 - val_acc: 0.9000\n",
      "Epoch 204/1000\n",
      " - 3s - loss: 0.0797 - acc: 0.9873 - val_loss: 0.2515 - val_acc: 0.9000\n",
      "Epoch 205/1000\n",
      " - 3s - loss: 0.0802 - acc: 0.9878 - val_loss: 0.2514 - val_acc: 0.9000\n",
      "Epoch 206/1000\n",
      " - 3s - loss: 0.0799 - acc: 0.9889 - val_loss: 0.2514 - val_acc: 0.9000\n",
      "Epoch 207/1000\n",
      " - 3s - loss: 0.0797 - acc: 0.9860 - val_loss: 0.2513 - val_acc: 0.9000\n",
      "Epoch 208/1000\n",
      " - 3s - loss: 0.0784 - acc: 0.9883 - val_loss: 0.2512 - val_acc: 0.9000\n",
      "Epoch 209/1000\n",
      " - 3s - loss: 0.0775 - acc: 0.9887 - val_loss: 0.2512 - val_acc: 0.9000\n",
      "Epoch 210/1000\n",
      " - 3s - loss: 0.0774 - acc: 0.9877 - val_loss: 0.2511 - val_acc: 0.9000\n",
      "Epoch 211/1000\n",
      " - 3s - loss: 0.0782 - acc: 0.9887 - val_loss: 0.2510 - val_acc: 0.9000\n",
      "Epoch 212/1000\n",
      " - 3s - loss: 0.0784 - acc: 0.9886 - val_loss: 0.2509 - val_acc: 0.8990\n",
      "Epoch 213/1000\n",
      " - 3s - loss: 0.0773 - acc: 0.9887 - val_loss: 0.2508 - val_acc: 0.8990\n",
      "Epoch 214/1000\n",
      " - 3s - loss: 0.0757 - acc: 0.9898 - val_loss: 0.2507 - val_acc: 0.8990\n",
      "Epoch 215/1000\n",
      " - 3s - loss: 0.0750 - acc: 0.9889 - val_loss: 0.2507 - val_acc: 0.8990\n",
      "Epoch 216/1000\n",
      " - 3s - loss: 0.0760 - acc: 0.9880 - val_loss: 0.2506 - val_acc: 0.9000\n",
      "Epoch 217/1000\n",
      " - 3s - loss: 0.0750 - acc: 0.9890 - val_loss: 0.2505 - val_acc: 0.8990\n",
      "Epoch 218/1000\n",
      " - 3s - loss: 0.0754 - acc: 0.9882 - val_loss: 0.2505 - val_acc: 0.8990\n",
      "Epoch 219/1000\n",
      " - 3s - loss: 0.0751 - acc: 0.9886 - val_loss: 0.2504 - val_acc: 0.8990\n",
      "Epoch 220/1000\n",
      " - 3s - loss: 0.0741 - acc: 0.9897 - val_loss: 0.2504 - val_acc: 0.8990\n",
      "Epoch 221/1000\n",
      " - 3s - loss: 0.0745 - acc: 0.9884 - val_loss: 0.2503 - val_acc: 0.8990\n",
      "Epoch 222/1000\n",
      " - 3s - loss: 0.0736 - acc: 0.9888 - val_loss: 0.2502 - val_acc: 0.8990\n",
      "Epoch 223/1000\n",
      " - 3s - loss: 0.0722 - acc: 0.9899 - val_loss: 0.2502 - val_acc: 0.9000\n",
      "Epoch 224/1000\n",
      " - 3s - loss: 0.0730 - acc: 0.9894 - val_loss: 0.2501 - val_acc: 0.8990\n",
      "Epoch 225/1000\n",
      " - 3s - loss: 0.0729 - acc: 0.9887 - val_loss: 0.2501 - val_acc: 0.9000\n",
      "Epoch 226/1000\n",
      " - 3s - loss: 0.0718 - acc: 0.9904 - val_loss: 0.2500 - val_acc: 0.9010\n",
      "Epoch 227/1000\n",
      " - 3s - loss: 0.0727 - acc: 0.9894 - val_loss: 0.2500 - val_acc: 0.9010\n",
      "Epoch 228/1000\n",
      " - 3s - loss: 0.0710 - acc: 0.9887 - val_loss: 0.2500 - val_acc: 0.9010\n",
      "Epoch 229/1000\n",
      " - 3s - loss: 0.0711 - acc: 0.9897 - val_loss: 0.2499 - val_acc: 0.9010\n",
      "Epoch 230/1000\n",
      " - 3s - loss: 0.0709 - acc: 0.9891 - val_loss: 0.2499 - val_acc: 0.9000\n",
      "Epoch 231/1000\n",
      " - 3s - loss: 0.0700 - acc: 0.9907 - val_loss: 0.2499 - val_acc: 0.9000\n",
      "Epoch 232/1000\n",
      " - 3s - loss: 0.0698 - acc: 0.9891 - val_loss: 0.2499 - val_acc: 0.9000\n",
      "Epoch 233/1000\n",
      " - 3s - loss: 0.0686 - acc: 0.9909 - val_loss: 0.2499 - val_acc: 0.9000\n",
      "Validation accuracy: 0.9, loss: 0.24991402840614318\n",
      "Accuracy: 0.9, Parameters: (layers=1, units=64)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 3s - loss: 0.6847 - acc: 0.7332 - val_loss: 0.6763 - val_acc: 0.7680\n",
      "Epoch 2/1000\n",
      " - 3s - loss: 0.6650 - acc: 0.8414 - val_loss: 0.6603 - val_acc: 0.8050\n",
      "Epoch 3/1000\n",
      " - 3s - loss: 0.6462 - acc: 0.8829 - val_loss: 0.6453 - val_acc: 0.8290\n",
      "Epoch 4/1000\n",
      " - 3s - loss: 0.6284 - acc: 0.8971 - val_loss: 0.6311 - val_acc: 0.8510\n",
      "Epoch 5/1000\n",
      " - 3s - loss: 0.6115 - acc: 0.9099 - val_loss: 0.6175 - val_acc: 0.8530\n",
      "Epoch 6/1000\n",
      " - 3s - loss: 0.5957 - acc: 0.9173 - val_loss: 0.6049 - val_acc: 0.8640\n",
      "Epoch 7/1000\n",
      " - 3s - loss: 0.5801 - acc: 0.9209 - val_loss: 0.5926 - val_acc: 0.8690\n",
      "Epoch 8/1000\n",
      " - 3s - loss: 0.5655 - acc: 0.9238 - val_loss: 0.5812 - val_acc: 0.8720\n",
      "Epoch 9/1000\n",
      " - 3s - loss: 0.5517 - acc: 0.9282 - val_loss: 0.5701 - val_acc: 0.8730\n",
      "Epoch 10/1000\n",
      " - 3s - loss: 0.5375 - acc: 0.9318 - val_loss: 0.5596 - val_acc: 0.8700\n",
      "Epoch 11/1000\n",
      " - 3s - loss: 0.5256 - acc: 0.9309 - val_loss: 0.5496 - val_acc: 0.8700\n",
      "Epoch 12/1000\n",
      " - 3s - loss: 0.5136 - acc: 0.9314 - val_loss: 0.5400 - val_acc: 0.8710\n",
      "Epoch 13/1000\n",
      " - 3s - loss: 0.5015 - acc: 0.9332 - val_loss: 0.5309 - val_acc: 0.8720\n",
      "Epoch 14/1000\n",
      " - 3s - loss: 0.4901 - acc: 0.9352 - val_loss: 0.5223 - val_acc: 0.8710\n",
      "Epoch 15/1000\n",
      " - 3s - loss: 0.4796 - acc: 0.9357 - val_loss: 0.5140 - val_acc: 0.8720\n",
      "Epoch 16/1000\n",
      " - 3s - loss: 0.4698 - acc: 0.9386 - val_loss: 0.5061 - val_acc: 0.8740\n",
      "Epoch 17/1000\n",
      " - 3s - loss: 0.4584 - acc: 0.9393 - val_loss: 0.4985 - val_acc: 0.8740\n",
      "Epoch 18/1000\n",
      " - 3s - loss: 0.4504 - acc: 0.9398 - val_loss: 0.4912 - val_acc: 0.8760\n",
      "Epoch 19/1000\n",
      " - 3s - loss: 0.4410 - acc: 0.9381 - val_loss: 0.4843 - val_acc: 0.8760\n",
      "Epoch 20/1000\n",
      " - 3s - loss: 0.4330 - acc: 0.9380 - val_loss: 0.4776 - val_acc: 0.8770\n",
      "Epoch 21/1000\n",
      " - 3s - loss: 0.4250 - acc: 0.9402 - val_loss: 0.4712 - val_acc: 0.8750\n",
      "Epoch 22/1000\n",
      " - 3s - loss: 0.4161 - acc: 0.9428 - val_loss: 0.4651 - val_acc: 0.8770\n",
      "Epoch 23/1000\n",
      " - 3s - loss: 0.4088 - acc: 0.9430 - val_loss: 0.4592 - val_acc: 0.8770\n",
      "Epoch 24/1000\n",
      " - 3s - loss: 0.4013 - acc: 0.9418 - val_loss: 0.4536 - val_acc: 0.8770\n",
      "Epoch 25/1000\n",
      " - 3s - loss: 0.3944 - acc: 0.9446 - val_loss: 0.4481 - val_acc: 0.8780\n",
      "Epoch 26/1000\n",
      " - 3s - loss: 0.3876 - acc: 0.9438 - val_loss: 0.4429 - val_acc: 0.8780\n",
      "Epoch 27/1000\n",
      " - 3s - loss: 0.3806 - acc: 0.9447 - val_loss: 0.4379 - val_acc: 0.8810\n",
      "Epoch 28/1000\n",
      " - 3s - loss: 0.3738 - acc: 0.9474 - val_loss: 0.4329 - val_acc: 0.8790\n",
      "Epoch 29/1000\n",
      " - 3s - loss: 0.3668 - acc: 0.9472 - val_loss: 0.4281 - val_acc: 0.8790\n",
      "Epoch 30/1000\n",
      " - 3s - loss: 0.3612 - acc: 0.9458 - val_loss: 0.4236 - val_acc: 0.8800\n",
      "Epoch 31/1000\n",
      " - 3s - loss: 0.3552 - acc: 0.9489 - val_loss: 0.4192 - val_acc: 0.8800\n",
      "Epoch 32/1000\n",
      " - 3s - loss: 0.3502 - acc: 0.9501 - val_loss: 0.4150 - val_acc: 0.8820\n",
      "Epoch 33/1000\n",
      " - 3s - loss: 0.3442 - acc: 0.9491 - val_loss: 0.4109 - val_acc: 0.8820\n",
      "Epoch 34/1000\n",
      " - 3s - loss: 0.3390 - acc: 0.9508 - val_loss: 0.4070 - val_acc: 0.8820\n",
      "Epoch 35/1000\n",
      " - 3s - loss: 0.3331 - acc: 0.9509 - val_loss: 0.4031 - val_acc: 0.8810\n",
      "Epoch 36/1000\n",
      " - 3s - loss: 0.3285 - acc: 0.9519 - val_loss: 0.3995 - val_acc: 0.8820\n",
      "Epoch 37/1000\n",
      " - 3s - loss: 0.3229 - acc: 0.9491 - val_loss: 0.3959 - val_acc: 0.8820\n",
      "Epoch 38/1000\n",
      " - 3s - loss: 0.3197 - acc: 0.9502 - val_loss: 0.3924 - val_acc: 0.8820\n",
      "Epoch 39/1000\n",
      " - 3s - loss: 0.3146 - acc: 0.9522 - val_loss: 0.3890 - val_acc: 0.8830\n",
      "Epoch 40/1000\n",
      " - 3s - loss: 0.3102 - acc: 0.9550 - val_loss: 0.3858 - val_acc: 0.8840\n",
      "Epoch 41/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.3051 - acc: 0.9526 - val_loss: 0.3826 - val_acc: 0.8840\n",
      "Epoch 42/1000\n",
      " - 3s - loss: 0.3011 - acc: 0.9533 - val_loss: 0.3795 - val_acc: 0.8840\n",
      "Epoch 43/1000\n",
      " - 3s - loss: 0.2980 - acc: 0.9547 - val_loss: 0.3765 - val_acc: 0.8840\n",
      "Epoch 44/1000\n",
      " - 3s - loss: 0.2929 - acc: 0.9563 - val_loss: 0.3737 - val_acc: 0.8850\n",
      "Epoch 45/1000\n",
      " - 3s - loss: 0.2898 - acc: 0.9556 - val_loss: 0.3709 - val_acc: 0.8850\n",
      "Epoch 46/1000\n",
      " - 3s - loss: 0.2849 - acc: 0.9562 - val_loss: 0.3682 - val_acc: 0.8860\n",
      "Epoch 47/1000\n",
      " - 3s - loss: 0.2812 - acc: 0.9566 - val_loss: 0.3655 - val_acc: 0.8860\n",
      "Epoch 48/1000\n",
      " - 3s - loss: 0.2785 - acc: 0.9568 - val_loss: 0.3629 - val_acc: 0.8860\n",
      "Epoch 49/1000\n",
      " - 3s - loss: 0.2736 - acc: 0.9558 - val_loss: 0.3604 - val_acc: 0.8840\n",
      "Epoch 50/1000\n",
      " - 3s - loss: 0.2713 - acc: 0.9569 - val_loss: 0.3580 - val_acc: 0.8840\n",
      "Epoch 51/1000\n",
      " - 3s - loss: 0.2684 - acc: 0.9570 - val_loss: 0.3556 - val_acc: 0.8850\n",
      "Epoch 52/1000\n",
      " - 3s - loss: 0.2650 - acc: 0.9587 - val_loss: 0.3533 - val_acc: 0.8850\n",
      "Epoch 53/1000\n",
      " - 3s - loss: 0.2624 - acc: 0.9578 - val_loss: 0.3510 - val_acc: 0.8870\n",
      "Epoch 54/1000\n",
      " - 3s - loss: 0.2567 - acc: 0.9609 - val_loss: 0.3488 - val_acc: 0.8870\n",
      "Epoch 55/1000\n",
      " - 3s - loss: 0.2539 - acc: 0.9619 - val_loss: 0.3467 - val_acc: 0.8870\n",
      "Epoch 56/1000\n",
      " - 3s - loss: 0.2520 - acc: 0.9603 - val_loss: 0.3446 - val_acc: 0.8870\n",
      "Epoch 57/1000\n",
      " - 3s - loss: 0.2488 - acc: 0.9596 - val_loss: 0.3425 - val_acc: 0.8900\n",
      "Epoch 58/1000\n",
      " - 3s - loss: 0.2455 - acc: 0.9596 - val_loss: 0.3405 - val_acc: 0.8890\n",
      "Epoch 59/1000\n",
      " - 3s - loss: 0.2440 - acc: 0.9614 - val_loss: 0.3386 - val_acc: 0.8900\n",
      "Epoch 60/1000\n",
      " - 3s - loss: 0.2400 - acc: 0.9628 - val_loss: 0.3367 - val_acc: 0.8910\n",
      "Epoch 61/1000\n",
      " - 3s - loss: 0.2378 - acc: 0.9619 - val_loss: 0.3349 - val_acc: 0.8910\n",
      "Epoch 62/1000\n",
      " - 3s - loss: 0.2342 - acc: 0.9636 - val_loss: 0.3331 - val_acc: 0.8910\n",
      "Epoch 63/1000\n",
      " - 3s - loss: 0.2328 - acc: 0.9623 - val_loss: 0.3313 - val_acc: 0.8930\n",
      "Epoch 64/1000\n",
      " - 3s - loss: 0.2296 - acc: 0.9653 - val_loss: 0.3296 - val_acc: 0.8920\n",
      "Epoch 65/1000\n",
      " - 3s - loss: 0.2273 - acc: 0.9629 - val_loss: 0.3280 - val_acc: 0.8920\n",
      "Epoch 66/1000\n",
      " - 3s - loss: 0.2245 - acc: 0.9639 - val_loss: 0.3264 - val_acc: 0.8920\n",
      "Epoch 67/1000\n",
      " - 3s - loss: 0.2222 - acc: 0.9651 - val_loss: 0.3247 - val_acc: 0.8920\n",
      "Epoch 68/1000\n",
      " - 3s - loss: 0.2193 - acc: 0.9640 - val_loss: 0.3232 - val_acc: 0.8920\n",
      "Epoch 69/1000\n",
      " - 3s - loss: 0.2179 - acc: 0.9648 - val_loss: 0.3217 - val_acc: 0.8920\n",
      "Epoch 70/1000\n",
      " - 3s - loss: 0.2149 - acc: 0.9663 - val_loss: 0.3201 - val_acc: 0.8930\n",
      "Epoch 71/1000\n",
      " - 3s - loss: 0.2131 - acc: 0.9676 - val_loss: 0.3187 - val_acc: 0.8930\n",
      "Epoch 72/1000\n",
      " - 3s - loss: 0.2117 - acc: 0.9667 - val_loss: 0.3172 - val_acc: 0.8940\n",
      "Epoch 73/1000\n",
      " - 3s - loss: 0.2086 - acc: 0.9647 - val_loss: 0.3158 - val_acc: 0.8950\n",
      "Epoch 74/1000\n",
      " - 3s - loss: 0.2064 - acc: 0.9669 - val_loss: 0.3145 - val_acc: 0.8950\n",
      "Epoch 75/1000\n",
      " - 3s - loss: 0.2050 - acc: 0.9661 - val_loss: 0.3131 - val_acc: 0.8960\n",
      "Epoch 76/1000\n",
      " - 3s - loss: 0.2030 - acc: 0.9662 - val_loss: 0.3119 - val_acc: 0.8960\n",
      "Epoch 77/1000\n",
      " - 3s - loss: 0.2005 - acc: 0.9691 - val_loss: 0.3105 - val_acc: 0.8960\n",
      "Epoch 78/1000\n",
      " - 3s - loss: 0.1998 - acc: 0.9670 - val_loss: 0.3093 - val_acc: 0.8960\n",
      "Epoch 79/1000\n",
      " - 3s - loss: 0.1969 - acc: 0.9678 - val_loss: 0.3081 - val_acc: 0.8960\n",
      "Epoch 80/1000\n",
      " - 3s - loss: 0.1956 - acc: 0.9668 - val_loss: 0.3068 - val_acc: 0.8960\n",
      "Epoch 81/1000\n",
      " - 3s - loss: 0.1931 - acc: 0.9697 - val_loss: 0.3057 - val_acc: 0.8970\n",
      "Epoch 82/1000\n",
      " - 3s - loss: 0.1915 - acc: 0.9690 - val_loss: 0.3045 - val_acc: 0.8980\n",
      "Epoch 83/1000\n",
      " - 3s - loss: 0.1897 - acc: 0.9694 - val_loss: 0.3034 - val_acc: 0.8980\n",
      "Epoch 84/1000\n",
      " - 3s - loss: 0.1873 - acc: 0.9696 - val_loss: 0.3023 - val_acc: 0.8980\n",
      "Epoch 85/1000\n",
      " - 3s - loss: 0.1856 - acc: 0.9693 - val_loss: 0.3012 - val_acc: 0.8980\n",
      "Epoch 86/1000\n",
      " - 3s - loss: 0.1835 - acc: 0.9693 - val_loss: 0.3001 - val_acc: 0.8980\n",
      "Epoch 87/1000\n",
      " - 3s - loss: 0.1808 - acc: 0.9716 - val_loss: 0.2991 - val_acc: 0.8970\n",
      "Epoch 88/1000\n",
      " - 3s - loss: 0.1812 - acc: 0.9704 - val_loss: 0.2981 - val_acc: 0.8970\n",
      "Epoch 89/1000\n",
      " - 3s - loss: 0.1790 - acc: 0.9701 - val_loss: 0.2971 - val_acc: 0.8970\n",
      "Epoch 90/1000\n",
      " - 3s - loss: 0.1769 - acc: 0.9717 - val_loss: 0.2961 - val_acc: 0.8970\n",
      "Epoch 91/1000\n",
      " - 3s - loss: 0.1762 - acc: 0.9703 - val_loss: 0.2951 - val_acc: 0.8970\n",
      "Epoch 92/1000\n",
      " - 3s - loss: 0.1731 - acc: 0.9734 - val_loss: 0.2942 - val_acc: 0.8990\n",
      "Epoch 93/1000\n",
      " - 3s - loss: 0.1726 - acc: 0.9692 - val_loss: 0.2932 - val_acc: 0.8990\n",
      "Epoch 94/1000\n",
      " - 3s - loss: 0.1711 - acc: 0.9716 - val_loss: 0.2923 - val_acc: 0.8980\n",
      "Epoch 95/1000\n",
      " - 3s - loss: 0.1693 - acc: 0.9719 - val_loss: 0.2915 - val_acc: 0.8980\n",
      "Epoch 96/1000\n",
      " - 3s - loss: 0.1682 - acc: 0.9730 - val_loss: 0.2906 - val_acc: 0.8990\n",
      "Epoch 97/1000\n",
      " - 3s - loss: 0.1670 - acc: 0.9722 - val_loss: 0.2898 - val_acc: 0.8980\n",
      "Epoch 98/1000\n",
      " - 3s - loss: 0.1655 - acc: 0.9723 - val_loss: 0.2890 - val_acc: 0.8980\n",
      "Epoch 99/1000\n",
      " - 3s - loss: 0.1633 - acc: 0.9738 - val_loss: 0.2882 - val_acc: 0.8970\n",
      "Epoch 100/1000\n",
      " - 3s - loss: 0.1626 - acc: 0.9721 - val_loss: 0.2874 - val_acc: 0.8980\n",
      "Epoch 101/1000\n",
      " - 3s - loss: 0.1598 - acc: 0.9744 - val_loss: 0.2866 - val_acc: 0.8960\n",
      "Epoch 102/1000\n",
      " - 3s - loss: 0.1600 - acc: 0.9730 - val_loss: 0.2858 - val_acc: 0.8950\n",
      "Epoch 103/1000\n",
      " - 3s - loss: 0.1587 - acc: 0.9723 - val_loss: 0.2850 - val_acc: 0.8980\n",
      "Epoch 104/1000\n",
      " - 3s - loss: 0.1566 - acc: 0.9742 - val_loss: 0.2843 - val_acc: 0.8950\n",
      "Epoch 105/1000\n",
      " - 3s - loss: 0.1545 - acc: 0.9747 - val_loss: 0.2836 - val_acc: 0.8950\n",
      "Epoch 106/1000\n",
      " - 3s - loss: 0.1547 - acc: 0.9757 - val_loss: 0.2828 - val_acc: 0.8950\n",
      "Epoch 107/1000\n",
      " - 3s - loss: 0.1527 - acc: 0.9741 - val_loss: 0.2821 - val_acc: 0.8950\n",
      "Epoch 108/1000\n",
      " - 3s - loss: 0.1525 - acc: 0.9773 - val_loss: 0.2815 - val_acc: 0.8950\n",
      "Epoch 109/1000\n",
      " - 3s - loss: 0.1504 - acc: 0.9761 - val_loss: 0.2807 - val_acc: 0.8970\n",
      "Epoch 110/1000\n",
      " - 3s - loss: 0.1492 - acc: 0.9760 - val_loss: 0.2801 - val_acc: 0.8980\n",
      "Epoch 111/1000\n",
      " - 3s - loss: 0.1485 - acc: 0.9758 - val_loss: 0.2794 - val_acc: 0.8960\n",
      "Epoch 112/1000\n",
      " - 3s - loss: 0.1476 - acc: 0.9774 - val_loss: 0.2787 - val_acc: 0.8970\n",
      "Epoch 113/1000\n",
      " - 3s - loss: 0.1459 - acc: 0.9761 - val_loss: 0.2781 - val_acc: 0.8970\n",
      "Epoch 114/1000\n",
      " - 3s - loss: 0.1436 - acc: 0.9776 - val_loss: 0.2775 - val_acc: 0.8970\n",
      "Epoch 115/1000\n",
      " - 3s - loss: 0.1439 - acc: 0.9756 - val_loss: 0.2769 - val_acc: 0.8970\n",
      "Epoch 116/1000\n",
      " - 3s - loss: 0.1424 - acc: 0.9774 - val_loss: 0.2763 - val_acc: 0.8970\n",
      "Epoch 117/1000\n",
      " - 3s - loss: 0.1419 - acc: 0.9764 - val_loss: 0.2757 - val_acc: 0.8970\n",
      "Epoch 118/1000\n",
      " - 3s - loss: 0.1400 - acc: 0.9778 - val_loss: 0.2751 - val_acc: 0.8950\n",
      "Epoch 119/1000\n",
      " - 3s - loss: 0.1392 - acc: 0.9779 - val_loss: 0.2746 - val_acc: 0.8950\n",
      "Epoch 120/1000\n",
      " - 3s - loss: 0.1378 - acc: 0.9788 - val_loss: 0.2740 - val_acc: 0.8950\n",
      "Epoch 121/1000\n",
      " - 3s - loss: 0.1369 - acc: 0.9787 - val_loss: 0.2735 - val_acc: 0.8950\n",
      "Epoch 122/1000\n",
      " - 3s - loss: 0.1357 - acc: 0.9772 - val_loss: 0.2730 - val_acc: 0.8950\n",
      "Epoch 123/1000\n",
      " - 3s - loss: 0.1359 - acc: 0.9768 - val_loss: 0.2724 - val_acc: 0.8950\n",
      "Epoch 124/1000\n",
      " - 3s - loss: 0.1344 - acc: 0.9776 - val_loss: 0.2719 - val_acc: 0.8950\n",
      "Epoch 125/1000\n",
      " - 3s - loss: 0.1331 - acc: 0.9783 - val_loss: 0.2714 - val_acc: 0.8960\n",
      "Epoch 126/1000\n",
      " - 3s - loss: 0.1314 - acc: 0.9791 - val_loss: 0.2709 - val_acc: 0.8960\n",
      "Epoch 127/1000\n",
      " - 3s - loss: 0.1309 - acc: 0.9779 - val_loss: 0.2704 - val_acc: 0.8960\n",
      "Epoch 128/1000\n",
      " - 3s - loss: 0.1307 - acc: 0.9781 - val_loss: 0.2699 - val_acc: 0.8960\n",
      "Epoch 129/1000\n",
      " - 3s - loss: 0.1300 - acc: 0.9798 - val_loss: 0.2695 - val_acc: 0.8960\n",
      "Epoch 130/1000\n",
      " - 3s - loss: 0.1280 - acc: 0.9796 - val_loss: 0.2690 - val_acc: 0.8960\n",
      "Epoch 131/1000\n",
      " - 3s - loss: 0.1281 - acc: 0.9798 - val_loss: 0.2685 - val_acc: 0.8960\n",
      "Epoch 132/1000\n",
      " - 3s - loss: 0.1275 - acc: 0.9786 - val_loss: 0.2681 - val_acc: 0.8960\n",
      "Epoch 133/1000\n",
      " - 3s - loss: 0.1259 - acc: 0.9779 - val_loss: 0.2677 - val_acc: 0.8960\n",
      "Epoch 134/1000\n",
      " - 3s - loss: 0.1247 - acc: 0.9801 - val_loss: 0.2673 - val_acc: 0.8960\n",
      "Epoch 135/1000\n",
      " - 3s - loss: 0.1232 - acc: 0.9804 - val_loss: 0.2669 - val_acc: 0.8960\n",
      "Epoch 136/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.1237 - acc: 0.9799 - val_loss: 0.2666 - val_acc: 0.8960\n",
      "Epoch 137/1000\n",
      " - 3s - loss: 0.1222 - acc: 0.9801 - val_loss: 0.2661 - val_acc: 0.8960\n",
      "Epoch 138/1000\n",
      " - 3s - loss: 0.1199 - acc: 0.9816 - val_loss: 0.2657 - val_acc: 0.8960\n",
      "Epoch 139/1000\n",
      " - 3s - loss: 0.1200 - acc: 0.9810 - val_loss: 0.2653 - val_acc: 0.8960\n",
      "Epoch 140/1000\n",
      " - 3s - loss: 0.1197 - acc: 0.9798 - val_loss: 0.2649 - val_acc: 0.8960\n",
      "Epoch 141/1000\n",
      " - 3s - loss: 0.1181 - acc: 0.9810 - val_loss: 0.2646 - val_acc: 0.8960\n",
      "Epoch 142/1000\n",
      " - 3s - loss: 0.1188 - acc: 0.9804 - val_loss: 0.2642 - val_acc: 0.8960\n",
      "Epoch 143/1000\n",
      " - 3s - loss: 0.1168 - acc: 0.9810 - val_loss: 0.2638 - val_acc: 0.8960\n",
      "Epoch 144/1000\n",
      " - 3s - loss: 0.1161 - acc: 0.9818 - val_loss: 0.2635 - val_acc: 0.8970\n",
      "Epoch 145/1000\n",
      " - 3s - loss: 0.1155 - acc: 0.9819 - val_loss: 0.2631 - val_acc: 0.8970\n",
      "Epoch 146/1000\n",
      " - 3s - loss: 0.1143 - acc: 0.9809 - val_loss: 0.2628 - val_acc: 0.8970\n",
      "Epoch 147/1000\n",
      " - 3s - loss: 0.1148 - acc: 0.9819 - val_loss: 0.2624 - val_acc: 0.8970\n",
      "Epoch 148/1000\n",
      " - 3s - loss: 0.1139 - acc: 0.9814 - val_loss: 0.2621 - val_acc: 0.8970\n",
      "Epoch 149/1000\n",
      " - 3s - loss: 0.1129 - acc: 0.9823 - val_loss: 0.2618 - val_acc: 0.8970\n",
      "Epoch 150/1000\n",
      " - 3s - loss: 0.1116 - acc: 0.9823 - val_loss: 0.2614 - val_acc: 0.8970\n",
      "Epoch 151/1000\n",
      " - 3s - loss: 0.1100 - acc: 0.9828 - val_loss: 0.2611 - val_acc: 0.8970\n",
      "Epoch 152/1000\n",
      " - 3s - loss: 0.1099 - acc: 0.9844 - val_loss: 0.2607 - val_acc: 0.8970\n",
      "Epoch 153/1000\n",
      " - 3s - loss: 0.1090 - acc: 0.9818 - val_loss: 0.2604 - val_acc: 0.8990\n",
      "Epoch 154/1000\n",
      " - 3s - loss: 0.1094 - acc: 0.9814 - val_loss: 0.2602 - val_acc: 0.8980\n",
      "Epoch 155/1000\n",
      " - 3s - loss: 0.1092 - acc: 0.9823 - val_loss: 0.2599 - val_acc: 0.8980\n",
      "Epoch 156/1000\n",
      " - 3s - loss: 0.1074 - acc: 0.9824 - val_loss: 0.2596 - val_acc: 0.8980\n",
      "Epoch 157/1000\n",
      " - 3s - loss: 0.1069 - acc: 0.9826 - val_loss: 0.2592 - val_acc: 0.8980\n",
      "Epoch 158/1000\n",
      " - 3s - loss: 0.1075 - acc: 0.9818 - val_loss: 0.2590 - val_acc: 0.8980\n",
      "Epoch 159/1000\n",
      " - 3s - loss: 0.1073 - acc: 0.9816 - val_loss: 0.2587 - val_acc: 0.8980\n",
      "Epoch 160/1000\n",
      " - 3s - loss: 0.1057 - acc: 0.9822 - val_loss: 0.2585 - val_acc: 0.8970\n",
      "Epoch 161/1000\n",
      " - 3s - loss: 0.1036 - acc: 0.9830 - val_loss: 0.2583 - val_acc: 0.8970\n",
      "Epoch 162/1000\n",
      " - 3s - loss: 0.1043 - acc: 0.9833 - val_loss: 0.2580 - val_acc: 0.8980\n",
      "Epoch 163/1000\n",
      " - 3s - loss: 0.1030 - acc: 0.9823 - val_loss: 0.2578 - val_acc: 0.8990\n",
      "Epoch 164/1000\n",
      " - 3s - loss: 0.1021 - acc: 0.9837 - val_loss: 0.2576 - val_acc: 0.8980\n",
      "Epoch 165/1000\n",
      " - 3s - loss: 0.1011 - acc: 0.9831 - val_loss: 0.2573 - val_acc: 0.8980\n",
      "Epoch 166/1000\n",
      " - 3s - loss: 0.1001 - acc: 0.9850 - val_loss: 0.2571 - val_acc: 0.8990\n",
      "Epoch 167/1000\n",
      " - 3s - loss: 0.0991 - acc: 0.9859 - val_loss: 0.2568 - val_acc: 0.8990\n",
      "Epoch 168/1000\n",
      " - 3s - loss: 0.0995 - acc: 0.9834 - val_loss: 0.2567 - val_acc: 0.8990\n",
      "Epoch 169/1000\n",
      " - 3s - loss: 0.1004 - acc: 0.9833 - val_loss: 0.2564 - val_acc: 0.8980\n",
      "Epoch 170/1000\n",
      " - 3s - loss: 0.0980 - acc: 0.9840 - val_loss: 0.2562 - val_acc: 0.8980\n",
      "Epoch 171/1000\n",
      " - 3s - loss: 0.0978 - acc: 0.9844 - val_loss: 0.2561 - val_acc: 0.8970\n",
      "Epoch 172/1000\n",
      " - 3s - loss: 0.0987 - acc: 0.9831 - val_loss: 0.2559 - val_acc: 0.8980\n",
      "Epoch 173/1000\n",
      " - 3s - loss: 0.0961 - acc: 0.9847 - val_loss: 0.2557 - val_acc: 0.8980\n",
      "Epoch 174/1000\n",
      " - 3s - loss: 0.0966 - acc: 0.9840 - val_loss: 0.2554 - val_acc: 0.8970\n",
      "Epoch 175/1000\n",
      " - 3s - loss: 0.0956 - acc: 0.9837 - val_loss: 0.2552 - val_acc: 0.8970\n",
      "Epoch 176/1000\n",
      " - 3s - loss: 0.0957 - acc: 0.9850 - val_loss: 0.2550 - val_acc: 0.8980\n",
      "Epoch 177/1000\n",
      " - 3s - loss: 0.0941 - acc: 0.9849 - val_loss: 0.2548 - val_acc: 0.8990\n",
      "Epoch 178/1000\n",
      " - 3s - loss: 0.0947 - acc: 0.9851 - val_loss: 0.2546 - val_acc: 0.8990\n",
      "Epoch 179/1000\n",
      " - 3s - loss: 0.0929 - acc: 0.9857 - val_loss: 0.2544 - val_acc: 0.8990\n",
      "Epoch 180/1000\n",
      " - 3s - loss: 0.0930 - acc: 0.9854 - val_loss: 0.2542 - val_acc: 0.8990\n",
      "Epoch 181/1000\n",
      " - 3s - loss: 0.0932 - acc: 0.9842 - val_loss: 0.2541 - val_acc: 0.8990\n",
      "Epoch 182/1000\n",
      " - 3s - loss: 0.0915 - acc: 0.9873 - val_loss: 0.2540 - val_acc: 0.8990\n",
      "Epoch 183/1000\n",
      " - 3s - loss: 0.0908 - acc: 0.9869 - val_loss: 0.2537 - val_acc: 0.8990\n",
      "Epoch 184/1000\n",
      " - 3s - loss: 0.0909 - acc: 0.9857 - val_loss: 0.2536 - val_acc: 0.8990\n",
      "Epoch 185/1000\n",
      " - 3s - loss: 0.0903 - acc: 0.9853 - val_loss: 0.2535 - val_acc: 0.8990\n",
      "Epoch 186/1000\n",
      " - 3s - loss: 0.0896 - acc: 0.9867 - val_loss: 0.2533 - val_acc: 0.8990\n",
      "Epoch 187/1000\n",
      " - 3s - loss: 0.0878 - acc: 0.9862 - val_loss: 0.2532 - val_acc: 0.8990\n",
      "Epoch 188/1000\n",
      " - 3s - loss: 0.0888 - acc: 0.9867 - val_loss: 0.2530 - val_acc: 0.8990\n",
      "Epoch 189/1000\n",
      " - 3s - loss: 0.0880 - acc: 0.9868 - val_loss: 0.2529 - val_acc: 0.8990\n",
      "Epoch 190/1000\n",
      " - 3s - loss: 0.0873 - acc: 0.9867 - val_loss: 0.2528 - val_acc: 0.8990\n",
      "Epoch 191/1000\n",
      " - 3s - loss: 0.0885 - acc: 0.9843 - val_loss: 0.2526 - val_acc: 0.8990\n",
      "Epoch 192/1000\n",
      " - 3s - loss: 0.0863 - acc: 0.9872 - val_loss: 0.2526 - val_acc: 0.8990\n",
      "Epoch 193/1000\n",
      " - 3s - loss: 0.0845 - acc: 0.9879 - val_loss: 0.2524 - val_acc: 0.8990\n",
      "Epoch 194/1000\n",
      " - 3s - loss: 0.0863 - acc: 0.9858 - val_loss: 0.2523 - val_acc: 0.8990\n",
      "Epoch 195/1000\n",
      " - 3s - loss: 0.0835 - acc: 0.9876 - val_loss: 0.2522 - val_acc: 0.8990\n",
      "Epoch 196/1000\n",
      " - 3s - loss: 0.0833 - acc: 0.9877 - val_loss: 0.2521 - val_acc: 0.8990\n",
      "Epoch 197/1000\n",
      " - 3s - loss: 0.0844 - acc: 0.9876 - val_loss: 0.2519 - val_acc: 0.9000\n",
      "Epoch 198/1000\n",
      " - 3s - loss: 0.0842 - acc: 0.9859 - val_loss: 0.2519 - val_acc: 0.8990\n",
      "Epoch 199/1000\n",
      " - 3s - loss: 0.0831 - acc: 0.9869 - val_loss: 0.2517 - val_acc: 0.9000\n",
      "Epoch 200/1000\n",
      " - 3s - loss: 0.0827 - acc: 0.9877 - val_loss: 0.2516 - val_acc: 0.8990\n",
      "Epoch 201/1000\n",
      " - 3s - loss: 0.0816 - acc: 0.9876 - val_loss: 0.2515 - val_acc: 0.9000\n",
      "Epoch 202/1000\n",
      " - 3s - loss: 0.0806 - acc: 0.9876 - val_loss: 0.2514 - val_acc: 0.9000\n",
      "Epoch 203/1000\n",
      " - 3s - loss: 0.0825 - acc: 0.9860 - val_loss: 0.2513 - val_acc: 0.8990\n",
      "Epoch 204/1000\n",
      " - 3s - loss: 0.0793 - acc: 0.9888 - val_loss: 0.2513 - val_acc: 0.9000\n",
      "Epoch 205/1000\n",
      " - 3s - loss: 0.0814 - acc: 0.9880 - val_loss: 0.2511 - val_acc: 0.8990\n",
      "Epoch 206/1000\n",
      " - 3s - loss: 0.0799 - acc: 0.9886 - val_loss: 0.2511 - val_acc: 0.8990\n",
      "Epoch 207/1000\n",
      " - 3s - loss: 0.0795 - acc: 0.9886 - val_loss: 0.2511 - val_acc: 0.8990\n",
      "Epoch 208/1000\n",
      " - 3s - loss: 0.0795 - acc: 0.9873 - val_loss: 0.2509 - val_acc: 0.8990\n",
      "Epoch 209/1000\n",
      " - 3s - loss: 0.0786 - acc: 0.9884 - val_loss: 0.2509 - val_acc: 0.8990\n",
      "Epoch 210/1000\n",
      " - 3s - loss: 0.0790 - acc: 0.9866 - val_loss: 0.2509 - val_acc: 0.8990\n",
      "Epoch 211/1000\n",
      " - 3s - loss: 0.0794 - acc: 0.9878 - val_loss: 0.2508 - val_acc: 0.8990\n",
      "Epoch 212/1000\n",
      " - 3s - loss: 0.0768 - acc: 0.9886 - val_loss: 0.2507 - val_acc: 0.8990\n",
      "Epoch 213/1000\n",
      " - 3s - loss: 0.0777 - acc: 0.9886 - val_loss: 0.2507 - val_acc: 0.8990\n",
      "Epoch 214/1000\n",
      " - 3s - loss: 0.0780 - acc: 0.9878 - val_loss: 0.2506 - val_acc: 0.8990\n",
      "Epoch 215/1000\n",
      " - 3s - loss: 0.0768 - acc: 0.9879 - val_loss: 0.2505 - val_acc: 0.8990\n",
      "Epoch 216/1000\n",
      " - 3s - loss: 0.0763 - acc: 0.9884 - val_loss: 0.2505 - val_acc: 0.8990\n",
      "Epoch 217/1000\n",
      " - 3s - loss: 0.0755 - acc: 0.9876 - val_loss: 0.2504 - val_acc: 0.8990\n",
      "Epoch 218/1000\n",
      " - 3s - loss: 0.0748 - acc: 0.9892 - val_loss: 0.2504 - val_acc: 0.9000\n",
      "Epoch 219/1000\n",
      " - 3s - loss: 0.0748 - acc: 0.9890 - val_loss: 0.2503 - val_acc: 0.8990\n",
      "Epoch 220/1000\n",
      " - 3s - loss: 0.0736 - acc: 0.9873 - val_loss: 0.2502 - val_acc: 0.9010\n",
      "Epoch 221/1000\n",
      " - 3s - loss: 0.0741 - acc: 0.9883 - val_loss: 0.2501 - val_acc: 0.9000\n",
      "Epoch 222/1000\n",
      " - 3s - loss: 0.0733 - acc: 0.9896 - val_loss: 0.2501 - val_acc: 0.9010\n",
      "Epoch 223/1000\n",
      " - 3s - loss: 0.0724 - acc: 0.9891 - val_loss: 0.2500 - val_acc: 0.9010\n",
      "Epoch 224/1000\n",
      " - 3s - loss: 0.0725 - acc: 0.9892 - val_loss: 0.2500 - val_acc: 0.9010\n",
      "Epoch 225/1000\n",
      " - 3s - loss: 0.0718 - acc: 0.9896 - val_loss: 0.2499 - val_acc: 0.9010\n",
      "Epoch 226/1000\n",
      " - 3s - loss: 0.0726 - acc: 0.9893 - val_loss: 0.2499 - val_acc: 0.9010\n",
      "Epoch 227/1000\n",
      " - 3s - loss: 0.0714 - acc: 0.9904 - val_loss: 0.2498 - val_acc: 0.9020\n",
      "Epoch 228/1000\n",
      " - 3s - loss: 0.0716 - acc: 0.9898 - val_loss: 0.2498 - val_acc: 0.9000\n",
      "Epoch 229/1000\n",
      " - 3s - loss: 0.0695 - acc: 0.9894 - val_loss: 0.2497 - val_acc: 0.9000\n",
      "Epoch 230/1000\n",
      " - 3s - loss: 0.0695 - acc: 0.9903 - val_loss: 0.2496 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/1000\n",
      " - 3s - loss: 0.0704 - acc: 0.9894 - val_loss: 0.2496 - val_acc: 0.9000\n",
      "Epoch 232/1000\n",
      " - 3s - loss: 0.0701 - acc: 0.9890 - val_loss: 0.2496 - val_acc: 0.9000\n",
      "Epoch 233/1000\n",
      " - 3s - loss: 0.0693 - acc: 0.9894 - val_loss: 0.2496 - val_acc: 0.9000\n",
      "Epoch 234/1000\n",
      " - 3s - loss: 0.0693 - acc: 0.9901 - val_loss: 0.2496 - val_acc: 0.9000\n",
      "Epoch 235/1000\n",
      " - 3s - loss: 0.0684 - acc: 0.9904 - val_loss: 0.2496 - val_acc: 0.9000\n",
      "Epoch 236/1000\n",
      " - 3s - loss: 0.0683 - acc: 0.9901 - val_loss: 0.2495 - val_acc: 0.9000\n",
      "Epoch 237/1000\n",
      " - 3s - loss: 0.0681 - acc: 0.9900 - val_loss: 0.2494 - val_acc: 0.9000\n",
      "Epoch 238/1000\n",
      " - 3s - loss: 0.0684 - acc: 0.9898 - val_loss: 0.2494 - val_acc: 0.9000\n",
      "Epoch 239/1000\n",
      " - 3s - loss: 0.0671 - acc: 0.9899 - val_loss: 0.2495 - val_acc: 0.9000\n",
      "Epoch 240/1000\n",
      " - 3s - loss: 0.0680 - acc: 0.9898 - val_loss: 0.2494 - val_acc: 0.9010\n",
      "Validation accuracy: 0.9009999976158142, loss: 0.24936785173416137\n",
      "Accuracy: 0.9009999976158142, Parameters: (layers=1, units=128)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 4s - loss: 0.6702 - acc: 0.7706 - val_loss: 0.6411 - val_acc: 0.8740\n",
      "Epoch 2/1000\n",
      " - 3s - loss: 0.5975 - acc: 0.8949 - val_loss: 0.5734 - val_acc: 0.8730\n",
      "Epoch 3/1000\n",
      " - 3s - loss: 0.5152 - acc: 0.9046 - val_loss: 0.5052 - val_acc: 0.8770\n",
      "Epoch 4/1000\n",
      " - 3s - loss: 0.4374 - acc: 0.9171 - val_loss: 0.4463 - val_acc: 0.8790\n",
      "Epoch 5/1000\n",
      " - 3s - loss: 0.3733 - acc: 0.9222 - val_loss: 0.3998 - val_acc: 0.8850\n",
      "Epoch 6/1000\n",
      " - 3s - loss: 0.3244 - acc: 0.9300 - val_loss: 0.3644 - val_acc: 0.8850\n",
      "Epoch 7/1000\n",
      " - 3s - loss: 0.2828 - acc: 0.9398 - val_loss: 0.3378 - val_acc: 0.8940\n",
      "Epoch 8/1000\n",
      " - 3s - loss: 0.2492 - acc: 0.9469 - val_loss: 0.3171 - val_acc: 0.8880\n",
      "Epoch 9/1000\n",
      " - 3s - loss: 0.2248 - acc: 0.9496 - val_loss: 0.3010 - val_acc: 0.8930\n",
      "Epoch 10/1000\n",
      " - 3s - loss: 0.2053 - acc: 0.9501 - val_loss: 0.2891 - val_acc: 0.8930\n",
      "Epoch 11/1000\n",
      " - 3s - loss: 0.1850 - acc: 0.9571 - val_loss: 0.2794 - val_acc: 0.8980\n",
      "Epoch 12/1000\n",
      " - 3s - loss: 0.1704 - acc: 0.9597 - val_loss: 0.2712 - val_acc: 0.8980\n",
      "Epoch 13/1000\n",
      " - 3s - loss: 0.1557 - acc: 0.9639 - val_loss: 0.2658 - val_acc: 0.8960\n",
      "Epoch 14/1000\n",
      " - 3s - loss: 0.1445 - acc: 0.9686 - val_loss: 0.2609 - val_acc: 0.9010\n",
      "Epoch 15/1000\n",
      " - 3s - loss: 0.1359 - acc: 0.9699 - val_loss: 0.2573 - val_acc: 0.8990\n",
      "Epoch 16/1000\n",
      " - 3s - loss: 0.1266 - acc: 0.9709 - val_loss: 0.2542 - val_acc: 0.8950\n",
      "Epoch 17/1000\n",
      " - 3s - loss: 0.1180 - acc: 0.9726 - val_loss: 0.2514 - val_acc: 0.8970\n",
      "Epoch 18/1000\n",
      " - 3s - loss: 0.1085 - acc: 0.9750 - val_loss: 0.2493 - val_acc: 0.8970\n",
      "Epoch 19/1000\n",
      " - 3s - loss: 0.1039 - acc: 0.9767 - val_loss: 0.2489 - val_acc: 0.8960\n",
      "Epoch 20/1000\n",
      " - 3s - loss: 0.0979 - acc: 0.9782 - val_loss: 0.2468 - val_acc: 0.8970\n",
      "Epoch 21/1000\n",
      " - 3s - loss: 0.0924 - acc: 0.9784 - val_loss: 0.2462 - val_acc: 0.8960\n",
      "Epoch 22/1000\n",
      " - 3s - loss: 0.0857 - acc: 0.9804 - val_loss: 0.2457 - val_acc: 0.8970\n",
      "Epoch 23/1000\n",
      " - 3s - loss: 0.0805 - acc: 0.9829 - val_loss: 0.2451 - val_acc: 0.9010\n",
      "Epoch 24/1000\n",
      " - 3s - loss: 0.0781 - acc: 0.9827 - val_loss: 0.2451 - val_acc: 0.9010\n",
      "Epoch 25/1000\n",
      " - 3s - loss: 0.0744 - acc: 0.9826 - val_loss: 0.2455 - val_acc: 0.8990\n",
      "Epoch 26/1000\n",
      " - 3s - loss: 0.0704 - acc: 0.9852 - val_loss: 0.2458 - val_acc: 0.8980\n",
      "Validation accuracy: 0.898, loss: 0.2458450584411621\n",
      "Accuracy: 0.898, Parameters: (layers=2, units=8)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 4s - loss: 0.6645 - acc: 0.8139 - val_loss: 0.6268 - val_acc: 0.8640\n",
      "Epoch 2/1000\n",
      " - 4s - loss: 0.5644 - acc: 0.9079 - val_loss: 0.5297 - val_acc: 0.8740\n",
      "Epoch 3/1000\n",
      " - 4s - loss: 0.4482 - acc: 0.9228 - val_loss: 0.4407 - val_acc: 0.8800\n",
      "Epoch 4/1000\n",
      " - 4s - loss: 0.3540 - acc: 0.9320 - val_loss: 0.3774 - val_acc: 0.8850\n",
      "Epoch 5/1000\n",
      " - 4s - loss: 0.2860 - acc: 0.9402 - val_loss: 0.3353 - val_acc: 0.8890\n",
      "Epoch 6/1000\n",
      " - 4s - loss: 0.2352 - acc: 0.9524 - val_loss: 0.3072 - val_acc: 0.8900\n",
      "Epoch 7/1000\n",
      " - 4s - loss: 0.2013 - acc: 0.9553 - val_loss: 0.2882 - val_acc: 0.8970\n",
      "Epoch 8/1000\n",
      " - 4s - loss: 0.1762 - acc: 0.9610 - val_loss: 0.2753 - val_acc: 0.8950\n",
      "Epoch 9/1000\n",
      " - 4s - loss: 0.1535 - acc: 0.9656 - val_loss: 0.2656 - val_acc: 0.8970\n",
      "Epoch 10/1000\n",
      " - 4s - loss: 0.1381 - acc: 0.9703 - val_loss: 0.2576 - val_acc: 0.9000\n",
      "Epoch 11/1000\n",
      " - 4s - loss: 0.1244 - acc: 0.9697 - val_loss: 0.2526 - val_acc: 0.8980\n",
      "Epoch 12/1000\n",
      " - 4s - loss: 0.1120 - acc: 0.9764 - val_loss: 0.2496 - val_acc: 0.9040\n",
      "Epoch 13/1000\n",
      " - 4s - loss: 0.1000 - acc: 0.9788 - val_loss: 0.2475 - val_acc: 0.9000\n",
      "Epoch 14/1000\n",
      " - 4s - loss: 0.0913 - acc: 0.9806 - val_loss: 0.2458 - val_acc: 0.9020\n",
      "Epoch 15/1000\n",
      " - 4s - loss: 0.0839 - acc: 0.9831 - val_loss: 0.2446 - val_acc: 0.9020\n",
      "Epoch 16/1000\n",
      " - 4s - loss: 0.0793 - acc: 0.9811 - val_loss: 0.2450 - val_acc: 0.8990\n",
      "Epoch 17/1000\n",
      " - 4s - loss: 0.0718 - acc: 0.9854 - val_loss: 0.2446 - val_acc: 0.9050\n",
      "Epoch 18/1000\n",
      " - 4s - loss: 0.0655 - acc: 0.9873 - val_loss: 0.2461 - val_acc: 0.8990\n",
      "Epoch 19/1000\n",
      " - 4s - loss: 0.0611 - acc: 0.9871 - val_loss: 0.2456 - val_acc: 0.9050\n",
      "Validation accuracy: 0.9050000014305115, loss: 0.24556897687911988\n",
      "Accuracy: 0.9050000014305115, Parameters: (layers=2, units=16)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 5s - loss: 0.6425 - acc: 0.7632 - val_loss: 0.5776 - val_acc: 0.8720\n",
      "Epoch 2/1000\n",
      " - 4s - loss: 0.4856 - acc: 0.9120 - val_loss: 0.4470 - val_acc: 0.8810\n",
      "Epoch 3/1000\n",
      " - 4s - loss: 0.3470 - acc: 0.9380 - val_loss: 0.3604 - val_acc: 0.8830\n",
      "Epoch 4/1000\n",
      " - 4s - loss: 0.2557 - acc: 0.9481 - val_loss: 0.3128 - val_acc: 0.8950\n",
      "Epoch 5/1000\n",
      " - 4s - loss: 0.2004 - acc: 0.9570 - val_loss: 0.2851 - val_acc: 0.8940\n",
      "Epoch 6/1000\n",
      " - 4s - loss: 0.1627 - acc: 0.9650 - val_loss: 0.2679 - val_acc: 0.8960\n",
      "Epoch 7/1000\n",
      " - 4s - loss: 0.1361 - acc: 0.9684 - val_loss: 0.2571 - val_acc: 0.8920\n",
      "Epoch 8/1000\n",
      " - 4s - loss: 0.1163 - acc: 0.9739 - val_loss: 0.2504 - val_acc: 0.8980\n",
      "Epoch 9/1000\n",
      " - 4s - loss: 0.1016 - acc: 0.9778 - val_loss: 0.2462 - val_acc: 0.9000\n",
      "Epoch 10/1000\n",
      " - 4s - loss: 0.0870 - acc: 0.9819 - val_loss: 0.2444 - val_acc: 0.9000\n",
      "Epoch 11/1000\n",
      " - 4s - loss: 0.0758 - acc: 0.9861 - val_loss: 0.2441 - val_acc: 0.8980\n",
      "Epoch 12/1000\n",
      " - 4s - loss: 0.0692 - acc: 0.9858 - val_loss: 0.2450 - val_acc: 0.8980\n",
      "Epoch 13/1000\n",
      " - 4s - loss: 0.0606 - acc: 0.9886 - val_loss: 0.2462 - val_acc: 0.9000\n",
      "Validation accuracy: 0.9000000014305115, loss: 0.24620721006393434\n",
      "Accuracy: 0.9000000014305115, Parameters: (layers=2, units=32)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 6s - loss: 0.6235 - acc: 0.7930 - val_loss: 0.5316 - val_acc: 0.8790\n",
      "Epoch 2/1000\n",
      " - 6s - loss: 0.4075 - acc: 0.9268 - val_loss: 0.3746 - val_acc: 0.8830\n",
      "Epoch 3/1000\n",
      " - 6s - loss: 0.2587 - acc: 0.9466 - val_loss: 0.3035 - val_acc: 0.8890\n",
      "Epoch 4/1000\n",
      " - 5s - loss: 0.1839 - acc: 0.9598 - val_loss: 0.2730 - val_acc: 0.8980\n",
      "Epoch 5/1000\n",
      " - 5s - loss: 0.1407 - acc: 0.9698 - val_loss: 0.2569 - val_acc: 0.8990\n",
      "Epoch 6/1000\n",
      " - 5s - loss: 0.1134 - acc: 0.9752 - val_loss: 0.2497 - val_acc: 0.8980\n",
      "Epoch 7/1000\n",
      " - 5s - loss: 0.0897 - acc: 0.9803 - val_loss: 0.2460 - val_acc: 0.8990\n",
      "Epoch 8/1000\n",
      " - 5s - loss: 0.0763 - acc: 0.9854 - val_loss: 0.2439 - val_acc: 0.9030\n",
      "Epoch 9/1000\n",
      " - 5s - loss: 0.0630 - acc: 0.9879 - val_loss: 0.2460 - val_acc: 0.8990\n",
      "Epoch 10/1000\n",
      " - 5s - loss: 0.0550 - acc: 0.9891 - val_loss: 0.2473 - val_acc: 0.9040\n",
      "Validation accuracy: 0.9039999976158142, loss: 0.24725477981567384\n",
      "Accuracy: 0.9039999976158142, Parameters: (layers=2, units=64)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 9s - loss: 0.5934 - acc: 0.8100 - val_loss: 0.4653 - val_acc: 0.8780\n",
      "Epoch 2/1000\n",
      " - 8s - loss: 0.3215 - acc: 0.9330 - val_loss: 0.3110 - val_acc: 0.8910\n",
      "Epoch 3/1000\n",
      " - 8s - loss: 0.1902 - acc: 0.9552 - val_loss: 0.2645 - val_acc: 0.8990\n",
      "Epoch 4/1000\n",
      " - 8s - loss: 0.1305 - acc: 0.9700 - val_loss: 0.2490 - val_acc: 0.8950\n",
      "Epoch 5/1000\n",
      " - 8s - loss: 0.0959 - acc: 0.9774 - val_loss: 0.2423 - val_acc: 0.9050\n",
      "Epoch 6/1000\n",
      " - 8s - loss: 0.0738 - acc: 0.9846 - val_loss: 0.2418 - val_acc: 0.9040\n",
      "Epoch 7/1000\n",
      " - 8s - loss: 0.0608 - acc: 0.9870 - val_loss: 0.2445 - val_acc: 0.9010\n",
      "Epoch 8/1000\n",
      " - 8s - loss: 0.0482 - acc: 0.9910 - val_loss: 0.2487 - val_acc: 0.9030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9030000014305115, loss: 0.24866126775741576\n",
      "Accuracy: 0.9030000014305115, Parameters: (layers=2, units=128)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 3s - loss: 0.6752 - acc: 0.7001 - val_loss: 0.6466 - val_acc: 0.8570\n",
      "Epoch 2/1000\n",
      " - 3s - loss: 0.5978 - acc: 0.8460 - val_loss: 0.5583 - val_acc: 0.8550\n",
      "Epoch 3/1000\n",
      " - 3s - loss: 0.4832 - acc: 0.8816 - val_loss: 0.4530 - val_acc: 0.8790\n",
      "Epoch 4/1000\n",
      " - 3s - loss: 0.3674 - acc: 0.9158 - val_loss: 0.3648 - val_acc: 0.8950\n",
      "Epoch 5/1000\n",
      " - 3s - loss: 0.2834 - acc: 0.9362 - val_loss: 0.3086 - val_acc: 0.8960\n",
      "Epoch 6/1000\n",
      " - 3s - loss: 0.2226 - acc: 0.9467 - val_loss: 0.2750 - val_acc: 0.8990\n",
      "Epoch 7/1000\n",
      " - 3s - loss: 0.1781 - acc: 0.9571 - val_loss: 0.2580 - val_acc: 0.8960\n",
      "Epoch 8/1000\n",
      " - 3s - loss: 0.1494 - acc: 0.9629 - val_loss: 0.2520 - val_acc: 0.9030\n",
      "Epoch 9/1000\n",
      " - 3s - loss: 0.1233 - acc: 0.9728 - val_loss: 0.2510 - val_acc: 0.8960\n",
      "Epoch 10/1000\n",
      " - 3s - loss: 0.1055 - acc: 0.9744 - val_loss: 0.2531 - val_acc: 0.9070\n",
      "Epoch 11/1000\n",
      " - 3s - loss: 0.0931 - acc: 0.9794 - val_loss: 0.2609 - val_acc: 0.9070\n",
      "Validation accuracy: 0.9069999990463257, loss: 0.2608669421672821\n",
      "Accuracy: 0.9069999990463257, Parameters: (layers=3, units=8)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 4s - loss: 0.6713 - acc: 0.7043 - val_loss: 0.6232 - val_acc: 0.8700\n",
      "Epoch 2/1000\n",
      " - 3s - loss: 0.5041 - acc: 0.8937 - val_loss: 0.4023 - val_acc: 0.8820\n",
      "Epoch 3/1000\n",
      " - 3s - loss: 0.2828 - acc: 0.9310 - val_loss: 0.2869 - val_acc: 0.8930\n",
      "Epoch 4/1000\n",
      " - 3s - loss: 0.1762 - acc: 0.9516 - val_loss: 0.2516 - val_acc: 0.9000\n",
      "Epoch 5/1000\n",
      " - 3s - loss: 0.1275 - acc: 0.9653 - val_loss: 0.2435 - val_acc: 0.9030\n",
      "Epoch 6/1000\n",
      " - 3s - loss: 0.0951 - acc: 0.9737 - val_loss: 0.2452 - val_acc: 0.9010\n",
      "Epoch 7/1000\n",
      " - 3s - loss: 0.0732 - acc: 0.9821 - val_loss: 0.2477 - val_acc: 0.9030\n",
      "Validation accuracy: 0.9029999976158142, loss: 0.24773764944076537\n",
      "Accuracy: 0.9029999976158142, Parameters: (layers=3, units=16)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 5s - loss: 0.6347 - acc: 0.7158 - val_loss: 0.5046 - val_acc: 0.8760\n",
      "Epoch 2/1000\n",
      " - 4s - loss: 0.3195 - acc: 0.9256 - val_loss: 0.2680 - val_acc: 0.8940\n",
      "Epoch 3/1000\n",
      " - 4s - loss: 0.1413 - acc: 0.9600 - val_loss: 0.2438 - val_acc: 0.8950\n",
      "Epoch 4/1000\n",
      " - 4s - loss: 0.0866 - acc: 0.9753 - val_loss: 0.2515 - val_acc: 0.8930\n",
      "Epoch 5/1000\n",
      " - 4s - loss: 0.0602 - acc: 0.9821 - val_loss: 0.2665 - val_acc: 0.8930\n",
      "Validation accuracy: 0.893, loss: 0.266548011302948\n",
      "Accuracy: 0.893, Parameters: (layers=3, units=32)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 6s - loss: 0.5582 - acc: 0.8146 - val_loss: 0.3346 - val_acc: 0.8850\n",
      "Epoch 2/1000\n",
      " - 5s - loss: 0.1791 - acc: 0.9436 - val_loss: 0.2421 - val_acc: 0.9050\n",
      "Epoch 3/1000\n",
      " - 5s - loss: 0.0803 - acc: 0.9746 - val_loss: 0.2513 - val_acc: 0.9060\n",
      "Epoch 4/1000\n",
      " - 5s - loss: 0.0459 - acc: 0.9877 - val_loss: 0.2776 - val_acc: 0.8950\n",
      "Validation accuracy: 0.8949999976158142, loss: 0.2776239504814148\n",
      "Accuracy: 0.8949999976158142, Parameters: (layers=3, units=64)\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 8s - loss: 0.4772 - acc: 0.8493 - val_loss: 0.2610 - val_acc: 0.8880\n",
      "Epoch 2/1000\n",
      " - 7s - loss: 0.1167 - acc: 0.9588 - val_loss: 0.2677 - val_acc: 0.8980\n",
      "Epoch 3/1000\n",
      " - 7s - loss: 0.0542 - acc: 0.9831 - val_loss: 0.2981 - val_acc: 0.8980\n",
      "Validation accuracy: 0.898, loss: 0.29812693309783933\n",
      "Accuracy: 0.898, Parameters: (layers=3, units=128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVk5VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c78103668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tune_ngram_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Module to train sequence model.\n",
    "Vectorizes training and validation texts into sequences and uses that for\n",
    "training a sequence model - a sepCNN model. We use sequence model for text\n",
    "classification when the ratio of number of samples to number of words per\n",
    "sample for the given dataset is very large (>~15K).\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#import load_data\n",
    "#import explore_data\n",
    "#import vectorize_data\n",
    "#import build_model\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_sequence_model(data,\n",
    "                         learning_rate=1e-3,\n",
    "                         epochs=1000,\n",
    "                         batch_size=128,\n",
    "                         blocks=2,\n",
    "                         filters=64,\n",
    "                         dropout_rate=0.2,\n",
    "                         embedding_dim=200,\n",
    "                         kernel_size=3,\n",
    "                         pool_size=3):\n",
    "    \"\"\"Trains sequence model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of sepCNN layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val, word_index = sequence_vectorize(\n",
    "            train_texts, val_texts)\n",
    "\n",
    "    # Number of features will be the embedding input dimension. Add 1 for the\n",
    "    # reserved index 0.\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('rotten_tomatoes_sepcnn_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 32s - loss: 0.6967 - acc: 0.5187 - val_loss: 0.7015 - val_acc: 0.4880\n",
      "Epoch 2/1000\n",
      " - 30s - loss: 0.6402 - acc: 0.6434 - val_loss: 0.5611 - val_acc: 0.7760\n",
      "Epoch 3/1000\n",
      " - 30s - loss: 0.4821 - acc: 0.7848 - val_loss: 0.3797 - val_acc: 0.8540\n",
      "Epoch 4/1000\n",
      " - 30s - loss: 0.3364 - acc: 0.8668 - val_loss: 0.3385 - val_acc: 0.8720\n",
      "Epoch 5/1000\n",
      " - 30s - loss: 0.2526 - acc: 0.9031 - val_loss: 0.3246 - val_acc: 0.8780\n",
      "Epoch 6/1000\n",
      " - 30s - loss: 0.2304 - acc: 0.9090 - val_loss: 0.3278 - val_acc: 0.8780\n",
      "Epoch 7/1000\n",
      " - 31s - loss: 0.2049 - acc: 0.9248 - val_loss: 0.3242 - val_acc: 0.8740\n",
      "Epoch 8/1000\n",
      " - 30s - loss: 0.1434 - acc: 0.9507 - val_loss: 0.3446 - val_acc: 0.8790\n",
      "Epoch 9/1000\n",
      " - 33s - loss: 0.1396 - acc: 0.9492 - val_loss: 0.3548 - val_acc: 0.8750\n",
      "Validation accuracy: 0.875, loss: 0.35478141260147095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.875, 0.35478141260147095)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequence_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Module to train sequence model with fine-tuned pre-trained embeddings.\n",
    "Vectorizes training and validation texts into sequences and uses that for\n",
    "training a sequence model - a sepCNN model. We use sequence model with\n",
    "pre-trained embeddings that are fine-tuned for text classification when the\n",
    "ratio of number of samples to number of words per sample for the given dataset\n",
    "is neither small nor very large (~> 1500 && ~< 15K).\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_embedding_matrix(word_index, embedding_data_dir, embedding_dim):\n",
    "    \"\"\"Gets embedding matrix from the embedding index data.\n",
    "    # Arguments\n",
    "        word_index: dict, word to index map that was generated from the data.\n",
    "        embedding_data_dir: string, path to the pre-training embeddings.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "    # Returns\n",
    "        dict, word vectors for words in word_index from pre-trained embedding.\n",
    "    # References:\n",
    "        https://nlp.stanford.edu/projects/glove/\n",
    "        Download and uncompress archive from:\n",
    "        http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the pre-trained embedding file and get word to word vector mappings.\n",
    "    embedding_matrix_all = {}\n",
    "\n",
    "    # We are using 200d GloVe embeddings.\n",
    "    fname = os.path.join(embedding_data_dir, 'glove.6B.300d.txt')\n",
    "    with open(fname) as f:\n",
    "        for line in f:  # Every line contains word followed by the vector value\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_matrix_all[word] = coefs\n",
    "\n",
    "    # Prepare embedding matrix with just the words in our word_index dictionary\n",
    "    num_words = min(len(word_index) + 1, TOP_K)\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= TOP_K:\n",
    "            continue\n",
    "        embedding_vector = embedding_matrix_all.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_fine_tuned_sequence_model(data,\n",
    "                                    embedding_data_dir,\n",
    "                                    learning_rate=1e-3,\n",
    "                                    epochs=1000,\n",
    "                                    batch_size=128,\n",
    "                                    blocks=2,\n",
    "                                    filters=64,\n",
    "                                    dropout_rate=0.2,\n",
    "                                    embedding_dim=300,\n",
    "                                    kernel_size=3,\n",
    "                                    pool_size=3):\n",
    "    \"\"\"Trains sequence model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        embedding_data_dir: string, path to the pre-training embeddings.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of sepCNN layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val, word_index = sequence_vectorize(\n",
    "            train_texts, val_texts)\n",
    "\n",
    "    # Number of features will be the embedding input dimension. Add 1 for the\n",
    "    # reserved index 0.\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    embedding_matrix = _get_embedding_matrix(\n",
    "        word_index, embedding_data_dir, embedding_dim)\n",
    "\n",
    "    # Create model instance. First time we will train rest of network while\n",
    "    # keeping embedding layer weights frozen. So, we set\n",
    "    # is_embedding_trainable as False.\n",
    "    model = sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features,\n",
    "                                     use_pretrained_embedding=True,\n",
    "                                     is_embedding_trainable=False,\n",
    "                                     embedding_matrix=embedding_matrix)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    model.fit(x_train,\n",
    "              train_labels,\n",
    "              epochs=epochs,\n",
    "              callbacks=callbacks,\n",
    "              validation_data=(x_val, val_labels),\n",
    "              verbose=2,  # Logs once per epoch.\n",
    "              batch_size=batch_size)\n",
    "\n",
    "    # Save the model.\n",
    "    model.save_weights('sequence_model_with_pre_trained_embedding.h5')\n",
    "\n",
    "    # Create another model instance. This time we will unfreeze the embedding\n",
    "    # layer and let it fine-tune to the given dataset.\n",
    "    model = sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features,\n",
    "                                     use_pretrained_embedding=True,\n",
    "                                     is_embedding_trainable=True,\n",
    "                                     embedding_matrix=embedding_matrix)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Load the weights that we had saved into this new model.\n",
    "    model.load_weights('sequence_model_with_pre_trained_embedding.h5')\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(x_train,\n",
    "                        train_labels,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=(x_val, val_labels),\n",
    "                        verbose=2,  # Logs once per epoch.\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('tweet_weather_sepcnn_fine_tuned_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      " - 34s - loss: 0.5864 - acc: 0.6952 - val_loss: 0.4544 - val_acc: 0.7970\n",
      "Epoch 2/1000\n",
      " - 28s - loss: 0.4439 - acc: 0.8112 - val_loss: 0.3827 - val_acc: 0.8390\n",
      "Epoch 3/1000\n",
      " - 30s - loss: 0.4055 - acc: 0.8299 - val_loss: 0.3542 - val_acc: 0.8560\n",
      "Epoch 4/1000\n",
      " - 32s - loss: 0.3741 - acc: 0.8443 - val_loss: 0.3491 - val_acc: 0.8620\n",
      "Epoch 5/1000\n",
      " - 30s - loss: 0.3555 - acc: 0.8523 - val_loss: 0.3431 - val_acc: 0.8720\n",
      "Epoch 6/1000\n",
      " - 30s - loss: 0.3384 - acc: 0.8591 - val_loss: 0.3154 - val_acc: 0.8780\n",
      "Epoch 7/1000\n",
      " - 29s - loss: 0.3251 - acc: 0.8681 - val_loss: 0.3113 - val_acc: 0.8870\n",
      "Epoch 8/1000\n",
      " - 29s - loss: 0.3357 - acc: 0.8624 - val_loss: 0.3540 - val_acc: 0.8390\n",
      "Epoch 9/1000\n",
      " - 31s - loss: 0.3104 - acc: 0.8748 - val_loss: 0.3032 - val_acc: 0.8800\n",
      "Epoch 10/1000\n",
      " - 28s - loss: 0.3031 - acc: 0.8771 - val_loss: 0.2966 - val_acc: 0.8930\n",
      "Epoch 11/1000\n",
      " - 27s - loss: 0.2894 - acc: 0.8839 - val_loss: 0.2870 - val_acc: 0.8950\n",
      "Epoch 12/1000\n",
      " - 28s - loss: 0.2795 - acc: 0.8894 - val_loss: 0.2859 - val_acc: 0.8910\n",
      "Epoch 13/1000\n",
      " - 30s - loss: 0.2656 - acc: 0.8936 - val_loss: 0.3269 - val_acc: 0.8530\n",
      "Epoch 14/1000\n",
      " - 28s - loss: 0.2615 - acc: 0.8950 - val_loss: 0.2938 - val_acc: 0.8790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ankush/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-68-6072f71c2237>\", line 1, in <module>\n",
      "    train_fine_tuned_sequence_model(data,'/home/ankush/Github/Machine Learning/Text Classification/AnalyticsVidya/data/glove.6B')\n",
      "  File \"<ipython-input-67-57f7ea96eede>\", line 92, in train_fine_tuned_sequence_model\n",
      "    model.save_weights('sequence_model_with_pre_trained_embedding.h5')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/models.py\", line 719, in save_weights\n",
      "    f = h5py.File(filepath, 'w')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py\", line 394, in __init__\n",
      "    swmr=swmr)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py\", line 176, in make_fid\n",
      "    fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 105, in h5py.h5f.create\n",
      "OSError: Unable to create file (unable to open file: name = 'sequence_model_with_pre_trained_embedding.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ankush/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ankush/.local/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ankush/.local/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ankush/.local/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1410, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 672, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 701, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 685, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/usr/lib/python3.5/posixpath.py\", line 361, in abspath\n",
      "    cwd = os.getcwd()\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = 'sequence_model_with_pre_trained_embedding.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "train_fine_tuned_sequence_model(data,'/home/ankush/Github/Machine Learning/Text Classification/AnalyticsVidya/data/glove.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "On 200d set\n",
    "Train on 9000 samples, validate on 1000 samples\n",
    "Epoch 1/1000\n",
    " - 26s - loss: 0.6418 - acc: 0.6311 - val_loss: 0.5102 - val_acc: 0.7840\n",
    "Epoch 2/1000\n",
    " - 25s - loss: 0.4875 - acc: 0.7794 - val_loss: 0.4249 - val_acc: 0.8250\n",
    "Epoch 3/1000\n",
    " - 23s - loss: 0.4338 - acc: 0.8127 - val_loss: 0.3887 - val_acc: 0.8410\n",
    "Epoch 4/1000\n",
    " - 23s - loss: 0.4057 - acc: 0.8279 - val_loss: 0.3680 - val_acc: 0.8510\n",
    "Epoch 5/1000\n",
    " - 23s - loss: 0.3791 - acc: 0.8398 - val_loss: 0.3523 - val_acc: 0.8610\n",
    "Epoch 6/1000\n",
    " - 22s - loss: 0.3630 - acc: 0.8472 - val_loss: 0.3413 - val_acc: 0.8690\n",
    "Epoch 7/1000\n",
    " - 22s - loss: 0.3612 - acc: 0.8484 - val_loss: 0.3450 - val_acc: 0.8760\n",
    "Epoch 8/1000\n",
    " - 23s - loss: 0.3404 - acc: 0.8620 - val_loss: 0.3329 - val_acc: 0.8710\n",
    "Epoch 9/1000\n",
    " - 25s - loss: 0.3435 - acc: 0.8556 - val_loss: 0.3231 - val_acc: 0.8670\n",
    "Epoch 10/1000\n",
    " - 26s - loss: 0.3264 - acc: 0.8669 - val_loss: 0.3280 - val_acc: 0.8690\n",
    "Epoch 11/1000\n",
    " - 25s - loss: 0.3227 - acc: 0.8666 - val_loss: 0.3203 - val_acc: 0.8720\n",
    "Epoch 12/1000\n",
    " - 24s - loss: 0.3100 - acc: 0.8734 - val_loss: 0.3009 - val_acc: 0.8820\n",
    "Epoch 13/1000\n",
    " - 23s - loss: 0.3026 - acc: 0.8752 - val_loss: 0.2979 - val_acc: 0.8840\n",
    "Epoch 14/1000\n",
    " - 22s - loss: 0.2952 - acc: 0.8781 - val_loss: 0.2902 - val_acc: 0.8790\n",
    "Epoch 15/1000\n",
    " - 22s - loss: 0.2792 - acc: 0.8861 - val_loss: 0.2774 - val_acc: 0.8890\n",
    "Epoch 16/1000\n",
    " - 22s - loss: 0.2683 - acc: 0.8902 - val_loss: 0.2904 - val_acc: 0.8870\n",
    "Epoch 17/1000\n",
    " - 23s - loss: 0.2665 - acc: 0.8930 - val_loss: 0.2761 - val_acc: 0.8910\n",
    "Epoch 18/1000\n",
    " - 22s - loss: 0.2576 - acc: 0.8939 - val_loss: 0.2907 - val_acc: 0.8810\n",
    "Epoch 19/1000\n",
    " - 22s - loss: 0.2491 - acc: 0.8970 - val_loss: 0.2956 - val_acc: 0.8830\n",
    "Train on 9000 samples, validate on 1000 samples\n",
    "Epoch 1/1000\n",
    " - 31s - loss: 0.4787 - acc: 0.7944 - val_loss: 0.2600 - val_acc: 0.9000\n",
    "Epoch 2/1000\n",
    " - 30s - loss: 0.1914 - acc: 0.9314 - val_loss: 0.2500 - val_acc: 0.9100\n",
    "Epoch 3/1000\n",
    " - 30s - loss: 0.1052 - acc: 0.9641 - val_loss: 0.3113 - val_acc: 0.8930\n",
    "Epoch 4/1000\n",
    " - 30s - loss: 0.0646 - acc: 0.9799 - val_loss: 0.3858 - val_acc: 0.8890\n",
    "Validation accuracy: 0.889, loss: 0.38577818536758424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _data_generator(x, y, num_features, batch_size):\n",
    "    \"\"\"Generates batches of vectorized texts for training/validation.\n",
    "    # Arguments\n",
    "        x: np.matrix, feature matrix.\n",
    "        y: np.ndarray, labels.\n",
    "        num_features: int, number of features.\n",
    "        batch_size: int, number of samples per batch.\n",
    "    # Returns\n",
    "        Yields feature and label data in batches.\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    if num_samples % batch_size:\n",
    "        num_batches += 1\n",
    "\n",
    "    while 1:\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            if end_idx > num_samples:\n",
    "                end_idx = num_samples\n",
    "            x_batch = x[start_idx:end_idx]\n",
    "            y_batch = y[start_idx:end_idx]\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_train_sequence_model(data,\n",
    "                               learning_rate=1e-3,\n",
    "                               epochs=1000,\n",
    "                               batch_size=128,\n",
    "                               blocks=2,\n",
    "                               filters=64,\n",
    "                               dropout_rate=0.2,\n",
    "                               embedding_dim=200,\n",
    "                               kernel_size=3,\n",
    "                               pool_size=3):\n",
    "    \"\"\"Trains sequence model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of sepCNN layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val, word_index = sequence_vectorize(\n",
    "            train_texts, val_texts)\n",
    "\n",
    "    # Number of features will be the embedding input dimension. Add 1 for the\n",
    "    # reserved index 0.\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Create training and validation generators.\n",
    "    training_generator = _data_generator(\n",
    "        x_train, train_labels, num_features, batch_size)\n",
    "    validation_generator = _data_generator(\n",
    "        x_val, val_labels, num_features, batch_size)\n",
    "\n",
    "    # Get number of training steps. This indicated the number of steps it takes\n",
    "    # to cover all samples in one epoch.\n",
    "    steps_per_epoch = x_train.shape[0] // batch_size\n",
    "    if x_train.shape[0] % batch_size:\n",
    "        steps_per_epoch += 1\n",
    "\n",
    "    # Get number of validation steps.\n",
    "    validation_steps = x_val.shape[0] // batch_size\n",
    "    if x_val.shape[0] % batch_size:\n",
    "        validation_steps += 1\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit_generator(\n",
    "            generator=training_generator,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            epochs=epochs,\n",
    "            verbose=2)  # Logs once per epoch.\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('amazon_reviews_sepcnn_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " - 32s - loss: 0.7137 - acc: 0.5104 - val_loss: 0.6913 - val_acc: 0.5160\n",
      "Epoch 2/1000\n",
      " - 32s - loss: 0.6518 - acc: 0.6288 - val_loss: 0.5884 - val_acc: 0.6960\n",
      "Epoch 3/1000\n",
      " - 31s - loss: 0.4646 - acc: 0.7948 - val_loss: 0.5830 - val_acc: 0.7330\n",
      "Epoch 4/1000\n",
      " - 14777s - loss: 0.3370 - acc: 0.8651 - val_loss: 0.3990 - val_acc: 0.8290\n",
      "Epoch 5/1000\n",
      " - 30s - loss: 0.2918 - acc: 0.8862 - val_loss: 0.4301 - val_acc: 0.8380\n",
      "Epoch 6/1000\n",
      " - 30s - loss: 0.2591 - acc: 0.9034 - val_loss: 0.3568 - val_acc: 0.8630\n",
      "Epoch 7/1000\n",
      " - 30s - loss: 0.1826 - acc: 0.9325 - val_loss: 0.3554 - val_acc: 0.8640\n",
      "Epoch 8/1000\n",
      " - 30s - loss: 0.1549 - acc: 0.9435 - val_loss: 0.3857 - val_acc: 0.8690\n",
      "Epoch 9/1000\n",
      " - 30s - loss: 0.1553 - acc: 0.9434 - val_loss: 0.4293 - val_acc: 0.8580\n",
      "Validation accuracy: 0.8579999995231629, loss: 0.4292738399505615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8579999995231629, 0.4292738399505615)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_train_sequence_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

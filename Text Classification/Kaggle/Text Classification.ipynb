{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas,numpy\n",
    "from sklearn import model_selection,preprocessing\n",
    "from keras.preprocessing import text, sequence\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = open('/home/ankush/Github/Machine Learning/Text Classification/AnalyticsVidya/data/corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'],test_size=0.1)\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 70\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(train_x)\n",
    "sequences = tok.texts_to_sequences(train_x)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 70, 50)            50000     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 96,337\n",
      "Trainable params: 96,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/10\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.6211 - acc: 0.6642 - val_loss: 0.5341 - val_acc: 0.7656\n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.4262 - acc: 0.8175 - val_loss: 0.5204 - val_acc: 0.7644\n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.3732 - acc: 0.8404 - val_loss: 0.4254 - val_acc: 0.8233\n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.3473 - acc: 0.8540 - val_loss: 0.4865 - val_acc: 0.8117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd8e933d68>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_matrix,train_y,batch_size=128,epochs=10,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.00001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(valid_x)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 652us/step\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(test_sequences_matrix,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "  Loss: 0.875\n",
      "  Accuracy: 0.793\n"
     ]
    }
   ],
   "source": [
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               36352     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 167,937\n",
      "Trainable params: 167,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n",
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/10\n",
      "7200/7200 [==============================] - 3s 442us/step - loss: 8.1297 - acc: 0.4889 - binary_accuracy: 0.4889 - val_loss: 8.1749 - val_acc: 0.4872 - val_binary_accuracy: 0.4872\n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 2s 333us/step - loss: 8.1439 - acc: 0.4892 - binary_accuracy: 0.4892 - val_loss: 8.1749 - val_acc: 0.4872 - val_binary_accuracy: 0.4872\n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 2s 319us/step - loss: 8.1439 - acc: 0.4892 - binary_accuracy: 0.4892 - val_loss: 8.1749 - val_acc: 0.4872 - val_binary_accuracy: 0.4872\n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 2s 317us/step - loss: 8.1439 - acc: 0.4892 - binary_accuracy: 0.4892 - val_loss: 8.1749 - val_acc: 0.4872 - val_binary_accuracy: 0.4872\n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 2s 317us/step - loss: 8.1439 - acc: 0.4892 - binary_accuracy: 0.4892 - val_loss: 8.1749 - val_acc: 0.4872 - val_binary_accuracy: 0.4872\n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 2s 343us/step - loss: 8.1439 - acc: 0.4892 - binary_accuracy: 0.4892 - val_loss: 8.1749 - val_acc: 0.4872 - val_binary_accuracy: 0.4872\n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 3s 348us/step - loss: 8.1439 - acc: 0.4892 - binary_accuracy: 0.4892 - val_loss: 8.1749 - val_acc: 0.4872 - val_binary_accuracy: 0.4872\n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 2s 346us/step - loss: 8.1439 - acc: 0.4892 - binary_accuracy: 0.4892 - val_loss: 8.1749 - val_acc: 0.4872 - val_binary_accuracy: 0.4872\n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 3s 383us/step - loss: 8.1439 - acc: 0.4892 - binary_accuracy: 0.4892 - val_loss: 8.1749 - val_acc: 0.4872 - val_binary_accuracy: 0.4872\n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 2s 322us/step - loss: 8.1439 - acc: 0.4892 - binary_accuracy: 0.4892 - val_loss: 8.1749 - val_acc: 0.4872 - val_binary_accuracy: 0.4872\n"
     ]
    }
   ],
   "source": [
    "def get_simple_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(max_len,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc',metrics.binary_accuracy])\n",
    "    print('compile done')\n",
    "    return model\n",
    "\n",
    "def check_model(model):\n",
    "    model.fit(sequences_matrix,train_y,batch_size=32,epochs=10,verbose=1,validation_split=0.2)\n",
    "\n",
    "m = get_simple_model()\n",
    "check_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 70, 20)            20000     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 70, 20)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 68, 64)            3904      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 40,801\n",
      "Trainable params: 40,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/10\n",
      "7200/7200 [==============================] - 6s 839us/step - loss: 0.6012 - acc: 0.6557 - binary_accuracy: 0.6557 - val_loss: 0.4257 - val_acc: 0.8044 - val_binary_accuracy: 0.8044\n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 5s 657us/step - loss: 0.3868 - acc: 0.8304 - binary_accuracy: 0.8304 - val_loss: 0.3739 - val_acc: 0.8283 - val_binary_accuracy: 0.8283\n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 5s 710us/step - loss: 0.3117 - acc: 0.8714 - binary_accuracy: 0.8714 - val_loss: 0.3641 - val_acc: 0.8394 - val_binary_accuracy: 0.8394\n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 5s 651us/step - loss: 0.2675 - acc: 0.8850 - binary_accuracy: 0.8850 - val_loss: 0.4167 - val_acc: 0.8139 - val_binary_accuracy: 0.8139\n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 5s 685us/step - loss: 0.2325 - acc: 0.9097 - binary_accuracy: 0.9097 - val_loss: 0.3893 - val_acc: 0.8350 - val_binary_accuracy: 0.8350\n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 5s 637us/step - loss: 0.2040 - acc: 0.9185 - binary_accuracy: 0.9185 - val_loss: 0.3905 - val_acc: 0.8433 - val_binary_accuracy: 0.8433\n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 5s 634us/step - loss: 0.1784 - acc: 0.9294 - binary_accuracy: 0.9294 - val_loss: 0.4086 - val_acc: 0.8367 - val_binary_accuracy: 0.8367\n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 5s 657us/step - loss: 0.1693 - acc: 0.9336 - binary_accuracy: 0.9336 - val_loss: 0.4174 - val_acc: 0.8378 - val_binary_accuracy: 0.8378\n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 5s 629us/step - loss: 0.1389 - acc: 0.9475 - binary_accuracy: 0.9475 - val_loss: 0.4826 - val_acc: 0.8267 - val_binary_accuracy: 0.8267\n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 5s 642us/step - loss: 0.1268 - acc: 0.9500 - binary_accuracy: 0.9500 - val_loss: 0.5662 - val_acc: 0.8150 - val_binary_accuracy: 0.8150\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model_v1():   \n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(1000,\n",
    "                        20,\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64,\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m = get_cnn_model_v1()\n",
    "check_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 70, 50)            50000     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 70, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 68, 64)            9664      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 76,561\n",
      "Trainable params: 76,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/10\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.5745 - acc: 0.6794 - binary_accuracy: 0.6794 - val_loss: 0.4053 - val_acc: 0.8183 - val_binary_accuracy: 0.8183\n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 7s 995us/step - loss: 0.3531 - acc: 0.8442 - binary_accuracy: 0.8442 - val_loss: 0.3579 - val_acc: 0.8294 - val_binary_accuracy: 0.8294\n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 6s 863us/step - loss: 0.2723 - acc: 0.8893 - binary_accuracy: 0.8893 - val_loss: 0.3782 - val_acc: 0.8250 - val_binary_accuracy: 0.8250\n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 6s 817us/step - loss: 0.2218 - acc: 0.9115 - binary_accuracy: 0.9115 - val_loss: 0.3776 - val_acc: 0.8294 - val_binary_accuracy: 0.8294\n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 6s 792us/step - loss: 0.1614 - acc: 0.9367 - binary_accuracy: 0.9367 - val_loss: 0.4381 - val_acc: 0.8300 - val_binary_accuracy: 0.8300\n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 6s 825us/step - loss: 0.1273 - acc: 0.9531 - binary_accuracy: 0.9531 - val_loss: 0.4414 - val_acc: 0.8306 - val_binary_accuracy: 0.8306\n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 6s 849us/step - loss: 0.0846 - acc: 0.9699 - binary_accuracy: 0.9699 - val_loss: 0.5019 - val_acc: 0.8339 - val_binary_accuracy: 0.8339\n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 6s 851us/step - loss: 0.0730 - acc: 0.9725 - binary_accuracy: 0.9725 - val_loss: 0.5433 - val_acc: 0.8350 - val_binary_accuracy: 0.8350\n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 6s 880us/step - loss: 0.0489 - acc: 0.9832 - binary_accuracy: 0.9832 - val_loss: 0.6154 - val_acc: 0.8289 - val_binary_accuracy: 0.8289\n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 6s 900us/step - loss: 0.0489 - acc: 0.9831 - binary_accuracy: 0.9831 - val_loss: 0.6206 - val_acc: 0.8422 - val_binary_accuracy: 0.8422\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model_v2(): # added embed   \n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(1000,\n",
    "                        50, #!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64,\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m = get_cnn_model_v2()\n",
    "check_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 70, 20)            20000     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 70, 20)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 68, 256)           15616     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 101,665\n",
      "Trainable params: 101,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/10\n",
      "7200/7200 [==============================] - 14s 2ms/step - loss: 0.5737 - acc: 0.6767 - binary_accuracy: 0.6767 - val_loss: 0.4057 - val_acc: 0.8133 - val_binary_accuracy: 0.8133\n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 0.3734 - acc: 0.8399 - binary_accuracy: 0.8399 - val_loss: 0.3613 - val_acc: 0.8378 - val_binary_accuracy: 0.8378\n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 14s 2ms/step - loss: 0.3055 - acc: 0.8699 - binary_accuracy: 0.8699 - val_loss: 0.3739 - val_acc: 0.8244 - val_binary_accuracy: 0.8244\n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.2657 - acc: 0.8926 - binary_accuracy: 0.8926 - val_loss: 0.3621 - val_acc: 0.8400 - val_binary_accuracy: 0.8400\n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 0.2364 - acc: 0.9069 - binary_accuracy: 0.9069 - val_loss: 0.3749 - val_acc: 0.8467 - val_binary_accuracy: 0.8467\n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 0.2102 - acc: 0.9160 - binary_accuracy: 0.9160 - val_loss: 0.3963 - val_acc: 0.8350 - val_binary_accuracy: 0.8350\n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.1921 - acc: 0.9267 - binary_accuracy: 0.9267 - val_loss: 0.4070 - val_acc: 0.8400 - val_binary_accuracy: 0.8400\n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.1606 - acc: 0.9411 - binary_accuracy: 0.9411 - val_loss: 0.4527 - val_acc: 0.8339 - val_binary_accuracy: 0.8339\n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.1478 - acc: 0.9432 - binary_accuracy: 0.9432 - val_loss: 0.4715 - val_acc: 0.8294 - val_binary_accuracy: 0.8294\n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.1343 - acc: 0.9501 - binary_accuracy: 0.9501 - val_loss: 0.4603 - val_acc: 0.8494 - val_binary_accuracy: 0.8494\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model_v3():    # added filter\n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(1000,\n",
    "                        20,\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(256, #!!!!!!!!!!!!!!!!!!!\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m = get_cnn_model_v3()\n",
    "check_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('/home/ankush/Github/Machine Learning/Text Classification/AnalyticsVidya/data/wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 70, 300)           10041000  \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 70, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 68, 256)           230656    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 10,337,705\n",
      "Trainable params: 296,705\n",
      "Non-trainable params: 10,041,000\n",
      "_________________________________________________________________\n",
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/10\n",
      "7200/7200 [==============================] - 29s 4ms/step - loss: 0.5892 - acc: 0.6621 - binary_accuracy: 0.6621 - val_loss: 0.4750 - val_acc: 0.7672 - val_binary_accuracy: 0.7672\n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 23s 3ms/step - loss: 0.4141 - acc: 0.8090 - binary_accuracy: 0.8090 - val_loss: 0.4088 - val_acc: 0.8056 - val_binary_accuracy: 0.8056: 0.41\n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 26s 4ms/step - loss: 0.3142 - acc: 0.8642 - binary_accuracy: 0.8642 - val_loss: 0.3944 - val_acc: 0.8222 - val_binary_accuracy: 0.8222\n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 28s 4ms/step - loss: 0.2197 - acc: 0.9142 - binary_accuracy: 0.9142 - val_loss: 0.4001 - val_acc: 0.8328 - val_binary_accuracy: 0.8328\n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 24s 3ms/step - loss: 0.1448 - acc: 0.9467 - binary_accuracy: 0.9467 - val_loss: 0.4342 - val_acc: 0.8272 - val_binary_accuracy: 0.8272\n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 24s 3ms/step - loss: 0.1055 - acc: 0.9604 - binary_accuracy: 0.9604 - val_loss: 0.4927 - val_acc: 0.8211 - val_binary_accuracy: 0.8211\n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 24s 3ms/step - loss: 0.0767 - acc: 0.9726 - binary_accuracy: 0.9726 - val_loss: 0.6269 - val_acc: 0.8067 - val_binary_accuracy: 0.8067\n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 24s 3ms/step - loss: 0.1034 - acc: 0.9585 - binary_accuracy: 0.9585 - val_loss: 0.5498 - val_acc: 0.8217 - val_binary_accuracy: 0.8217\n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 24s 3ms/step - loss: 0.0484 - acc: 0.9833 - binary_accuracy: 0.9833 - val_loss: 0.6700 - val_acc: 0.8178 - val_binary_accuracy: 0.8178\n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 28s 4ms/step - loss: 0.0377 - acc: 0.9875 - binary_accuracy: 0.9875 - val_loss: 0.6305 - val_acc: 0.8244 - val_binary_accuracy: 0.8244\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model_v4():    # added word2vec\n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix], trainable=False,input_length=70))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(256, #!!!!!!!!!!!!!!!!!!!\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m = get_cnn_model_v4()\n",
    "check_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/step\n",
      "Test set\n",
      "  Loss: 1.314\n",
      "  Accuracy: 0.640\n"
     ]
    }
   ],
   "source": [
    "accr = m.evaluate(valid_seq_x,valid_y)   \n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. Dataset preparation3. Model Building\n",
    "# load the dataset\n",
    "data = open('data/corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'],test_size=0.1)\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.1 Count Vectors as features\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.2 TF-IDF Vectors as features\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,5), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,5), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.3 Word Embeddings\n",
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.0897      0.016      -0.0571     ...  0.1559     -0.0254\n",
      "  -0.0259    ]\n",
      " [-0.0314      0.0149     -0.0205     ...  0.098       0.0893\n",
      "   0.0148    ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.0897     -0.0682      0.0726     ...  0.206       0.0839\n",
      "   0.1557    ]\n",
      " [-0.0857      0.0475      0.14489999 ...  0.0586      0.11\n",
      "   0.0743    ]]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.4 Text / NLP based features\n",
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.5 Topic Models as features\n",
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. Model Building\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)    \n",
    "    #print(predictions[0:20])\n",
    "    #print(valid_y[0:20])\n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.835\n",
      "NB, WordLevel TF-IDF:  0.85\n",
      "NB, N-Gram Vectors:  0.829\n",
      "NB, CharLevel Vectors:  0.815\n"
     ]
    }
   ],
   "source": [
    "#3.1 Naive Bayes\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB, Count Vectors:  0.8488\n",
    "NB, WordLevel TF-IDF:  0.8524\n",
    "NB, N-Gram Vectors:  0.8448\n",
    "NB, CharLevel Vectors:  0.8108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.868\n",
      "LR, WordLevel TF-IDF:  0.873\n",
      "LR, N-Gram Vectors:  0.835\n",
      "LR, CharLevel Vectors:  0.839\n"
     ]
    }
   ],
   "source": [
    "#3.2 Linear Classifier\n",
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF:  0.874\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(linear_model.LogisticRegression(penalty='l2',C=5.0), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With test size of 0.1 nest LR model is giving accuracy of 88.2 , we can also see how it works with word count dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF:  0.873\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(linear_model.LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.01,\n",
    "          verbose=0, warm_start=False), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_seq_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bcc0ecaf3efa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_seq_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_seq_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"LR, Embedding: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_seq_x' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(linear_model.LogisticRegression(), train_seq_x, train_y, valid_seq_x)\n",
    "print (\"LR, Embedding: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "LR, Count Vectors:  0.8688\n",
    "LR, WordLevel TF-IDF:  0.872\n",
    "LR, N-Gram Vectors:  0.8364\n",
    "LR, CharLevel Vectors:  0.847\n",
    "best LR, WordLevel TF-IDF:  0.882\n",
    "LR, Embedding:  0.4888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.836\n",
      "SVM,WordLevel WordCount:  0.833\n",
      "SVM,WordLevel TF-IDF:  0.876\n",
      "SVM,CharLevel Vectors:  0.854\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(svm.SVC(kernel='linear'), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"SVM, N-Gram Vectors: \", accuracy)\n",
    "accuracy = train_model(svm.SVC(kernel='linear'), xtrain_count, train_y, xvalid_count)\n",
    "print (\"SVM,WordLevel WordCount: \", accuracy)\n",
    "accuracy = train_model(svm.SVC(kernel='linear'), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"SVM,WordLevel TF-IDF: \", accuracy)\n",
    "accuracy = train_model(svm.SVC(kernel='linear'), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"SVM,CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SVM, N-Gram Vectors:  0.8168\n",
    "SVM,WordLevel WordCount:  0.8464\n",
    "SVM,WordLevel TF-IDF:  0.87\n",
    "SVM,CharLevel Vectors:  0.8352\n",
    "\n",
    "with test size of 0.1\n",
    "SVM, N-Gram Vectors:  0.836\n",
    "SVM,WordLevel WordCount:  0.833\n",
    "SVM,WordLevel TF-IDF:  0.876\n",
    "SVM,CharLevel Vectors:  0.854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.8248\n",
      "SVM,Count Vectors:  0.8488\n",
      "SVM,WordLevel TF-IDF:  0.8568\n",
      "SVM,CharLevel Vectors:  0.8312\n"
     ]
    }
   ],
   "source": [
    "#3.3 Implementing a SVM Model\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(C=100.0, gamma= 0.001), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"SVM, N-Gram Vectors: \", accuracy)\n",
    "accuracy = train_model(svm.SVC(C=100.0, gamma= 0.001), xtrain_count, train_y, xvalid_count)\n",
    "print (\"SVM,Count Vectors: \", accuracy)\n",
    "accuracy = train_model(svm.SVC(C=100.0, gamma= 0.001), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"SVM,WordLevel TF-IDF: \", accuracy)\n",
    "accuracy = train_model(svm.SVC(C=100.0, gamma= 0.001), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"SVM,CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SVM, N-Gram Vectors:  0.84\n",
    "SVM,Count Vectors:  0.856\n",
    "SVM,WordLevel TF-IDF:  0.8704\n",
    "SVM,CharLevel Vectors:  0.836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3.4 Bagging Model\n",
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(n_estimators=100,criterion='entropy'), xtrain_count, train_y, xvalid_count)\n",
    "print (\"RF, Count Vectors: \", accuracy)\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(n_estimators=100,criterion='entropy'), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RF, Count Vectors:  0.8464\n",
    "RF, WordLevel TF-IDF:  0.8344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0]\n",
      "[0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0]\n",
      "Xgb, Count Vectors:  0.8452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0]\n",
      "[0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0]\n",
      "Xgb, WordLevel TF-IDF:  0.8288\n",
      "[0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0]\n",
      "[0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0]\n",
      "Xgb, CharLevel Vectors:  0.8048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#3.5 Boosting Model\n",
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(max_depth=7,\n",
    "                           min_child_weight=1,\n",
    "                           learning_rate=0.1,\n",
    "                           n_estimators=500,\n",
    "                           silent=True,\n",
    "                           objective='binary:logistic',\n",
    "                           gamma=0,\n",
    "                           max_delta_step=0,\n",
    "                           subsample=1,\n",
    "                           colsample_bytree=1,\n",
    "                           colsample_bylevel=1,\n",
    "                           reg_alpha=0,\n",
    "                           reg_lambda=0,\n",
    "                           scale_pos_weight=1,\n",
    "                           seed=1,\n",
    "                           missing=None), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier( learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=4,\n",
    " min_child_weight=6,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " reg_alpha=0.005,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, TF IDF Vectors:  0.857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=1,\n",
    " colsample_bytree=1,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print (\"Xgb, TF IDF Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Naive TF IDF Vectors:  0.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print (\"Xgb, Naive TF IDF Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Xgb, Count Vectors: 0.8584\n",
    "Xgb, WordLevel TF-IDF: 0.8356\n",
    "Xgb, CharLevel Vectors:  0.8152\n",
    "Xgb, TF IDF Vectors:  0.8284\n",
    "Xgb, Naive TF IDF Vectors:  0.7916\n",
    "\n",
    "with test size of 0.1\n",
    "Xgb, TF IDF Vectors:  0.857\n",
    "Xgb, Naive TF IDF Vectors:  0.815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_jobs', 'multi_class', 'fit_intercept', 'warm_start', 'C', 'solver', 'random_state', 'verbose', 'max_iter', 'class_weight', 'tol', 'penalty', 'dual', 'intercept_scaling'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.LogisticRegression().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(linear_model.LogisticRegression(), grid,cv=10)\n",
    "gs_clf = gs_clf.fit(xtrain_tfidf, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8698888888888889"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.51000, std: 0.00000, params: {'penalty': 'l1', 'C': 0.001},\n",
       " mean: 0.51233, std: 0.00225, params: {'penalty': 'l2', 'C': 0.001},\n",
       " mean: 0.51000, std: 0.00000, params: {'penalty': 'l1', 'C': 0.01},\n",
       " mean: 0.77911, std: 0.00906, params: {'penalty': 'l2', 'C': 0.01},\n",
       " mean: 0.74689, std: 0.01393, params: {'penalty': 'l1', 'C': 0.1},\n",
       " mean: 0.83522, std: 0.01089, params: {'penalty': 'l2', 'C': 0.1},\n",
       " mean: 0.85589, std: 0.01231, params: {'penalty': 'l1', 'C': 1.0},\n",
       " mean: 0.86989, std: 0.01019, params: {'penalty': 'l2', 'C': 1.0},\n",
       " mean: 0.84944, std: 0.01480, params: {'penalty': 'l1', 'C': 10.0},\n",
       " mean: 0.86844, std: 0.01044, params: {'penalty': 'l2', 'C': 10.0},\n",
       " mean: 0.83344, std: 0.01140, params: {'penalty': 'l1', 'C': 100.0},\n",
       " mean: 0.84922, std: 0.01082, params: {'penalty': 'l2', 'C': 100.0},\n",
       " mean: 0.83078, std: 0.01159, params: {'penalty': 'l1', 'C': 1000.0},\n",
       " mean: 0.84000, std: 0.01230, params: {'penalty': 'l2', 'C': 1000.0}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_estimator_gs_clf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test = {\n",
    " 'max_depth':[4,5,6],\n",
    " 'min_child_weight':[4,5,6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsearch = GridSearchCV(estimator = xgboost.XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=2, gamma=0, subsample=1, colsample_bytree=1,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test, scoring='roc_auc',n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_model4 = gsearch.fit(xtrain_tfidf, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.90086, std: 0.00353, params: {'max_depth': 4, 'min_child_weight': 4},\n",
       " mean: 0.90122, std: 0.00439, params: {'max_depth': 4, 'min_child_weight': 5},\n",
       " mean: 0.90054, std: 0.00325, params: {'max_depth': 4, 'min_child_weight': 6},\n",
       " mean: 0.90449, std: 0.00437, params: {'max_depth': 5, 'min_child_weight': 4},\n",
       " mean: 0.90474, std: 0.00558, params: {'max_depth': 5, 'min_child_weight': 5},\n",
       " mean: 0.90437, std: 0.00324, params: {'max_depth': 5, 'min_child_weight': 6},\n",
       " mean: 0.90732, std: 0.00553, params: {'max_depth': 6, 'min_child_weight': 4},\n",
       " mean: 0.90706, std: 0.00379, params: {'max_depth': 6, 'min_child_weight': 5},\n",
       " mean: 0.90568, std: 0.00373, params: {'max_depth': 6, 'min_child_weight': 6}]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model4.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9073239176666685"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=6, min_child_weight=4, missing=None, n_estimators=140,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model4.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb,Grid search TF IDF Vectors:  0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=6, min_child_weight=4, missing=None, n_estimators=140,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
    "       subsample=1), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print (\"Xgb,Grid search TF IDF Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Xgb,Grid search TF IDF Vectors:  0.8156\n",
    "\n",
    "with test size of 0.1 \n",
    "Xgb,Grid search TF IDF Vectors:  0.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. Model Building\n",
    "def train_model_new(classifier, feature_vector_train, label, feature_vector_valid, epoch, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label,epochs=epoch,validation_data=(feature_vector_valid, valid_y))\n",
    "    \n",
    "    print(classifier.evaluate(feature_vector_valid,valid_y))\n",
    "    # predict the labels on validation dataset\n",
    "    #predictions = classifier.predict(feature_vector_valid,verbose=1) \n",
    "    #print(predictions[0:20])\n",
    "    #predictions = [1 if x >= 0.05 else 0 for x in predictions]\n",
    "    #if is_neural_net:\n",
    "        #predictions = predictions.argmax(axis=1)\n",
    "    \n",
    "    #print(predictions[0:20])\n",
    "    #print(valid_y[0:20])\n",
    "    #print(metrics.accuracy_score(valid_y,predictions))\n",
    "    #return metrics.roc_auc_score(valid_y,predictions)\n",
    "    return classifier.evaluate(feature_vector_valid,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 36s 4ms/step - loss: 0.4370 - acc: 0.7998 - val_loss: 0.3455 - val_acc: 0.8470\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 34s 4ms/step - loss: 0.2318 - acc: 0.9033 - val_loss: 0.3888 - val_acc: 0.8390\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 35s 4ms/step - loss: 0.1377 - acc: 0.9488 - val_loss: 0.4970 - val_acc: 0.8270\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 34s 4ms/step - loss: 0.0687 - acc: 0.9793 - val_loss: 0.6039 - val_acc: 0.8240\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 33s 4ms/step - loss: 0.0293 - acc: 0.9936 - val_loss: 0.7255 - val_acc: 0.8250\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 33s 4ms/step - loss: 0.0121 - acc: 0.9986 - val_loss: 0.8208 - val_acc: 0.8230\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 34s 4ms/step - loss: 0.0065 - acc: 0.9988 - val_loss: 0.8898 - val_acc: 0.8240\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 34s 4ms/step - loss: 0.0046 - acc: 0.9989 - val_loss: 0.9358 - val_acc: 0.8280\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 34s 4ms/step - loss: 0.0039 - acc: 0.9986 - val_loss: 0.9736 - val_acc: 0.8230\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 34s 4ms/step - loss: 0.0036 - acc: 0.9987 - val_loss: 1.0063 - val_acc: 0.8240\n",
      "1000/1000 [==============================] - 0s 146us/step\n",
      "[1.0063470530509948, 0.824]\n",
      "1000/1000 [==============================] - 0s 142us/step\n",
      "NN, Ngram Level TF IDF Vectors [1.0063470530509948, 0.824]\n"
     ]
    }
   ],
   "source": [
    "#3.6 Shallow Neural Networks\n",
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(1000, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['acc'])\n",
    "    return classifier \n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy = train_model_new(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram,10, is_neural_net=True)\n",
    "print (\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/4\n",
    "9000/9000 [==============================] - 35s 4ms/step - loss: 0.4346 - acc: 0.8030\n",
    "Epoch 2/4\n",
    "9000/9000 [==============================] - 33s 4ms/step - loss: 0.2230 - acc: 0.9084\n",
    "Epoch 3/4\n",
    "9000/9000 [==============================] - 33s 4ms/step - loss: 0.1362 - acc: 0.9504\n",
    "Epoch 4/4\n",
    "9000/9000 [==============================] - 37s 4ms/step - loss: 0.0689 - acc: 0.9773\n",
    "NN, Ngram Level TF IDF Vectors 0.8202636779425264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              5001000   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 5,002,001\n",
      "Trainable params: 5,002,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "9000/9000 [==============================] - 36s 4ms/step - loss: 0.6962 - acc: 0.5183 - val_loss: 0.6716 - val_acc: 0.5200\n",
      "Epoch 2/30\n",
      "9000/9000 [==============================] - 31s 3ms/step - loss: 0.5693 - acc: 0.7046 - val_loss: 0.3803 - val_acc: 0.8370\n",
      "Epoch 3/30\n",
      "9000/9000 [==============================] - 31s 3ms/step - loss: 0.4477 - acc: 0.8032 - val_loss: 0.3386 - val_acc: 0.8500\n",
      "Epoch 4/30\n",
      "9000/9000 [==============================] - 31s 3ms/step - loss: 0.4122 - acc: 0.8199 - val_loss: 0.3243 - val_acc: 0.8670\n",
      "Epoch 5/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.3826 - acc: 0.8362 - val_loss: 0.3358 - val_acc: 0.8660\n",
      "Epoch 6/30\n",
      "9000/9000 [==============================] - 29s 3ms/step - loss: 0.3559 - acc: 0.8493 - val_loss: 0.3262 - val_acc: 0.8530\n",
      "Epoch 7/30\n",
      "9000/9000 [==============================] - 33s 4ms/step - loss: 0.3375 - acc: 0.8634 - val_loss: 0.3176 - val_acc: 0.8710\n",
      "Epoch 8/30\n",
      "9000/9000 [==============================] - 35s 4ms/step - loss: 0.3165 - acc: 0.8746 - val_loss: 0.3117 - val_acc: 0.8770\n",
      "Epoch 9/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.2984 - acc: 0.8828 - val_loss: 0.3015 - val_acc: 0.8690\n",
      "Epoch 10/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.2838 - acc: 0.8870 - val_loss: 0.3183 - val_acc: 0.8740\n",
      "Epoch 11/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.2681 - acc: 0.8920 - val_loss: 0.3006 - val_acc: 0.8740\n",
      "Epoch 12/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.2471 - acc: 0.9063 - val_loss: 0.3934 - val_acc: 0.8450\n",
      "Epoch 13/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.2264 - acc: 0.9123 - val_loss: 0.3241 - val_acc: 0.8790\n",
      "Epoch 14/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.2107 - acc: 0.9200 - val_loss: 0.3179 - val_acc: 0.8750\n",
      "Epoch 15/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.2134 - acc: 0.9174 - val_loss: 0.3501 - val_acc: 0.8730\n",
      "Epoch 16/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1978 - acc: 0.9247 - val_loss: 0.3861 - val_acc: 0.8680\n",
      "Epoch 17/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1658 - acc: 0.9403 - val_loss: 0.3758 - val_acc: 0.8790\n",
      "Epoch 18/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1756 - acc: 0.9330 - val_loss: 0.3616 - val_acc: 0.8800\n",
      "Epoch 19/30\n",
      "9000/9000 [==============================] - 34s 4ms/step - loss: 0.1642 - acc: 0.9363 - val_loss: 0.3998 - val_acc: 0.8760\n",
      "Epoch 20/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1587 - acc: 0.9421 - val_loss: 0.3945 - val_acc: 0.8730\n",
      "Epoch 21/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1510 - acc: 0.9443 - val_loss: 0.4184 - val_acc: 0.8710\n",
      "Epoch 22/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1443 - acc: 0.9486 - val_loss: 0.4138 - val_acc: 0.8700\n",
      "Epoch 23/30\n",
      "9000/9000 [==============================] - 33s 4ms/step - loss: 0.1455 - acc: 0.9470 - val_loss: 0.4113 - val_acc: 0.8740\n",
      "Epoch 24/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1294 - acc: 0.9520 - val_loss: 0.4233 - val_acc: 0.8710\n",
      "Epoch 25/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1321 - acc: 0.9491 - val_loss: 0.4638 - val_acc: 0.8710\n",
      "Epoch 26/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1274 - acc: 0.9556 - val_loss: 0.4327 - val_acc: 0.8820\n",
      "Epoch 27/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1308 - acc: 0.9518 - val_loss: 0.4471 - val_acc: 0.8800\n",
      "Epoch 28/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1253 - acc: 0.9572 - val_loss: 0.4211 - val_acc: 0.8890\n",
      "Epoch 29/30\n",
      "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1190 - acc: 0.9579 - val_loss: 0.4262 - val_acc: 0.8790\n",
      "Epoch 30/30\n",
      "9000/9000 [==============================] - 33s 4ms/step - loss: 0.1148 - acc: 0.9600 - val_loss: 0.4280 - val_acc: 0.8750\n",
      "1000/1000 [==============================] - 1s 1ms/step\n",
      "[0.42802009439468386, 0.875]\n",
      "1000/1000 [==============================] - 1s 1ms/step\n",
      "CNN, Word Embeddings [0.42802009439468386, 0.875]\n"
     ]
    }
   ],
   "source": [
    "#3.7 Deep Neural Networks\n",
    "#3.7.1 Convolutional Neural Network\n",
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.5)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(300, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    #pooling_layer = layers.AveragePooling1D()(conv_layer)\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "    \n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(100, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.5)(output_layer1)\n",
    "    output_layer11 = layers.Dense(25, activation=\"relu\")(output_layer1)\n",
    "    output_layer11 = layers.Dropout(0.5)(output_layer11)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer11)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "accuracy = train_model_new(classifier, train_seq_x, train_y, valid_seq_x, 30, is_neural_net=True)\n",
    "print (\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train on 9000 samples, validate on 1000 samples\n",
    "Epoch 1/10\n",
    "9000/9000 [==============================] - 17s 2ms/step - loss: 0.5483 - acc: 0.7082 - val_loss: 0.3757 - val_acc: 0.8440\n",
    "Epoch 2/10\n",
    "9000/9000 [==============================] - 17s 2ms/step - loss: 0.3739 - acc: 0.8374 - val_loss: 0.3283 - val_acc: 0.8660\n",
    "Epoch 3/10\n",
    "9000/9000 [==============================] - 16s 2ms/step - loss: 0.3168 - acc: 0.8664 - val_loss: 0.3108 - val_acc: 0.8660\n",
    "Epoch 4/10\n",
    "9000/9000 [==============================] - 17s 2ms/step - loss: 0.2672 - acc: 0.8901 - val_loss: 0.3203 - val_acc: 0.8580\n",
    "Epoch 5/10\n",
    "9000/9000 [==============================] - 17s 2ms/step - loss: 0.2414 - acc: 0.8999 - val_loss: 0.3112 - val_acc: 0.8650\n",
    "Epoch 6/10\n",
    "9000/9000 [==============================] - 17s 2ms/step - loss: 0.2031 - acc: 0.9238 - val_loss: 0.3186 - val_acc: 0.8680\n",
    "Epoch 7/10\n",
    "9000/9000 [==============================] - 18s 2ms/step - loss: 0.1634 - acc: 0.9372 - val_loss: 0.3192 - val_acc: 0.8680\n",
    "Epoch 8/10\n",
    "9000/9000 [==============================] - 18s 2ms/step - loss: 0.1475 - acc: 0.9441 - val_loss: 0.3090 - val_acc: 0.8640\n",
    "Epoch 9/10\n",
    "9000/9000 [==============================] - 19s 2ms/step - loss: 0.1086 - acc: 0.9607 - val_loss: 0.3466 - val_acc: 0.8700\n",
    "Epoch 10/10\n",
    "9000/9000 [==============================] - 18s 2ms/step - loss: 0.0918 - acc: 0.9664 - val_loss: 0.3961 - val_acc: 0.8570\n",
    "1000/1000 [==============================] - 1s 645us/step\n",
    "[0.396127006329596, 0.857]\n",
    "1000/1000 [==============================] - 1s 697us/step\n",
    "CNN, Word Embeddings [0.396127006329596, 0.857]\n",
    "\n",
    "Train on 9000 samples, validate on 1000 samples\n",
    "Epoch 1/10\n",
    "9000/9000 [==============================] - 24s 3ms/step - loss: 0.5527 - acc: 0.7001 - val_loss: 0.3671 - val_acc: 0.8490\n",
    "Epoch 2/10\n",
    "9000/9000 [==============================] - 24s 3ms/step - loss: 0.3656 - acc: 0.8398 - val_loss: 0.3298 - val_acc: 0.8690\n",
    "Epoch 3/10\n",
    "9000/9000 [==============================] - 23s 3ms/step - loss: 0.2964 - acc: 0.8787 - val_loss: 0.3103 - val_acc: 0.8670\n",
    "Epoch 4/10\n",
    "9000/9000 [==============================] - 23s 3ms/step - loss: 0.2445 - acc: 0.8991 - val_loss: 0.3340 - val_acc: 0.8660\n",
    "Epoch 5/10\n",
    "9000/9000 [==============================] - 23s 3ms/step - loss: 0.1912 - acc: 0.9258 - val_loss: 0.3391 - val_acc: 0.8710\n",
    "Epoch 6/10\n",
    "9000/9000 [==============================] - 24s 3ms/step - loss: 0.1538 - acc: 0.9416 - val_loss: 0.3388 - val_acc: 0.8750\n",
    "Epoch 7/10\n",
    "9000/9000 [==============================] - 24s 3ms/step - loss: 0.1161 - acc: 0.9546 - val_loss: 0.3609 - val_acc: 0.8700\n",
    "Epoch 8/10\n",
    "9000/9000 [==============================] - 25s 3ms/step - loss: 0.0933 - acc: 0.9661 - val_loss: 0.3731 - val_acc: 0.8730\n",
    "Epoch 9/10\n",
    "9000/9000 [==============================] - 25s 3ms/step - loss: 0.0842 - acc: 0.9690 - val_loss: 0.3969 - val_acc: 0.8810\n",
    "Epoch 10/10\n",
    "9000/9000 [==============================] - 25s 3ms/step - loss: 0.0619 - acc: 0.9777 - val_loss: 0.4201 - val_acc: 0.8830\n",
    "1000/1000 [==============================] - 1s 929us/step\n",
    "[0.4201457166671753, 0.883]\n",
    "1000/1000 [==============================] - 1s 967us/step\n",
    "CNN, Word Embeddings [0.4201457166671753, 0.883]\n",
    "\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_6 (InputLayer)         (None, 70)                0         \n",
    "_________________________________________________________________\n",
    "embedding_5 (Embedding)      (None, 70, 300)           10041000  \n",
    "_________________________________________________________________\n",
    "spatial_dropout1d_5 (Spatial (None, 70, 300)           0         \n",
    "_________________________________________________________________\n",
    "conv1d_5 (Conv1D)            (None, 68, 200)           180200    \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_5 (Glob (None, 200)               0         \n",
    "_________________________________________________________________\n",
    "dense_11 (Dense)             (None, 100)               20100     \n",
    "_________________________________________________________________\n",
    "dropout_5 (Dropout)          (None, 100)               0         \n",
    "_________________________________________________________________\n",
    "dense_12 (Dense)             (None, 10)                1010      \n",
    "_________________________________________________________________\n",
    "dense_13 (Dense)             (None, 1)                 11        \n",
    "=================================================================\n",
    "Total params: 10,242,321\n",
    "Trainable params: 201,321\n",
    "Non-trainable params: 10,041,000\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-8a0a6a515563>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-8a0a6a515563>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Train on 9000 samples, validate on 1000 samples\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Train on 9000 samples, validate on 1000 samples\n",
    "Epoch 1/30\n",
    "9000/9000 [==============================] - 36s 4ms/step - loss: 0.6962 - acc: 0.5183 - val_loss: 0.6716 - val_acc: 0.5200\n",
    "Epoch 2/30\n",
    "9000/9000 [==============================] - 31s 3ms/step - loss: 0.5693 - acc: 0.7046 - val_loss: 0.3803 - val_acc: 0.8370\n",
    "Epoch 3/30\n",
    "9000/9000 [==============================] - 31s 3ms/step - loss: 0.4477 - acc: 0.8032 - val_loss: 0.3386 - val_acc: 0.8500\n",
    "Epoch 4/30\n",
    "9000/9000 [==============================] - 31s 3ms/step - loss: 0.4122 - acc: 0.8199 - val_loss: 0.3243 - val_acc: 0.8670\n",
    "Epoch 5/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.3826 - acc: 0.8362 - val_loss: 0.3358 - val_acc: 0.8660\n",
    "Epoch 6/30\n",
    "9000/9000 [==============================] - 29s 3ms/step - loss: 0.3559 - acc: 0.8493 - val_loss: 0.3262 - val_acc: 0.8530\n",
    "Epoch 7/30\n",
    "9000/9000 [==============================] - 33s 4ms/step - loss: 0.3375 - acc: 0.8634 - val_loss: 0.3176 - val_acc: 0.8710\n",
    "Epoch 8/30\n",
    "9000/9000 [==============================] - 35s 4ms/step - loss: 0.3165 - acc: 0.8746 - val_loss: 0.3117 - val_acc: 0.8770\n",
    "Epoch 9/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.2984 - acc: 0.8828 - val_loss: 0.3015 - val_acc: 0.8690\n",
    "Epoch 10/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.2838 - acc: 0.8870 - val_loss: 0.3183 - val_acc: 0.8740\n",
    "Epoch 11/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.2681 - acc: 0.8920 - val_loss: 0.3006 - val_acc: 0.8740\n",
    "Epoch 12/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.2471 - acc: 0.9063 - val_loss: 0.3934 - val_acc: 0.8450\n",
    "Epoch 13/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.2264 - acc: 0.9123 - val_loss: 0.3241 - val_acc: 0.8790\n",
    "Epoch 14/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.2107 - acc: 0.9200 - val_loss: 0.3179 - val_acc: 0.8750\n",
    "Epoch 15/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.2134 - acc: 0.9174 - val_loss: 0.3501 - val_acc: 0.8730\n",
    "Epoch 16/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1978 - acc: 0.9247 - val_loss: 0.3861 - val_acc: 0.8680\n",
    "Epoch 17/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1658 - acc: 0.9403 - val_loss: 0.3758 - val_acc: 0.8790\n",
    "Epoch 18/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1756 - acc: 0.9330 - val_loss: 0.3616 - val_acc: 0.8800\n",
    "Epoch 19/30\n",
    "9000/9000 [==============================] - 34s 4ms/step - loss: 0.1642 - acc: 0.9363 - val_loss: 0.3998 - val_acc: 0.8760\n",
    "Epoch 20/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1587 - acc: 0.9421 - val_loss: 0.3945 - val_acc: 0.8730\n",
    "Epoch 21/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1510 - acc: 0.9443 - val_loss: 0.4184 - val_acc: 0.8710\n",
    "Epoch 22/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1443 - acc: 0.9486 - val_loss: 0.4138 - val_acc: 0.8700\n",
    "Epoch 23/30\n",
    "9000/9000 [==============================] - 33s 4ms/step - loss: 0.1455 - acc: 0.9470 - val_loss: 0.4113 - val_acc: 0.8740\n",
    "Epoch 24/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1294 - acc: 0.9520 - val_loss: 0.4233 - val_acc: 0.8710\n",
    "Epoch 25/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1321 - acc: 0.9491 - val_loss: 0.4638 - val_acc: 0.8710\n",
    "Epoch 26/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1274 - acc: 0.9556 - val_loss: 0.4327 - val_acc: 0.8820\n",
    "Epoch 27/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1308 - acc: 0.9518 - val_loss: 0.4471 - val_acc: 0.8800\n",
    "Epoch 28/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1253 - acc: 0.9572 - val_loss: 0.4211 - val_acc: 0.8890\n",
    "Epoch 29/30\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.1190 - acc: 0.9579 - val_loss: 0.4262 - val_acc: 0.8790\n",
    "Epoch 30/30\n",
    "9000/9000 [==============================] - 33s 4ms/step - loss: 0.1148 - acc: 0.9600 - val_loss: 0.4280 - val_acc: 0.8750\n",
    "1000/1000 [==============================] - 1s 1ms/step\n",
    "[0.42802009439468386, 0.875]\n",
    "1000/1000 [==============================] - 1s 1ms/step\n",
    "CNN, Word Embeddings [0.42802009439468386, 0.875]\n",
    "\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_4 (InputLayer)         (None, 70)                0         \n",
    "_________________________________________________________________\n",
    "embedding_3 (Embedding)      (None, 70, 300)           10041000  \n",
    "_________________________________________________________________\n",
    "spatial_dropout1d_3 (Spatial (None, 70, 300)           0         \n",
    "_________________________________________________________________\n",
    "conv1d_3 (Conv1D)            (None, 68, 300)           270300    \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_3 (Glob (None, 300)               0         \n",
    "_________________________________________________________________\n",
    "dense_9 (Dense)              (None, 100)               30100     \n",
    "_________________________________________________________________\n",
    "dropout_5 (Dropout)          (None, 100)               0         \n",
    "_________________________________________________________________\n",
    "dense_10 (Dense)             (None, 25)                2525      \n",
    "_________________________________________________________________\n",
    "dropout_6 (Dropout)          (None, 25)                0         \n",
    "_________________________________________________________________\n",
    "dense_11 (Dense)             (None, 1)                 26        \n",
    "=================================================================\n",
    "Total params: 10,343,951\n",
    "Trainable params: 302,951\n",
    "Non-trainable params: 10,041,000\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 70, 300)           10041000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 70, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 68, 300)           270300    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 10,343,951\n",
      "Trainable params: 302,951\n",
      "Non-trainable params: 10,041,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "9000/9000 [==============================] - 117s 13ms/step - loss: 0.6170 - acc: 0.6604 - val_loss: 0.4988 - val_acc: 0.7720\n",
      "Epoch 2/20\n",
      "9000/9000 [==============================] - 115s 13ms/step - loss: 0.5343 - acc: 0.7530 - val_loss: 0.4759 - val_acc: 0.8030\n",
      "Epoch 3/20\n",
      "9000/9000 [==============================] - 115s 13ms/step - loss: 0.4945 - acc: 0.7779 - val_loss: 0.4256 - val_acc: 0.8010\n",
      "Epoch 4/20\n",
      "9000/9000 [==============================] - 117s 13ms/step - loss: 0.4662 - acc: 0.7936 - val_loss: 0.4449 - val_acc: 0.8230\n",
      "Epoch 5/20\n",
      "9000/9000 [==============================] - 114s 13ms/step - loss: 0.4283 - acc: 0.8131 - val_loss: 0.3773 - val_acc: 0.8410\n",
      "Epoch 6/20\n",
      "9000/9000 [==============================] - 114s 13ms/step - loss: 0.4078 - acc: 0.8219 - val_loss: 0.3663 - val_acc: 0.8490\n",
      "Epoch 7/20\n",
      "9000/9000 [==============================] - 114s 13ms/step - loss: 0.3868 - acc: 0.8356 - val_loss: 0.3500 - val_acc: 0.8660\n",
      "Epoch 8/20\n",
      "9000/9000 [==============================] - 114s 13ms/step - loss: 0.3583 - acc: 0.8498 - val_loss: 0.3367 - val_acc: 0.8480\n",
      "Epoch 9/20\n",
      "9000/9000 [==============================] - 114s 13ms/step - loss: 0.3550 - acc: 0.8504 - val_loss: 0.3321 - val_acc: 0.8620\n",
      "Epoch 10/20\n",
      "9000/9000 [==============================] - 113s 13ms/step - loss: 0.3288 - acc: 0.8641 - val_loss: 0.3392 - val_acc: 0.8580\n",
      "Epoch 11/20\n",
      "9000/9000 [==============================] - 113s 13ms/step - loss: 0.3177 - acc: 0.8710 - val_loss: 0.3239 - val_acc: 0.8570\n",
      "Epoch 12/20\n",
      "9000/9000 [==============================] - 114s 13ms/step - loss: 0.2962 - acc: 0.8773 - val_loss: 0.3282 - val_acc: 0.8720\n",
      "Epoch 13/20\n",
      "9000/9000 [==============================] - 113s 13ms/step - loss: 0.2949 - acc: 0.8793 - val_loss: 0.3273 - val_acc: 0.8630\n",
      "Epoch 14/20\n",
      "9000/9000 [==============================] - 114s 13ms/step - loss: 0.2769 - acc: 0.8890 - val_loss: 0.3853 - val_acc: 0.8580\n",
      "Epoch 15/20\n",
      "9000/9000 [==============================] - 113s 13ms/step - loss: 0.2615 - acc: 0.8953 - val_loss: 0.3556 - val_acc: 0.8680\n",
      "Epoch 16/20\n",
      "9000/9000 [==============================] - 113s 13ms/step - loss: 0.2451 - acc: 0.9016 - val_loss: 0.3565 - val_acc: 0.8610\n",
      "Epoch 17/20\n",
      "9000/9000 [==============================] - 113s 13ms/step - loss: 0.2294 - acc: 0.9107 - val_loss: 0.3595 - val_acc: 0.8550\n",
      "Epoch 18/20\n",
      "9000/9000 [==============================] - 113s 13ms/step - loss: 0.2193 - acc: 0.9114 - val_loss: 0.4043 - val_acc: 0.8670\n",
      "Epoch 19/20\n",
      "9000/9000 [==============================] - 114s 13ms/step - loss: 0.2048 - acc: 0.9218 - val_loss: 0.4984 - val_acc: 0.8570\n",
      "Epoch 20/20\n",
      "9000/9000 [==============================] - 113s 13ms/step - loss: 0.1935 - acc: 0.9241 - val_loss: 0.4303 - val_acc: 0.8650\n",
      "1000/1000 [==============================] - 4s 4ms/step\n",
      "[0.43025758779048917, 0.865]\n",
      "1000/1000 [==============================] - 5s 5ms/step\n",
      "RNN-LSTM, Word Embeddings [0.43025758779048917, 0.865]\n"
     ]
    }
   ],
   "source": [
    "#3.7.2 Recurrent Neural Network – LSTM\n",
    "# Add an Input Layer\n",
    "def create_rnn_lstm():\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(300)(embedding_layer)\n",
    "    lstm_layer = layers.Dropout(0.3)(lstm_layer)\n",
    "    \n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(100, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.3)(output_layer1)\n",
    "    output_layer11 = layers.Dense(10, activation=\"relu\")(output_layer1)\n",
    "    output_layer11 = layers.Dropout(0.3)(output_layer11)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer11)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "accuracy = train_model_new(classifier, train_seq_x, train_y, valid_seq_x, 20, is_neural_net=True)\n",
    "print (\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train on 9000 samples, validate on 1000 samples\n",
    "Epoch 1/5\n",
    "9000/9000 [==============================] - 38s 4ms/step - loss: 0.6367 - acc: 0.6231 - val_loss: 0.5328 - val_acc: 0.7560\n",
    "Epoch 2/5\n",
    "9000/9000 [==============================] - 34s 4ms/step - loss: 0.5233 - acc: 0.7596 - val_loss: 0.4308 - val_acc: 0.8130\n",
    "Epoch 3/5\n",
    "9000/9000 [==============================] - 34s 4ms/step - loss: 0.4925 - acc: 0.7810 - val_loss: 0.4216 - val_acc: 0.8220\n",
    "Epoch 4/5\n",
    "9000/9000 [==============================] - 34s 4ms/step - loss: 0.4643 - acc: 0.7889 - val_loss: 0.4107 - val_acc: 0.8240\n",
    "Epoch 5/5\n",
    "9000/9000 [==============================] - 34s 4ms/step - loss: 0.4509 - acc: 0.7969 - val_loss: 0.3704 - val_acc: 0.8270\n",
    "1000/1000 [==============================] - 2s 2ms/step\n",
    "[0.37039316987991333, 0.827]\n",
    "1000/1000 [==============================] - 2s 2ms/step\n",
    "RNN-LSTM, Word Embeddings [0.37039316987991333, 0.827]\n",
    "\n",
    "Train on 9000 samples, validate on 1000 samples\n",
    "Epoch 1/20\n",
    "9000/9000 [==============================] - 117s 13ms/step - loss: 0.6170 - acc: 0.6604 - val_loss: 0.4988 - val_acc: 0.7720\n",
    "Epoch 2/20\n",
    "9000/9000 [==============================] - 115s 13ms/step - loss: 0.5343 - acc: 0.7530 - val_loss: 0.4759 - val_acc: 0.8030\n",
    "Epoch 3/20\n",
    "9000/9000 [==============================] - 115s 13ms/step - loss: 0.4945 - acc: 0.7779 - val_loss: 0.4256 - val_acc: 0.8010\n",
    "Epoch 4/20\n",
    "9000/9000 [==============================] - 117s 13ms/step - loss: 0.4662 - acc: 0.7936 - val_loss: 0.4449 - val_acc: 0.8230\n",
    "Epoch 5/20\n",
    "9000/9000 [==============================] - 114s 13ms/step - loss: 0.4283 - acc: 0.8131 - val_loss: 0.3773 - val_acc: 0.8410\n",
    "Epoch 6/20\n",
    "9000/9000 [==============================] - 114s 13ms/step - loss: 0.4078 - acc: 0.8219 - val_loss: 0.3663 - val_acc: 0.8490\n",
    "Epoch 7/20\n",
    "9000/9000 [==============================] - 114s 13ms/step - loss: 0.3868 - acc: 0.8356 - val_loss: 0.3500 - val_acc: 0.8660\n",
    "Epoch 8/20\n",
    "9000/9000 [==============================] - 114s 13ms/step - loss: 0.3583 - acc: 0.8498 - val_loss: 0.3367 - val_acc: 0.8480\n",
    "Epoch 9/20\n",
    "9000/9000 [==============================] - 114s 13ms/step - loss: 0.3550 - acc: 0.8504 - val_loss: 0.3321 - val_acc: 0.8620\n",
    "Epoch 10/20\n",
    "9000/9000 [==============================] - 113s 13ms/step - loss: 0.3288 - acc: 0.8641 - val_loss: 0.3392 - val_acc: 0.8580\n",
    "Epoch 11/20\n",
    "9000/9000 [==============================] - 113s 13ms/step - loss: 0.3177 - acc: 0.8710 - val_loss: 0.3239 - val_acc: 0.8570\n",
    "Epoch 12/20\n",
    "9000/9000 [==============================] - 114s 13ms/step - loss: 0.2962 - acc: 0.8773 - val_loss: 0.3282 - val_acc: 0.8720\n",
    "Epoch 13/20\n",
    "9000/9000 [==============================] - 113s 13ms/step - loss: 0.2949 - acc: 0.8793 - val_loss: 0.3273 - val_acc: 0.8630\n",
    "Epoch 14/20\n",
    "9000/9000 [==============================] - 114s 13ms/step - loss: 0.2769 - acc: 0.8890 - val_loss: 0.3853 - val_acc: 0.8580\n",
    "Epoch 15/20\n",
    "9000/9000 [==============================] - 113s 13ms/step - loss: 0.2615 - acc: 0.8953 - val_loss: 0.3556 - val_acc: 0.8680\n",
    "Epoch 16/20\n",
    "9000/9000 [==============================] - 113s 13ms/step - loss: 0.2451 - acc: 0.9016 - val_loss: 0.3565 - val_acc: 0.8610\n",
    "Epoch 17/20\n",
    "9000/9000 [==============================] - 113s 13ms/step - loss: 0.2294 - acc: 0.9107 - val_loss: 0.3595 - val_acc: 0.8550\n",
    "Epoch 18/20\n",
    "9000/9000 [==============================] - 113s 13ms/step - loss: 0.2193 - acc: 0.9114 - val_loss: 0.4043 - val_acc: 0.8670\n",
    "Epoch 19/20\n",
    "9000/9000 [==============================] - 114s 13ms/step - loss: 0.2048 - acc: 0.9218 - val_loss: 0.4984 - val_acc: 0.8570\n",
    "Epoch 20/20\n",
    "9000/9000 [==============================] - 113s 13ms/step - loss: 0.1935 - acc: 0.9241 - val_loss: 0.4303 - val_acc: 0.8650\n",
    "1000/1000 [==============================] - 4s 4ms/step\n",
    "[0.43025758779048917, 0.865]\n",
    "1000/1000 [==============================] - 5s 5ms/step\n",
    "RNN-LSTM, Word Embeddings [0.43025758779048917, 0.865]\n",
    "\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_2 (InputLayer)         (None, 70)                0         \n",
    "_________________________________________________________________\n",
    "embedding_1 (Embedding)      (None, 70, 300)           10041000  \n",
    "_________________________________________________________________\n",
    "spatial_dropout1d_1 (Spatial (None, 70, 300)           0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 300)               721200    \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 300)               0         \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 100)               30100     \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 100)               0         \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 10)                1010      \n",
    "_________________________________________________________________\n",
    "dropout_3 (Dropout)          (None, 10)                0         \n",
    "_________________________________________________________________\n",
    "dense_5 (Dense)              (None, 1)                 11        \n",
    "=================================================================\n",
    "Total params: 10,793,321\n",
    "Trainable params: 752,321\n",
    "Non-trainable params: 10,041,000\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 70, 300)           10041000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 70, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 10,793,321\n",
      "Trainable params: 752,321\n",
      "Non-trainable params: 10,041,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "9000/9000 [==============================] - 93s 10ms/step - loss: 0.6576 - acc: 0.6053 - val_loss: 0.5076 - val_acc: 0.7520\n",
      "Epoch 2/30\n",
      "9000/9000 [==============================] - 90s 10ms/step - loss: 0.4966 - acc: 0.7743 - val_loss: 0.3860 - val_acc: 0.8390\n",
      "Epoch 3/30\n",
      "9000/9000 [==============================] - 90s 10ms/step - loss: 0.4230 - acc: 0.8187 - val_loss: 0.3962 - val_acc: 0.8030\n",
      "Epoch 4/30\n",
      "9000/9000 [==============================] - 91s 10ms/step - loss: 0.4052 - acc: 0.8273 - val_loss: 0.3307 - val_acc: 0.8590\n",
      "Epoch 5/30\n",
      "9000/9000 [==============================] - 91s 10ms/step - loss: 0.3826 - acc: 0.8329 - val_loss: 0.3193 - val_acc: 0.8660\n",
      "Epoch 6/30\n",
      "9000/9000 [==============================] - 91s 10ms/step - loss: 0.3723 - acc: 0.8399 - val_loss: 0.3342 - val_acc: 0.8700\n",
      "Epoch 7/30\n",
      "9000/9000 [==============================] - 91s 10ms/step - loss: 0.3559 - acc: 0.8503 - val_loss: 0.3354 - val_acc: 0.8530\n",
      "Epoch 8/30\n",
      "9000/9000 [==============================] - 91s 10ms/step - loss: 0.3387 - acc: 0.8571 - val_loss: 0.3361 - val_acc: 0.8530\n",
      "Epoch 9/30\n",
      "9000/9000 [==============================] - 91s 10ms/step - loss: 0.3306 - acc: 0.8640 - val_loss: 0.3082 - val_acc: 0.8610\n",
      "Epoch 10/30\n",
      "9000/9000 [==============================] - 91s 10ms/step - loss: 0.3120 - acc: 0.8711 - val_loss: 0.3129 - val_acc: 0.8590\n",
      "Epoch 11/30\n",
      "9000/9000 [==============================] - 91s 10ms/step - loss: 0.3002 - acc: 0.8750 - val_loss: 0.3252 - val_acc: 0.8580\n",
      "Epoch 12/30\n",
      "9000/9000 [==============================] - 91s 10ms/step - loss: 0.2844 - acc: 0.8831 - val_loss: 0.3188 - val_acc: 0.8610\n",
      "Epoch 13/30\n",
      "9000/9000 [==============================] - 93s 10ms/step - loss: 0.2709 - acc: 0.8914 - val_loss: 0.3232 - val_acc: 0.8650\n",
      "Epoch 14/30\n",
      "9000/9000 [==============================] - 90s 10ms/step - loss: 0.2516 - acc: 0.8993 - val_loss: 0.3348 - val_acc: 0.8690\n",
      "Epoch 15/30\n",
      "9000/9000 [==============================] - 90s 10ms/step - loss: 0.2310 - acc: 0.9057 - val_loss: 0.3805 - val_acc: 0.8660\n",
      "Epoch 16/30\n",
      "9000/9000 [==============================] - 90s 10ms/step - loss: 0.2165 - acc: 0.9127 - val_loss: 0.3920 - val_acc: 0.8600\n",
      "Epoch 17/30\n",
      "9000/9000 [==============================] - 90s 10ms/step - loss: 0.1978 - acc: 0.9218 - val_loss: 0.3887 - val_acc: 0.8630\n",
      "Epoch 18/30\n",
      "9000/9000 [==============================] - 92s 10ms/step - loss: 0.1922 - acc: 0.9230 - val_loss: 0.4065 - val_acc: 0.8660\n",
      "Epoch 19/30\n",
      "9000/9000 [==============================] - 31556s 4s/step - loss: 0.1834 - acc: 0.9252 - val_loss: 0.3735 - val_acc: 0.8760\n",
      "Epoch 20/30\n",
      "9000/9000 [==============================] - 92s 10ms/step - loss: 0.1713 - acc: 0.9322 - val_loss: 0.3961 - val_acc: 0.8680\n",
      "Epoch 21/30\n",
      "9000/9000 [==============================] - 92s 10ms/step - loss: 0.1560 - acc: 0.9413 - val_loss: 0.3914 - val_acc: 0.8730\n",
      "Epoch 22/30\n",
      "9000/9000 [==============================] - 89s 10ms/step - loss: 0.1380 - acc: 0.9487 - val_loss: 0.4204 - val_acc: 0.8640\n",
      "Epoch 23/30\n",
      "9000/9000 [==============================] - 91s 10ms/step - loss: 0.1375 - acc: 0.9454 - val_loss: 0.4249 - val_acc: 0.8720\n",
      "Epoch 24/30\n",
      "9000/9000 [==============================] - 90s 10ms/step - loss: 0.1258 - acc: 0.9532 - val_loss: 0.4790 - val_acc: 0.8640\n",
      "Epoch 25/30\n",
      "9000/9000 [==============================] - 91s 10ms/step - loss: 0.1163 - acc: 0.9578 - val_loss: 0.5106 - val_acc: 0.8640\n",
      "Epoch 26/30\n",
      "9000/9000 [==============================] - 91s 10ms/step - loss: 0.1164 - acc: 0.9571 - val_loss: 0.5215 - val_acc: 0.8690\n",
      "Epoch 27/30\n",
      "9000/9000 [==============================] - 91s 10ms/step - loss: 0.1012 - acc: 0.9618 - val_loss: 0.4882 - val_acc: 0.8700\n",
      "Epoch 28/30\n",
      "9000/9000 [==============================] - 92s 10ms/step - loss: 0.0959 - acc: 0.9618 - val_loss: 0.5731 - val_acc: 0.8610\n",
      "Epoch 29/30\n",
      "9000/9000 [==============================] - 93s 10ms/step - loss: 0.0907 - acc: 0.9652 - val_loss: 0.4812 - val_acc: 0.8750\n",
      "Epoch 30/30\n",
      "9000/9000 [==============================] - 92s 10ms/step - loss: 0.0861 - acc: 0.9690 - val_loss: 0.5491 - val_acc: 0.8770\n",
      "1000/1000 [==============================] - 4s 4ms/step\n",
      "[0.5491372299194336, 0.877]\n",
      "1000/1000 [==============================] - 4s 4ms/step\n",
      "RNN-GRU, Word Embeddings [0.5491372299194336, 0.877]\n"
     ]
    }
   ],
   "source": [
    "#3.7.3 Recurrent Neural Network – GRU\n",
    "def create_rnn_gru():\n",
    "        # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.4)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(300)(embedding_layer)\n",
    "    lstm_layer = layers.Dropout(0.4)(lstm_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(100, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.4)(output_layer1)\n",
    "    output_layer11 = layers.Dense(10, activation=\"relu\")(output_layer1)\n",
    "    output_layer11 = layers.Dropout(0.4)(output_layer11)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer11)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy',metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_gru()\n",
    "accuracy = train_model_new(classifier, train_seq_x, train_y, valid_seq_x, 30,is_neural_net=True)\n",
    "print (\"RNN-GRU, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train on 9000 samples, validate on 1000 samples\n",
    "Epoch 1/5\n",
    "9000/9000 [==============================] - 33s 4ms/step - loss: 0.6062 - acc: 0.6626 - val_loss: 0.5270 - val_acc: 0.7250\n",
    "Epoch 2/5\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.4541 - acc: 0.7907 - val_loss: 0.3607 - val_acc: 0.8470\n",
    "Epoch 3/5\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.3892 - acc: 0.8287 - val_loss: 0.3357 - val_acc: 0.8550\n",
    "Epoch 4/5\n",
    "9000/9000 [==============================] - 32s 4ms/step - loss: 0.3634 - acc: 0.8430 - val_loss: 0.3167 - val_acc: 0.8550\n",
    "Epoch 5/5\n",
    "9000/9000 [==============================] - 31s 3ms/step - loss: 0.3453 - acc: 0.8480 - val_loss: 0.3117 - val_acc: 0.8630\n",
    "1000/1000 [==============================] - 1s 1ms/step\n",
    "[0.3116701068878174, 0.863]\n",
    "1000/1000 [==============================] - 2s 2ms/step\n",
    "RNN-GRU, Word Embeddings [0.3116701068878174, 0.863]\n",
    "\n",
    "\n",
    "Train on 9000 samples, validate on 1000 samples\n",
    "Epoch 1/10\n",
    "9000/9000 [==============================] - 103s 11ms/step - loss: 0.6402 - acc: 0.6390 - val_loss: 0.5746 - val_acc: 0.6940\n",
    "Epoch 2/10\n",
    "9000/9000 [==============================] - 97s 11ms/step - loss: 0.4652 - acc: 0.7909 - val_loss: 0.3335 - val_acc: 0.8560\n",
    "Epoch 3/10\n",
    "9000/9000 [==============================] - 99s 11ms/step - loss: 0.4017 - acc: 0.8303 - val_loss: 0.3395 - val_acc: 0.8530\n",
    "Epoch 4/10\n",
    "9000/9000 [==============================] - 94s 10ms/step - loss: 0.3710 - acc: 0.8412 - val_loss: 0.3257 - val_acc: 0.8610\n",
    "Epoch 5/10\n",
    "9000/9000 [==============================] - 92s 10ms/step - loss: 0.3634 - acc: 0.8513 - val_loss: 0.3402 - val_acc: 0.8500\n",
    "Epoch 6/10\n",
    "9000/9000 [==============================] - 95s 11ms/step - loss: 0.3361 - acc: 0.8610 - val_loss: 0.3340 - val_acc: 0.8500\n",
    "Epoch 7/10\n",
    "9000/9000 [==============================] - 96s 11ms/step - loss: 0.3266 - acc: 0.8630 - val_loss: 0.3249 - val_acc: 0.8520\n",
    "Epoch 8/10\n",
    "9000/9000 [==============================] - 102s 11ms/step - loss: 0.3007 - acc: 0.8737 - val_loss: 0.3025 - val_acc: 0.8730\n",
    "Epoch 9/10\n",
    "9000/9000 [==============================] - 102s 11ms/step - loss: 0.2868 - acc: 0.8821 - val_loss: 0.3273 - val_acc: 0.8610\n",
    "Epoch 10/10\n",
    "9000/9000 [==============================] - 102s 11ms/step - loss: 0.2631 - acc: 0.8944 - val_loss: 0.3416 - val_acc: 0.8400\n",
    "1000/1000 [==============================] - 4s 4ms/step\n",
    "[0.34156269550323487, 0.84]\n",
    "1000/1000 [==============================] - 4s 4ms/step\n",
    "RNN-GRU, Word Embeddings [0.34156269550323487, 0.84]\n",
    "\n",
    "Train on 9000 samples, validate on 1000 samples\n",
    "Epoch 1/30\n",
    "9000/9000 [==============================] - 93s 10ms/step - loss: 0.6576 - acc: 0.6053 - val_loss: 0.5076 - val_acc: 0.7520\n",
    "Epoch 2/30\n",
    "9000/9000 [==============================] - 90s 10ms/step - loss: 0.4966 - acc: 0.7743 - val_loss: 0.3860 - val_acc: 0.8390\n",
    "Epoch 3/30\n",
    "9000/9000 [==============================] - 90s 10ms/step - loss: 0.4230 - acc: 0.8187 - val_loss: 0.3962 - val_acc: 0.8030\n",
    "Epoch 4/30\n",
    "9000/9000 [==============================] - 91s 10ms/step - loss: 0.4052 - acc: 0.8273 - val_loss: 0.3307 - val_acc: 0.8590\n",
    "Epoch 5/30\n",
    "9000/9000 [==============================] - 91s 10ms/step - loss: 0.3826 - acc: 0.8329 - val_loss: 0.3193 - val_acc: 0.8660\n",
    "Epoch 6/30\n",
    "9000/9000 [==============================] - 91s 10ms/step - loss: 0.3723 - acc: 0.8399 - val_loss: 0.3342 - val_acc: 0.8700\n",
    "Epoch 7/30\n",
    "9000/9000 [==============================] - 91s 10ms/step - loss: 0.3559 - acc: 0.8503 - val_loss: 0.3354 - val_acc: 0.8530\n",
    "Epoch 8/30\n",
    "9000/9000 [==============================] - 91s 10ms/step - loss: 0.3387 - acc: 0.8571 - val_loss: 0.3361 - val_acc: 0.8530\n",
    "Epoch 9/30\n",
    "9000/9000 [==============================] - 91s 10ms/step - loss: 0.3306 - acc: 0.8640 - val_loss: 0.3082 - val_acc: 0.8610\n",
    "Epoch 10/30\n",
    "9000/9000 [==============================] - 91s 10ms/step - loss: 0.3120 - acc: 0.8711 - val_loss: 0.3129 - val_acc: 0.8590\n",
    "Epoch 11/30\n",
    "9000/9000 [==============================] - 91s 10ms/step - loss: 0.3002 - acc: 0.8750 - val_loss: 0.3252 - val_acc: 0.8580\n",
    "Epoch 12/30\n",
    "9000/9000 [==============================] - 91s 10ms/step - loss: 0.2844 - acc: 0.8831 - val_loss: 0.3188 - val_acc: 0.8610\n",
    "Epoch 13/30\n",
    "9000/9000 [==============================] - 93s 10ms/step - loss: 0.2709 - acc: 0.8914 - val_loss: 0.3232 - val_acc: 0.8650\n",
    "Epoch 14/30\n",
    "9000/9000 [==============================] - 90s 10ms/step - loss: 0.2516 - acc: 0.8993 - val_loss: 0.3348 - val_acc: 0.8690\n",
    "Epoch 15/30\n",
    "9000/9000 [==============================] - 90s 10ms/step - loss: 0.2310 - acc: 0.9057 - val_loss: 0.3805 - val_acc: 0.8660\n",
    "Epoch 16/30\n",
    "9000/9000 [==============================] - 90s 10ms/step - loss: 0.2165 - acc: 0.9127 - val_loss: 0.3920 - val_acc: 0.8600\n",
    "Epoch 17/30\n",
    "9000/9000 [==============================] - 90s 10ms/step - loss: 0.1978 - acc: 0.9218 - val_loss: 0.3887 - val_acc: 0.8630\n",
    "Epoch 18/30\n",
    "9000/9000 [==============================] - 92s 10ms/step - loss: 0.1922 - acc: 0.9230 - val_loss: 0.4065 - val_acc: 0.8660\n",
    "Epoch 19/30\n",
    "9000/9000 [==============================] - 31556s 4s/step - loss: 0.1834 - acc: 0.9252 - val_loss: 0.3735 - val_acc: 0.8760\n",
    "Epoch 20/30\n",
    "9000/9000 [==============================] - 92s 10ms/step - loss: 0.1713 - acc: 0.9322 - val_loss: 0.3961 - val_acc: 0.8680\n",
    "Epoch 21/30\n",
    "9000/9000 [==============================] - 92s 10ms/step - loss: 0.1560 - acc: 0.9413 - val_loss: 0.3914 - val_acc: 0.8730\n",
    "Epoch 22/30\n",
    "9000/9000 [==============================] - 89s 10ms/step - loss: 0.1380 - acc: 0.9487 - val_loss: 0.4204 - val_acc: 0.8640\n",
    "Epoch 23/30\n",
    "9000/9000 [==============================] - 91s 10ms/step - loss: 0.1375 - acc: 0.9454 - val_loss: 0.4249 - val_acc: 0.8720\n",
    "Epoch 24/30\n",
    "9000/9000 [==============================] - 90s 10ms/step - loss: 0.1258 - acc: 0.9532 - val_loss: 0.4790 - val_acc: 0.8640\n",
    "Epoch 25/30\n",
    "9000/9000 [==============================] - 91s 10ms/step - loss: 0.1163 - acc: 0.9578 - val_loss: 0.5106 - val_acc: 0.8640\n",
    "Epoch 26/30\n",
    "9000/9000 [==============================] - 91s 10ms/step - loss: 0.1164 - acc: 0.9571 - val_loss: 0.5215 - val_acc: 0.8690\n",
    "Epoch 27/30\n",
    "9000/9000 [==============================] - 91s 10ms/step - loss: 0.1012 - acc: 0.9618 - val_loss: 0.4882 - val_acc: 0.8700\n",
    "Epoch 28/30\n",
    "9000/9000 [==============================] - 92s 10ms/step - loss: 0.0959 - acc: 0.9618 - val_loss: 0.5731 - val_acc: 0.8610\n",
    "Epoch 29/30\n",
    "9000/9000 [==============================] - 93s 10ms/step - loss: 0.0907 - acc: 0.9652 - val_loss: 0.4812 - val_acc: 0.8750\n",
    "Epoch 30/30\n",
    "9000/9000 [==============================] - 92s 10ms/step - loss: 0.0861 - acc: 0.9690 - val_loss: 0.5491 - val_acc: 0.8770\n",
    "1000/1000 [==============================] - 4s 4ms/step\n",
    "[0.5491372299194336, 0.877]\n",
    "1000/1000 [==============================] - 4s 4ms/step\n",
    "RNN-GRU, Word Embeddings [0.5491372299194336, 0.877]\n",
    "\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_12 (InputLayer)        (None, 70)                0         \n",
    "_________________________________________________________________\n",
    "embedding_12 (Embedding)     (None, 70, 300)           10041000  \n",
    "_________________________________________________________________\n",
    "spatial_dropout1d_13 (Spatia (None, 70, 300)           0         \n",
    "_________________________________________________________________\n",
    "gru_4 (GRU)                  (None, 300)               540900    \n",
    "_________________________________________________________________\n",
    "dropout_21 (Dropout)         (None, 300)               0         \n",
    "_________________________________________________________________\n",
    "dense_24 (Dense)             (None, 100)               30100     \n",
    "_________________________________________________________________\n",
    "dropout_22 (Dropout)         (None, 100)               0         \n",
    "_________________________________________________________________\n",
    "dense_25 (Dense)             (None, 10)                1010      \n",
    "_________________________________________________________________\n",
    "dropout_23 (Dropout)         (None, 10)                0         \n",
    "_________________________________________________________________\n",
    "dense_26 (Dense)             (None, 1)                 11        \n",
    "=================================================================\n",
    "Total params: 10,613,021\n",
    "Trainable params: 572,021\n",
    "Non-trainable params: 10,041,000\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 70, 300)           10041000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_13 (Spatia (None, 70, 300)           0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 300)               540900    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 10,613,021\n",
      "Trainable params: 572,021\n",
      "Non-trainable params: 10,041,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "9000/9000 [==============================] - 167s 19ms/step - loss: 0.6843 - acc: 0.5420 - val_loss: 0.6654 - val_acc: 0.6170\n",
      "Epoch 2/30\n",
      "9000/9000 [==============================] - 166s 18ms/step - loss: 0.6287 - acc: 0.6533 - val_loss: 0.4747 - val_acc: 0.7770\n",
      "Epoch 3/30\n",
      "9000/9000 [==============================] - 166s 18ms/step - loss: 0.4994 - acc: 0.7771 - val_loss: 0.3732 - val_acc: 0.8500\n",
      "Epoch 4/30\n",
      "9000/9000 [==============================] - 192s 21ms/step - loss: 0.4586 - acc: 0.8027 - val_loss: 0.3502 - val_acc: 0.8590\n",
      "Epoch 5/30\n",
      "9000/9000 [==============================] - 188s 21ms/step - loss: 0.4276 - acc: 0.8099 - val_loss: 0.3475 - val_acc: 0.8470\n",
      "Epoch 6/30\n",
      "9000/9000 [==============================] - 168s 19ms/step - loss: 0.4209 - acc: 0.8263 - val_loss: 0.3386 - val_acc: 0.8500\n",
      "Epoch 7/30\n",
      "9000/9000 [==============================] - 168s 19ms/step - loss: 0.3984 - acc: 0.8281 - val_loss: 0.3681 - val_acc: 0.8540\n",
      "Epoch 8/30\n",
      "9000/9000 [==============================] - 168s 19ms/step - loss: 0.3941 - acc: 0.8393 - val_loss: 0.3338 - val_acc: 0.8580\n",
      "Epoch 9/30\n",
      "9000/9000 [==============================] - 169s 19ms/step - loss: 0.3823 - acc: 0.8373 - val_loss: 0.3507 - val_acc: 0.8490\n",
      "Epoch 10/30\n",
      "9000/9000 [==============================] - 169s 19ms/step - loss: 0.3808 - acc: 0.8453 - val_loss: 0.3476 - val_acc: 0.8660\n",
      "Epoch 11/30\n",
      "9000/9000 [==============================] - 166s 18ms/step - loss: 0.3512 - acc: 0.8496 - val_loss: 0.3471 - val_acc: 0.8530\n",
      "Epoch 12/30\n",
      "9000/9000 [==============================] - 166s 18ms/step - loss: 0.3567 - acc: 0.8491 - val_loss: 0.3370 - val_acc: 0.8600\n",
      "Epoch 13/30\n",
      "9000/9000 [==============================] - 166s 18ms/step - loss: 0.3452 - acc: 0.8583 - val_loss: 0.3311 - val_acc: 0.8640\n",
      "Epoch 14/30\n",
      "9000/9000 [==============================] - 174s 19ms/step - loss: 0.3331 - acc: 0.8640 - val_loss: 0.3427 - val_acc: 0.8590\n",
      "Epoch 15/30\n",
      "9000/9000 [==============================] - 170s 19ms/step - loss: 0.3266 - acc: 0.8644 - val_loss: 0.3423 - val_acc: 0.8690\n",
      "Epoch 16/30\n",
      "9000/9000 [==============================] - 170s 19ms/step - loss: 0.3129 - acc: 0.8724 - val_loss: 0.3366 - val_acc: 0.8610\n",
      "Epoch 17/30\n",
      "9000/9000 [==============================] - 170s 19ms/step - loss: 0.2921 - acc: 0.8827 - val_loss: 0.3758 - val_acc: 0.8550\n",
      "Epoch 18/30\n",
      "9000/9000 [==============================] - 170s 19ms/step - loss: 0.2869 - acc: 0.8819 - val_loss: 0.3384 - val_acc: 0.8510\n",
      "Epoch 19/30\n",
      "9000/9000 [==============================] - 172s 19ms/step - loss: 0.2849 - acc: 0.8870 - val_loss: 0.3945 - val_acc: 0.8650\n",
      "Epoch 20/30\n",
      "9000/9000 [==============================] - 168s 19ms/step - loss: 0.2669 - acc: 0.8929 - val_loss: 0.3644 - val_acc: 0.8640\n",
      "Epoch 21/30\n",
      "9000/9000 [==============================] - 168s 19ms/step - loss: 0.2543 - acc: 0.8971 - val_loss: 0.3657 - val_acc: 0.8630\n",
      "Epoch 22/30\n",
      "9000/9000 [==============================] - 168s 19ms/step - loss: 0.2411 - acc: 0.9079 - val_loss: 0.3570 - val_acc: 0.8680\n",
      "Epoch 23/30\n",
      "9000/9000 [==============================] - 167s 19ms/step - loss: 0.2311 - acc: 0.9074 - val_loss: 0.3862 - val_acc: 0.8500\n",
      "Epoch 24/30\n",
      "9000/9000 [==============================] - 167s 19ms/step - loss: 0.2369 - acc: 0.9071 - val_loss: 0.3668 - val_acc: 0.8630\n",
      "Epoch 25/30\n",
      "9000/9000 [==============================] - 167s 19ms/step - loss: 0.2137 - acc: 0.9198 - val_loss: 0.4610 - val_acc: 0.8540\n",
      "Epoch 26/30\n",
      "9000/9000 [==============================] - 167s 19ms/step - loss: 0.2059 - acc: 0.9180 - val_loss: 0.4328 - val_acc: 0.8610\n",
      "Epoch 27/30\n",
      "9000/9000 [==============================] - 167s 19ms/step - loss: 0.2063 - acc: 0.9172 - val_loss: 0.3994 - val_acc: 0.8530\n",
      "Epoch 28/30\n",
      "9000/9000 [==============================] - 167s 19ms/step - loss: 0.2070 - acc: 0.9182 - val_loss: 0.4578 - val_acc: 0.8680\n",
      "Epoch 29/30\n",
      "9000/9000 [==============================] - 170s 19ms/step - loss: 0.1818 - acc: 0.9319 - val_loss: 0.4875 - val_acc: 0.8500\n",
      "Epoch 30/30\n",
      "9000/9000 [==============================] - 170s 19ms/step - loss: 0.1826 - acc: 0.9301 - val_loss: 0.4339 - val_acc: 0.8500\n",
      "1000/1000 [==============================] - 7s 7ms/step\n",
      "[0.4338808161020279, 0.85]\n",
      "1000/1000 [==============================] - 7s 7ms/step\n",
      "RNN-Bidirectional, Word Embeddings [0.4338808161020279, 0.85]\n"
     ]
    }
   ],
   "source": [
    "#3.7.4 Bidirectional RNN\n",
    "def create_bidirectional_rnn():\n",
    "        # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.5)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(300))(embedding_layer)\n",
    "    lstm_layer = layers.Dropout(0.5)(lstm_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(100, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.5)(output_layer1)\n",
    "    output_layer11 = layers.Dense(10, activation=\"relu\")(output_layer1)\n",
    "    output_layer11 = layers.Dropout(0.5)(output_layer11)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer11)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy',metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_bidirectional_rnn()\n",
    "accuracy = train_model_new(classifier, train_seq_x, train_y, valid_seq_x, 30, is_neural_net=True)\n",
    "print (\"RNN-Bidirectional, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train on 9000 samples, validate on 1000 samples\n",
    "Epoch 1/30\n",
    "9000/9000 [==============================] - 170s 19ms/step - loss: 0.6566 - acc: 0.6051 - val_loss: 0.4907 - val_acc: 0.7900\n",
    "Epoch 2/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.4919 - acc: 0.7726 - val_loss: 0.3375 - val_acc: 0.8590\n",
    "Epoch 3/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.3972 - acc: 0.8248 - val_loss: 0.3279 - val_acc: 0.8530\n",
    "Epoch 4/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.3786 - acc: 0.8420 - val_loss: 0.3419 - val_acc: 0.8580\n",
    "Epoch 5/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.3536 - acc: 0.8511 - val_loss: 0.3066 - val_acc: 0.8700\n",
    "Epoch 6/30\n",
    "9000/9000 [==============================] - 165s 18ms/step - loss: 0.3300 - acc: 0.8623 - val_loss: 0.3159 - val_acc: 0.8690\n",
    "Epoch 7/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.3121 - acc: 0.8710 - val_loss: 0.3129 - val_acc: 0.8680\n",
    "Epoch 8/30\n",
    "9000/9000 [==============================] - 167s 19ms/step - loss: 0.2770 - acc: 0.8861 - val_loss: 0.3069 - val_acc: 0.8630\n",
    "Epoch 9/30\n",
    "9000/9000 [==============================] - 182s 20ms/step - loss: 0.2652 - acc: 0.8920 - val_loss: 0.3224 - val_acc: 0.8730\n",
    "Epoch 10/30\n",
    "9000/9000 [==============================] - 171s 19ms/step - loss: 0.2354 - acc: 0.9099 - val_loss: 0.3441 - val_acc: 0.8720\n",
    "Epoch 11/30\n",
    "9000/9000 [==============================] - 169s 19ms/step - loss: 0.2161 - acc: 0.9143 - val_loss: 0.3517 - val_acc: 0.8550\n",
    "Epoch 12/30\n",
    "9000/9000 [==============================] - 170s 19ms/step - loss: 0.1884 - acc: 0.9272 - val_loss: 0.3586 - val_acc: 0.8740\n",
    "Epoch 13/30\n",
    "9000/9000 [==============================] - 172s 19ms/step - loss: 0.1819 - acc: 0.9309 - val_loss: 0.3743 - val_acc: 0.8730\n",
    "Epoch 14/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.1529 - acc: 0.9437 - val_loss: 0.4130 - val_acc: 0.8610\n",
    "Epoch 15/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.1384 - acc: 0.9473 - val_loss: 0.3638 - val_acc: 0.8760\n",
    "Epoch 16/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.1238 - acc: 0.9557 - val_loss: 0.4668 - val_acc: 0.8570\n",
    "Epoch 17/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.1054 - acc: 0.9626 - val_loss: 0.4543 - val_acc: 0.8620\n",
    "Epoch 18/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.1016 - acc: 0.9640 - val_loss: 0.4777 - val_acc: 0.8620\n",
    "Epoch 19/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.0858 - acc: 0.9689 - val_loss: 0.5529 - val_acc: 0.8680\n",
    "Epoch 20/30\n",
    "9000/9000 [==============================] - 165s 18ms/step - loss: 0.0776 - acc: 0.9749 - val_loss: 0.5531 - val_acc: 0.8670\n",
    "Epoch 21/30\n",
    "9000/9000 [==============================] - 165s 18ms/step - loss: 0.0709 - acc: 0.9760 - val_loss: 0.5856 - val_acc: 0.8690\n",
    "Epoch 22/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.0729 - acc: 0.9766 - val_loss: 0.5536 - val_acc: 0.8570\n",
    "Epoch 23/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.0672 - acc: 0.9782 - val_loss: 0.6229 - val_acc: 0.8730\n",
    "Epoch 24/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.0705 - acc: 0.9773 - val_loss: 0.5098 - val_acc: 0.8600\n",
    "Epoch 25/30\n",
    "9000/9000 [==============================] - 166s 18ms/step - loss: 0.0589 - acc: 0.9821 - val_loss: 0.6825 - val_acc: 0.8570\n",
    "Epoch 26/30\n",
    "9000/9000 [==============================] - 168s 19ms/step - loss: 0.0481 - acc: 0.9854 - val_loss: 0.5656 - val_acc: 0.8550\n",
    "Epoch 27/30\n",
    "9000/9000 [==============================] - 183s 20ms/step - loss: 0.0487 - acc: 0.9829 - val_loss: 0.6314 - val_acc: 0.8600\n",
    "Epoch 28/30\n",
    "9000/9000 [==============================] - 162s 18ms/step - loss: 0.0440 - acc: 0.9850 - val_loss: 0.7353 - val_acc: 0.8430\n",
    "Epoch 29/30\n",
    "9000/9000 [==============================] - 162s 18ms/step - loss: 0.0403 - acc: 0.9880 - val_loss: 0.6665 - val_acc: 0.8580\n",
    "Epoch 30/30\n",
    "9000/9000 [==============================] - 162s 18ms/step - loss: 0.0474 - acc: 0.9859 - val_loss: 0.6731 - val_acc: 0.8520\n",
    "1000/1000 [==============================] - 6s 6ms/step\n",
    "[0.6730928390026093, 0.852]\n",
    "1000/1000 [==============================] - 6s 6ms/step\n",
    "RNN-Bidirectional, Word Embeddings [0.6730928390026093, 0.852]\n",
    "\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_13 (InputLayer)        (None, 70)                0         \n",
    "_________________________________________________________________\n",
    "embedding_13 (Embedding)     (None, 70, 300)           10041000  \n",
    "_________________________________________________________________\n",
    "spatial_dropout1d_14 (Spatia (None, 70, 300)           0         \n",
    "_________________________________________________________________\n",
    "bidirectional_1 (Bidirection (None, 600)               1081800   \n",
    "_________________________________________________________________\n",
    "dropout_24 (Dropout)         (None, 600)               0         \n",
    "_________________________________________________________________\n",
    "dense_27 (Dense)             (None, 100)               60100     \n",
    "_________________________________________________________________\n",
    "dropout_25 (Dropout)         (None, 100)               0         \n",
    "_________________________________________________________________\n",
    "dense_28 (Dense)             (None, 10)                1010      \n",
    "_________________________________________________________________\n",
    "dropout_26 (Dropout)         (None, 10)                0         \n",
    "_________________________________________________________________\n",
    "dense_29 (Dense)             (None, 1)                 11        \n",
    "=================================================================\n",
    "Total params: 11,183,921\n",
    "Trainable params: 1,142,921\n",
    "Non-trainable params: 10,041,000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, 70, 300)           10041000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_14 (Spatia (None, 70, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 600)               1081800   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 100)               60100     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 11,183,921\n",
      "Trainable params: 1,142,921\n",
      "Non-trainable params: 10,041,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "9000/9000 [==============================] - 301s 33ms/step - loss: 0.6617 - acc: 0.5894 - val_loss: 0.4991 - val_acc: 0.7950\n",
      "Epoch 2/30\n",
      "9000/9000 [==============================] - 301s 33ms/step - loss: 0.4963 - acc: 0.7699 - val_loss: 0.3470 - val_acc: 0.8660\n",
      "Epoch 3/30\n",
      "9000/9000 [==============================] - 298s 33ms/step - loss: 0.4266 - acc: 0.8123 - val_loss: 0.3758 - val_acc: 0.8340\n",
      "Epoch 4/30\n",
      "9000/9000 [==============================] - 296s 33ms/step - loss: 0.3926 - acc: 0.8306 - val_loss: 0.3366 - val_acc: 0.8560\n",
      "Epoch 5/30\n",
      "9000/9000 [==============================] - 296s 33ms/step - loss: 0.3621 - acc: 0.8486 - val_loss: 0.3205 - val_acc: 0.8800\n",
      "Epoch 6/30\n",
      "9000/9000 [==============================] - 297s 33ms/step - loss: 0.3321 - acc: 0.8627 - val_loss: 0.3245 - val_acc: 0.8630\n",
      "Epoch 7/30\n",
      "9000/9000 [==============================] - 297s 33ms/step - loss: 0.3111 - acc: 0.8699 - val_loss: 0.3409 - val_acc: 0.8720\n",
      "Epoch 8/30\n",
      "9000/9000 [==============================] - 298s 33ms/step - loss: 0.2868 - acc: 0.8828 - val_loss: 0.3686 - val_acc: 0.8660\n",
      "Epoch 9/30\n",
      "9000/9000 [==============================] - 292s 32ms/step - loss: 0.2631 - acc: 0.8924 - val_loss: 0.3559 - val_acc: 0.8710\n",
      "Epoch 10/30\n",
      "9000/9000 [==============================] - 298s 33ms/step - loss: 0.2348 - acc: 0.9116 - val_loss: 0.3548 - val_acc: 0.8730\n",
      "Epoch 11/30\n",
      "9000/9000 [==============================] - 298s 33ms/step - loss: 0.2034 - acc: 0.9192 - val_loss: 0.3517 - val_acc: 0.8730\n",
      "Epoch 12/30\n",
      "9000/9000 [==============================] - 15904s 2s/step - loss: 0.2004 - acc: 0.9237 - val_loss: 0.3833 - val_acc: 0.8730\n",
      "Epoch 13/30\n",
      "9000/9000 [==============================] - 314s 35ms/step - loss: 0.1821 - acc: 0.9296 - val_loss: 0.4465 - val_acc: 0.8180\n",
      "Epoch 14/30\n",
      "9000/9000 [==============================] - 299s 33ms/step - loss: 0.1888 - acc: 0.9301 - val_loss: 0.4059 - val_acc: 0.8780\n",
      "Epoch 15/30\n",
      "9000/9000 [==============================] - 302s 34ms/step - loss: 0.1418 - acc: 0.9496 - val_loss: 0.4443 - val_acc: 0.8660\n",
      "Epoch 16/30\n",
      "9000/9000 [==============================] - 299s 33ms/step - loss: 0.1318 - acc: 0.9506 - val_loss: 0.4380 - val_acc: 0.8740\n",
      "Epoch 17/30\n",
      "9000/9000 [==============================] - 297s 33ms/step - loss: 0.1261 - acc: 0.9549 - val_loss: 0.4736 - val_acc: 0.8710\n",
      "Epoch 18/30\n",
      "9000/9000 [==============================] - 293s 33ms/step - loss: 0.1161 - acc: 0.9578 - val_loss: 0.4845 - val_acc: 0.8760\n",
      "Epoch 19/30\n",
      "9000/9000 [==============================] - 294s 33ms/step - loss: 0.1057 - acc: 0.9632 - val_loss: 0.5524 - val_acc: 0.8670\n",
      "Epoch 20/30\n",
      "9000/9000 [==============================] - 293s 33ms/step - loss: 0.1034 - acc: 0.9648 - val_loss: 0.4828 - val_acc: 0.8700\n",
      "Epoch 21/30\n",
      "9000/9000 [==============================] - 293s 33ms/step - loss: 0.0889 - acc: 0.9687 - val_loss: 0.5230 - val_acc: 0.8620\n",
      "Epoch 22/30\n",
      "9000/9000 [==============================] - 293s 33ms/step - loss: 0.0927 - acc: 0.9647 - val_loss: 0.5651 - val_acc: 0.8670\n",
      "Epoch 23/30\n",
      "9000/9000 [==============================] - 293s 33ms/step - loss: 0.0860 - acc: 0.9708 - val_loss: 0.6447 - val_acc: 0.8710\n",
      "Epoch 24/30\n",
      "9000/9000 [==============================] - 293s 33ms/step - loss: 0.0812 - acc: 0.9719 - val_loss: 0.5546 - val_acc: 0.8720\n",
      "Epoch 25/30\n",
      "9000/9000 [==============================] - 296s 33ms/step - loss: 0.0783 - acc: 0.9718 - val_loss: 0.5594 - val_acc: 0.8650\n",
      "Epoch 26/30\n",
      "9000/9000 [==============================] - 307s 34ms/step - loss: 0.0737 - acc: 0.9769 - val_loss: 0.6048 - val_acc: 0.8640\n",
      "Epoch 27/30\n",
      "9000/9000 [==============================] - 293s 33ms/step - loss: 0.0776 - acc: 0.9760 - val_loss: 0.5742 - val_acc: 0.8700\n",
      "Epoch 28/30\n",
      "9000/9000 [==============================] - 292s 32ms/step - loss: 0.0569 - acc: 0.9807 - val_loss: 0.5849 - val_acc: 0.8690\n",
      "Epoch 29/30\n",
      "9000/9000 [==============================] - 289s 32ms/step - loss: 0.0782 - acc: 0.9737 - val_loss: 0.5910 - val_acc: 0.8670\n",
      "Epoch 30/30\n",
      "9000/9000 [==============================] - 289s 32ms/step - loss: 0.0748 - acc: 0.9746 - val_loss: 0.6509 - val_acc: 0.8730\n",
      "1000/1000 [==============================] - 9s 9ms/step\n",
      "[0.6509137352230028, 0.873]\n",
      "1000/1000 [==============================] - 9s 9ms/step\n",
      "CNN, Word Embeddings [0.6509137352230028, 0.873]\n"
     ]
    }
   ],
   "source": [
    "#3.7.5 Recurrent Convolutional Neural Network\n",
    "def create_rcnn():\n",
    "        # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.5)(embedding_layer)\n",
    "       \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(500, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    #pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.GRU(300))(conv_layer)\n",
    "    rnn_layer = layers.Dropout(0.5)(rnn_layer)\n",
    "    \n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(20, activation=\"relu\")(rnn_layer)\n",
    "    output_layer1 = layers.Dropout(0.5)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy',metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rcnn()\n",
    "accuracy = train_model_new(classifier, train_seq_x, train_y, valid_seq_x,30, is_neural_net=True)\n",
    "print (\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train on 9000 samples, validate on 1000 samples\n",
    "Epoch 1/30\n",
    "9000/9000 [==============================] - 508s 56ms/step - loss: 0.6698 - acc: 0.5760 - val_loss: 0.5965 - val_acc: 0.7030\n",
    "Epoch 2/30\n",
    "9000/9000 [==============================] - 501s 56ms/step - loss: 0.4760 - acc: 0.7843 - val_loss: 0.3648 - val_acc: 0.8420\n",
    "Epoch 3/30\n",
    "9000/9000 [==============================] - 500s 56ms/step - loss: 0.3904 - acc: 0.8278 - val_loss: 0.3330 - val_acc: 0.8490\n",
    "Epoch 4/30\n",
    "9000/9000 [==============================] - 499s 55ms/step - loss: 0.3552 - acc: 0.8468 - val_loss: 0.3234 - val_acc: 0.8660\n",
    "Epoch 5/30\n",
    "9000/9000 [==============================] - 500s 56ms/step - loss: 0.3202 - acc: 0.8631 - val_loss: 0.3213 - val_acc: 0.8630\n",
    "Epoch 6/30\n",
    "9000/9000 [==============================] - 501s 56ms/step - loss: 0.2684 - acc: 0.8912 - val_loss: 0.3257 - val_acc: 0.8700\n",
    "Epoch 7/30\n",
    "9000/9000 [==============================] - 501s 56ms/step - loss: 0.2255 - acc: 0.9134 - val_loss: 0.3211 - val_acc: 0.8700\n",
    "Epoch 8/30\n",
    "9000/9000 [==============================] - 501s 56ms/step - loss: 0.1791 - acc: 0.9317 - val_loss: 0.3429 - val_acc: 0.8670\n",
    "Epoch 9/30\n",
    "9000/9000 [==============================] - 501s 56ms/step - loss: 0.1513 - acc: 0.9439 - val_loss: 0.3996 - val_acc: 0.8600\n",
    "Epoch 10/30\n",
    "9000/9000 [==============================] - 501s 56ms/step - loss: 0.1298 - acc: 0.9538 - val_loss: 0.3953 - val_acc: 0.8660\n",
    "Epoch 11/30\n",
    "9000/9000 [==============================] - 521s 58ms/step - loss: 0.1024 - acc: 0.9608 - val_loss: 0.4531 - val_acc: 0.8720\n",
    "Epoch 12/30\n",
    "9000/9000 [==============================] - 516s 57ms/step - loss: 0.0894 - acc: 0.9660 - val_loss: 0.4921 - val_acc: 0.8700\n",
    "Epoch 13/30\n",
    "9000/9000 [==============================] - 508s 56ms/step - loss: 0.0799 - acc: 0.9708 - val_loss: 0.5470 - val_acc: 0.8540\n",
    "Epoch 14/30\n",
    "9000/9000 [==============================] - 507s 56ms/step - loss: 0.0759 - acc: 0.9737 - val_loss: 0.4952 - val_acc: 0.8800\n",
    "Epoch 15/30\n",
    "9000/9000 [==============================] - 506s 56ms/step - loss: 0.0622 - acc: 0.9796 - val_loss: 0.6530 - val_acc: 0.8510\n",
    "Epoch 16/30\n",
    "9000/9000 [==============================] - 507s 56ms/step - loss: 0.0618 - acc: 0.9801 - val_loss: 0.5974 - val_acc: 0.8580\n",
    "Epoch 17/30\n",
    "9000/9000 [==============================] - 534s 59ms/step - loss: 0.0593 - acc: 0.9786 - val_loss: 0.5809 - val_acc: 0.8770\n",
    "Epoch 18/30\n",
    "9000/9000 [==============================] - 548s 61ms/step - loss: 0.0467 - acc: 0.9843 - val_loss: 0.5605 - val_acc: 0.8740\n",
    "Epoch 19/30\n",
    "9000/9000 [==============================] - 514s 57ms/step - loss: 0.0431 - acc: 0.9852 - val_loss: 0.6432 - val_acc: 0.8720\n",
    "Epoch 20/30\n",
    "9000/9000 [==============================] - 504s 56ms/step - loss: 0.0493 - acc: 0.9833 - val_loss: 0.6313 - val_acc: 0.8680\n",
    "Epoch 21/30\n",
    "9000/9000 [==============================] - 501s 56ms/step - loss: 0.0368 - acc: 0.9867 - val_loss: 0.6529 - val_acc: 0.8680\n",
    "Epoch 22/30\n",
    "9000/9000 [==============================] - 508s 56ms/step - loss: 0.0408 - acc: 0.9872 - val_loss: 0.7504 - val_acc: 0.8790\n",
    "Epoch 23/30\n",
    "9000/9000 [==============================] - 502s 56ms/step - loss: 0.0394 - acc: 0.9853 - val_loss: 0.6455 - val_acc: 0.8670\n",
    "Epoch 24/30\n",
    "9000/9000 [==============================] - 501s 56ms/step - loss: 0.0458 - acc: 0.9838 - val_loss: 0.6057 - val_acc: 0.8740\n",
    "Epoch 25/30\n",
    "9000/9000 [==============================] - 514s 57ms/step - loss: 0.0376 - acc: 0.9877 - val_loss: 0.7650 - val_acc: 0.8580\n",
    "Epoch 26/30\n",
    "9000/9000 [==============================] - 525s 58ms/step - loss: 0.0341 - acc: 0.9889 - val_loss: 0.7672 - val_acc: 0.8600\n",
    "Epoch 27/30\n",
    "9000/9000 [==============================] - 514s 57ms/step - loss: 0.0314 - acc: 0.9892 - val_loss: 0.7108 - val_acc: 0.8590\n",
    "Epoch 28/30\n",
    "9000/9000 [==============================] - 517s 57ms/step - loss: 0.0376 - acc: 0.9884 - val_loss: 0.6822 - val_acc: 0.8700\n",
    "Epoch 29/30\n",
    "9000/9000 [==============================] - 516s 57ms/step - loss: 0.0326 - acc: 0.9890 - val_loss: 0.6904 - val_acc: 0.8770\n",
    "Epoch 30/30\n",
    "9000/9000 [==============================] - 516s 57ms/step - loss: 0.0312 - acc: 0.9893 - val_loss: 0.7660 - val_acc: 0.8640\n",
    "1000/1000 [==============================] - 17s 17ms/step\n",
    "[0.7660071992874146, 0.864]\n",
    "1000/1000 [==============================] - 17s 17ms/step\n",
    "CNN, Word Embeddings [0.7660071992874146, 0.864]\n",
    "\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_18 (InputLayer)        (None, 70)                0         \n",
    "_________________________________________________________________\n",
    "embedding_18 (Embedding)     (None, 70, 300)           10041000  \n",
    "_________________________________________________________________\n",
    "spatial_dropout1d_19 (Spatia (None, 70, 300)           0         \n",
    "_________________________________________________________________\n",
    "conv1d_4 (Conv1D)            (None, 68, 1000)          901000    \n",
    "_________________________________________________________________\n",
    "bidirectional_6 (Bidirection (None, 600)               2341800   \n",
    "_________________________________________________________________\n",
    "dropout_36 (Dropout)         (None, 600)               0         \n",
    "_________________________________________________________________\n",
    "dense_39 (Dense)             (None, 20)                12020     \n",
    "_________________________________________________________________\n",
    "dropout_37 (Dropout)         (None, 20)                0         \n",
    "_________________________________________________________________\n",
    "dense_40 (Dense)             (None, 1)                 21        \n",
    "=================================================================\n",
    "Total params: 13,295,841\n",
    "Trainable params: 3,254,841\n",
    "Non-trainable params: 10,041,000\n",
    "_________________________________________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "embedding_18 (Embedding)     (None, 70, 300)           10041000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_19 (Spatia (None, 70, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 68, 1000)          901000    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 600)               2341800   \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 20)                12020     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 13,295,841\n",
      "Trainable params: 3,254,841\n",
      "Non-trainable params: 10,041,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. Dataset preparation3. Model Building\n",
    "# load the dataset\n",
    "data = open('data/corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label\n",
       "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
       "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
       "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
       "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.1 Count Vectors as features\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.2 TF-IDF Vectors as features\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.3 Word Embeddings\n",
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.0897      0.016      -0.0571     ...  0.1559     -0.0254\n",
      "  -0.0259    ]\n",
      " [-0.0314      0.0149     -0.0205     ...  0.098       0.0893\n",
      "   0.0148    ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.0897     -0.0682      0.0726     ...  0.206       0.0839\n",
      "   0.1557    ]\n",
      " [-0.0857      0.0475      0.14489999 ...  0.0586      0.11\n",
      "   0.0743    ]]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.4 Text / NLP based features\n",
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.5 Topic Models as features\n",
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. Model Building\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.8356\n",
      "NB, WordLevel TF-IDF:  0.8472\n",
      "NB, N-Gram Vectors:  0.8328\n",
      "NB, CharLevel Vectors:  0.814\n"
     ]
    }
   ],
   "source": [
    "#3.1 Naive Bayes\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn.naive_bayes' has no attribute 'ComplementNB'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-92d7e643becc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive_bayes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mComplementNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtrain_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxvalid_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"CB, Count Vectors: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sklearn.naive_bayes' has no attribute 'ComplementNB'"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.ComplementNB(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"CB, Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-166b17a58bdd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-166b17a58bdd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    NB, Count Vectors:  0.8488\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "NB, Count Vectors:  0.8488\n",
    "NB, WordLevel TF-IDF:  0.8524\n",
    "NB, N-Gram Vectors:  0.8448\n",
    "NB, CharLevel Vectors:  0.8108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.864\n",
      "LR, WordLevel TF-IDF:  0.8708\n",
      "LR, N-Gram Vectors:  0.8364\n",
      "LR, CharLevel Vectors:  0.8476\n"
     ]
    }
   ],
   "source": [
    "#3.2 Linear Classifier\n",
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR, Count Vectors:  0.8688\n",
    "LR, WordLevel TF-IDF:  0.872\n",
    "LR, N-Gram Vectors:  0.8364\n",
    "LR, CharLevel Vectors:  0.8448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. Model Building\n",
    "def train_model_SVM(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.8424\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model_SVM(svm.SVC(kernel='linear'), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"SVM, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8528\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "clf.fit(xtrain_count, train_y)\n",
    "y_pred = clf.predict(xvalid_count)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(valid_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM,WordLevel TF-IDF:  0.878\n",
      "SVM,CharLevel Vectors:  0.8556\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model_SVM(svm.SVC(kernel='linear'), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"SVM,WordLevel TF-IDF: \", accuracy)\n",
    "accuracy = train_model_SVM(svm.SVC(kernel='linear'), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"SVM,CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM, N-Gram Vectors:  0.8424\n",
    "SVM,WordLevel TF-IDF:  0.878\n",
    "SVM,CharLevel Vectors:  0.8556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. Model Building\n",
    "def train_model_SVC(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.84\n"
     ]
    }
   ],
   "source": [
    "#3.3 Implementing a SVM Model\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model_SVC(svm.SVC(C=100.0, gamma= 0.001), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"SVM, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM,Count Vectors:  0.856\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model_SVC(svm.SVC(C=100.0, gamma= 0.001), xtrain_count, train_y, xvalid_count)\n",
    "print (\"SVM,Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM,WordLevel TF-IDF:  0.8704\n",
      "SVM,CharLevel Vectors:  0.836\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model_SVC(svm.SVC(C=100.0, gamma= 0.001), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"SVM,WordLevel TF-IDF: \", accuracy)\n",
    "accuracy = train_model_SVC(svm.SVC(C=100.0, gamma= 0.001), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"SVM,CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM, N-Gram Vectors:  0.84\n",
    "SVM,Count Vectors:  0.856\n",
    "SVM,WordLevel TF-IDF:  0.8704\n",
    "SVM,CharLevel Vectors:  0.836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.8344\n",
      "RF, WordLevel TF-IDF:  0.8344\n"
     ]
    }
   ],
   "source": [
    "#3.4 Bagging Model\n",
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(n_estimators=100,criterion='entropy'), xtrain_count, train_y, xvalid_count)\n",
    "print (\"RF, Count Vectors: \", accuracy)\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(n_estimators=100,criterion='entropy'), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF, Count Vectors:  0.8464\n",
    "RF, WordLevel TF-IDF:  0.8344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.8584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, WordLevel TF-IDF:  0.8356\n",
      "Xgb, CharLevel Vectors:  0.8132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#3.5 Boosting Model\n",
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(max_depth=7,\n",
    "                           min_child_weight=1,\n",
    "                           learning_rate=0.1,\n",
    "                           n_estimators=500,\n",
    "                           silent=True,\n",
    "                           objective='binary:logistic',\n",
    "                           gamma=0,\n",
    "                           max_delta_step=0,\n",
    "                           subsample=1,\n",
    "                           colsample_bytree=1,\n",
    "                           colsample_bylevel=1,\n",
    "                           reg_alpha=0,\n",
    "                           reg_lambda=0,\n",
    "                           scale_pos_weight=1,\n",
    "                           seed=1,\n",
    "                           missing=None), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier( learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=4,\n",
    " min_child_weight=6,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " reg_alpha=0.005,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xgb, Count Vectors: 0.8584\n",
    "Xgb, WordLevel TF-IDF: 0.8356\n",
    "Xgb, CharLevel Vectors:  0.8152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. Model Building\n",
    "def train_model_new(classifier, feature_vector_train, label, feature_vector_valid, epoch, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label,epochs=epoch)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    #print(predictions.shape)\n",
    "    print(predictions[0:20])\n",
    "   \n",
    "    predictions = [1 if x >= 0.2 else 0 for x in predictions[:, 0]]\n",
    "    #if is_neural_net:\n",
    "        #predictions = predictions.argmax(axis=1)\n",
    "    \n",
    "    print(predictions[0:20])\n",
    "    print(valid_y[0:20])\n",
    "    #print(metrics.accuracy_score(valid_y,predictions))\n",
    "    return metrics.roc_auc_score(valid_y,predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7500/7500 [==============================] - 4s 532us/step - loss: 0.5257 - acc: 0.7780\n",
      "Epoch 2/20\n",
      "7500/7500 [==============================] - 4s 487us/step - loss: 0.2806 - acc: 0.8913\n",
      "Epoch 3/20\n",
      "7500/7500 [==============================] - 4s 499us/step - loss: 0.1967 - acc: 0.9304\n",
      "Epoch 4/20\n",
      "7500/7500 [==============================] - 4s 486us/step - loss: 0.1456 - acc: 0.9535\n",
      "Epoch 5/20\n",
      "7500/7500 [==============================] - 4s 504us/step - loss: 0.1077 - acc: 0.9699\n",
      "Epoch 6/20\n",
      "7500/7500 [==============================] - 4s 489us/step - loss: 0.0795 - acc: 0.9804\n",
      "Epoch 7/20\n",
      "7500/7500 [==============================] - 4s 530us/step - loss: 0.0576 - acc: 0.9904\n",
      "Epoch 8/20\n",
      "7500/7500 [==============================] - 4s 498us/step - loss: 0.0392 - acc: 0.9969\n",
      "Epoch 9/20\n",
      "7500/7500 [==============================] - 4s 472us/step - loss: 0.0281 - acc: 0.9984\n",
      "Epoch 10/20\n",
      "7500/7500 [==============================] - 4s 479us/step - loss: 0.0202 - acc: 0.9988\n",
      "Epoch 11/20\n",
      "7500/7500 [==============================] - 4s 487us/step - loss: 0.0151 - acc: 0.9989\n",
      "Epoch 12/20\n",
      "7500/7500 [==============================] - 4s 477us/step - loss: 0.0116 - acc: 0.9989\n",
      "Epoch 13/20\n",
      "7500/7500 [==============================] - 3s 459us/step - loss: 0.0093 - acc: 0.9991\n",
      "Epoch 14/20\n",
      "7500/7500 [==============================] - 4s 471us/step - loss: 0.0077 - acc: 0.9991\n",
      "Epoch 15/20\n",
      "7500/7500 [==============================] - 4s 469us/step - loss: 0.0067 - acc: 0.9991\n",
      "Epoch 16/20\n",
      "7500/7500 [==============================] - 4s 478us/step - loss: 0.0057 - acc: 0.9991\n",
      "Epoch 17/20\n",
      "7500/7500 [==============================] - 4s 490us/step - loss: 0.0051 - acc: 0.9991\n",
      "Epoch 18/20\n",
      "7500/7500 [==============================] - 4s 486us/step - loss: 0.0045 - acc: 0.9991\n",
      "Epoch 19/20\n",
      "7500/7500 [==============================] - 4s 485us/step - loss: 0.0041 - acc: 0.9991\n",
      "Epoch 20/20\n",
      "7500/7500 [==============================] - 4s 483us/step - loss: 0.0038 - acc: 0.9991\n",
      "[[9.62931454e-01]\n",
      " [5.93264995e-04]\n",
      " [9.36065987e-03]\n",
      " [9.99808729e-01]\n",
      " [1.01982643e-07]\n",
      " [9.98434961e-01]\n",
      " [6.62268937e-01]\n",
      " [1.00000000e+00]\n",
      " [1.11841795e-03]\n",
      " [1.39047825e-05]\n",
      " [4.15102541e-01]\n",
      " [9.04520750e-01]\n",
      " [3.02216470e-01]\n",
      " [6.15945783e-06]\n",
      " [7.67044330e-05]\n",
      " [8.46495037e-04]\n",
      " [1.80277407e-01]\n",
      " [3.00596294e-04]\n",
      " [6.40375912e-01]\n",
      " [3.56699165e-04]]\n",
      "[1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0]\n",
      "[1 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0]\n",
      "NN, Ngram Level TF IDF Vectors 0.8164254610655737\n"
     ]
    }
   ],
   "source": [
    "#3.6 Shallow Neural Networks\n",
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['acc'])\n",
    "    return classifier \n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy = train_model_new(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, 20, is_neural_net=True)\n",
    "print (\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/1\n",
    "7500/7500 [==============================] - 8s 1ms/step - loss: 0.5251\n",
    "NN, Ngram Level TF IDF Vectors 0.5016\n",
    "\n",
    "Epoch 1/20\n",
    "7500/7500 [==============================] - 5s 709us/step - loss: 0.5273 - acc: 0.7760\n",
    "Epoch 2/20\n",
    "7500/7500 [==============================] - 4s 470us/step - loss: 0.2785 - acc: 0.8925\n",
    "Epoch 3/20\n",
    "7500/7500 [==============================] - 4s 487us/step - loss: 0.1936 - acc: 0.9320\n",
    "Epoch 4/20\n",
    "7500/7500 [==============================] - 3s 390us/step - loss: 0.1423 - acc: 0.9543\n",
    "Epoch 5/20\n",
    "7500/7500 [==============================] - 3s 385us/step - loss: 0.1046 - acc: 0.9719\n",
    "Epoch 6/20\n",
    "7500/7500 [==============================] - 3s 382us/step - loss: 0.0752 - acc: 0.9845\n",
    "Epoch 7/20\n",
    "7500/7500 [==============================] - 3s 385us/step - loss: 0.0527 - acc: 0.9931\n",
    "Epoch 8/20\n",
    "7500/7500 [==============================] - 3s 382us/step - loss: 0.0368 - acc: 0.9968\n",
    "Epoch 9/20\n",
    "7500/7500 [==============================] - 3s 381us/step - loss: 0.0259 - acc: 0.9979\n",
    "Epoch 10/20\n",
    "7500/7500 [==============================] - 3s 447us/step - loss: 0.0188 - acc: 0.9987\n",
    "Epoch 11/20\n",
    "7500/7500 [==============================] - 3s 466us/step - loss: 0.0141 - acc: 0.9987\n",
    "Epoch 12/20\n",
    "7500/7500 [==============================] - 4s 517us/step - loss: 0.0111 - acc: 0.9988\n",
    "Epoch 13/20\n",
    "7500/7500 [==============================] - 5s 624us/step - loss: 0.0090 - acc: 0.9988\n",
    "Epoch 14/20\n",
    "7500/7500 [==============================] - 4s 472us/step - loss: 0.0075 - acc: 0.9988\n",
    "Epoch 15/20\n",
    "7500/7500 [==============================] - 4s 473us/step - loss: 0.0064 - acc: 0.9988\n",
    "Epoch 16/20\n",
    "7500/7500 [==============================] - 4s 474us/step - loss: 0.0056 - acc: 0.9988\n",
    "Epoch 17/20\n",
    "7500/7500 [==============================] - 4s 480us/step - loss: 0.0050 - acc: 0.9988\n",
    "Epoch 18/20\n",
    "7500/7500 [==============================] - 4s 476us/step - loss: 0.0045 - acc: 0.9987\n",
    "Epoch 19/20\n",
    "7500/7500 [==============================] - 4s 478us/step - loss: 0.0041 - acc: 0.9987\n",
    "Epoch 20/20\n",
    "7500/7500 [==============================] - 4s 478us/step - loss: 0.0038 - acc: 0.9987\n",
    "NN, Ngram Level TF IDF Vectors 0.8967110726366244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-55b5457c5416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_seq_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_seq_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_neural_net\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"CNN, Word Embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-55b5457c5416>\u001b[0m in \u001b[0;36mcreate_cnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Add the word embedding Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0membedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0membedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpatialDropout1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "#3.7 Deep Neural Networks\n",
    "#3.7.1 Convolutional Neural Network\n",
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "accuracy = train_model_new(classifier, train_seq_x, train_y, valid_seq_x, 20, is_neural_net=True)\n",
    "print (\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/1\n",
    "7500/7500 [==============================] - 17s 2ms/step - loss: 0.5841\n",
    "CNN, Word Embeddings 0.5016\n",
    "\n",
    "Epoch 1/20\n",
    "7500/7500 [==============================] - 14s 2ms/step - loss: 0.5549\n",
    "Epoch 2/20\n",
    "7500/7500 [==============================] - 12s 2ms/step - loss: 0.3796\n",
    "Epoch 3/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.3206\n",
    "Epoch 4/20\n",
    "7500/7500 [==============================] - 12s 2ms/step - loss: 0.2778\n",
    "Epoch 5/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.2384\n",
    "Epoch 6/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.1917\n",
    "Epoch 7/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.1575\n",
    "Epoch 8/20\n",
    "7500/7500 [==============================] - 14s 2ms/step - loss: 0.1238\n",
    "Epoch 9/20\n",
    "7500/7500 [==============================] - 14s 2ms/step - loss: 0.1015\n",
    "Epoch 10/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0889\n",
    "Epoch 11/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0688\n",
    "Epoch 12/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0716\n",
    "Epoch 13/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0612\n",
    "Epoch 14/20\n",
    "7500/7500 [==============================] - 14s 2ms/step - loss: 0.0563\n",
    "Epoch 15/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0523\n",
    "Epoch 16/20\n",
    "7500/7500 [==============================] - 14s 2ms/step - loss: 0.0483\n",
    "Epoch 17/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0537\n",
    "Epoch 18/20\n",
    "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0442\n",
    "Epoch 19/20\n",
    "7500/7500 [==============================] - 14s 2ms/step - loss: 0.0456\n",
    "Epoch 20/20\n",
    "7500/7500 [==============================] - 12s 2ms/step - loss: 0.0425\n",
    "(2500, 1)\n",
    "[7.8318059e-01 9.9925035e-01 6.1134610e-04 9.9998665e-01 9.9999905e-01\n",
    " 9.9962318e-01 9.9859709e-01 9.9917930e-01 7.0752937e-01 9.9864143e-01\n",
    " 9.6448255e-01 1.1390647e-04 9.6624172e-01 5.3794348e-01 9.9877435e-01\n",
    " 6.1611372e-01 9.9974352e-01 1.3516644e-06 8.7371105e-01 7.7482120e-08]\n",
    "[1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 0]\n",
    "CNN, Word Embeddings 0.9373635358053192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7500/7500 [==============================] - 30s 4ms/step - loss: 0.5878 - acc: 0.6832\n",
      "Epoch 2/20\n",
      "7500/7500 [==============================] - 33s 4ms/step - loss: 0.5014 - acc: 0.7619\n",
      "Epoch 3/20\n",
      "7500/7500 [==============================] - 32s 4ms/step - loss: 0.4734 - acc: 0.7832\n",
      "Epoch 4/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.4569 - acc: 0.7856\n",
      "Epoch 5/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.4377 - acc: 0.7977\n",
      "Epoch 6/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.4178 - acc: 0.8145\n",
      "Epoch 7/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.3922 - acc: 0.8229\n",
      "Epoch 8/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.3734 - acc: 0.8315\n",
      "Epoch 9/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.3548 - acc: 0.8428\n",
      "Epoch 10/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.3319 - acc: 0.8535\n",
      "Epoch 11/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.3195 - acc: 0.8592\n",
      "Epoch 12/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.3079 - acc: 0.8683\n",
      "Epoch 13/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.2907 - acc: 0.8735\n",
      "Epoch 14/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.2714 - acc: 0.8859\n",
      "Epoch 15/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.2528 - acc: 0.8923\n",
      "Epoch 16/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.2457 - acc: 0.8949\n",
      "Epoch 17/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.2277 - acc: 0.9029\n",
      "Epoch 18/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.2167 - acc: 0.9093\n",
      "Epoch 19/20\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.2068 - acc: 0.9111\n",
      "Epoch 20/20\n",
      "7500/7500 [==============================] - 28s 4ms/step - loss: 0.1888 - acc: 0.9235\n",
      "(2500, 1)\n",
      "[[5.6749523e-02]\n",
      " [8.3308637e-01]\n",
      " [9.9879092e-01]\n",
      " [9.9999177e-01]\n",
      " [2.0139949e-02]\n",
      " [3.8266245e-02]\n",
      " [5.1640469e-04]\n",
      " [5.4006672e-01]\n",
      " [4.0028162e-02]\n",
      " [3.0131629e-01]\n",
      " [4.0146457e-05]\n",
      " [9.9299735e-01]\n",
      " [9.9577868e-01]\n",
      " [2.2251075e-02]\n",
      " [9.9274325e-01]\n",
      " [9.9896109e-01]\n",
      " [9.9785280e-01]\n",
      " [9.9829048e-01]\n",
      " [2.9750664e-03]\n",
      " [5.2361798e-01]]\n",
      "[0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1]\n",
      "[0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0]\n",
      "RNN-LSTM, Word Embeddings 0.8376\n"
     ]
    }
   ],
   "source": [
    "#3.7.2 Recurrent Neural Network â€“ LSTM\n",
    "# Add an Input Layer\n",
    "def create_rnn_lstm():\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "accuracy = train_model_new(classifier, train_seq_x, train_y, valid_seq_x, 20, is_neural_net=True)\n",
    "print (\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/20\n",
    "7500/7500 [==============================] - 30s 4ms/step - loss: 0.5878 - acc: 0.6832\n",
    "Epoch 2/20\n",
    "7500/7500 [==============================] - 33s 4ms/step - loss: 0.5014 - acc: 0.7619\n",
    "Epoch 3/20\n",
    "7500/7500 [==============================] - 32s 4ms/step - loss: 0.4734 - acc: 0.7832\n",
    "Epoch 4/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.4569 - acc: 0.7856\n",
    "Epoch 5/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.4377 - acc: 0.7977\n",
    "Epoch 6/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.4178 - acc: 0.8145\n",
    "Epoch 7/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.3922 - acc: 0.8229\n",
    "Epoch 8/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.3734 - acc: 0.8315\n",
    "Epoch 9/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.3548 - acc: 0.8428\n",
    "Epoch 10/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.3319 - acc: 0.8535\n",
    "Epoch 11/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.3195 - acc: 0.8592\n",
    "Epoch 12/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.3079 - acc: 0.8683\n",
    "Epoch 13/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.2907 - acc: 0.8735\n",
    "Epoch 14/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.2714 - acc: 0.8859\n",
    "Epoch 15/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.2528 - acc: 0.8923\n",
    "Epoch 16/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.2457 - acc: 0.8949\n",
    "Epoch 17/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.2277 - acc: 0.9029\n",
    "Epoch 18/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.2167 - acc: 0.9093\n",
    "Epoch 19/20\n",
    "7500/7500 [==============================] - 27s 4ms/step - loss: 0.2068 - acc: 0.9111\n",
    "Epoch 20/20\n",
    "7500/7500 [==============================] - 28s 4ms/step - loss: 0.1888 - acc: 0.9235\n",
    "(2500, 1)\n",
    "[[5.6749523e-02]\n",
    " [8.3308637e-01]\n",
    " [9.9879092e-01]\n",
    " [9.9999177e-01]\n",
    " [2.0139949e-02]\n",
    " [3.8266245e-02]\n",
    " [5.1640469e-04]\n",
    " [5.4006672e-01]\n",
    " [4.0028162e-02]\n",
    " [3.0131629e-01]\n",
    " [4.0146457e-05]\n",
    " [9.9299735e-01]\n",
    " [9.9577868e-01]\n",
    " [2.2251075e-02]\n",
    " [9.9274325e-01]\n",
    " [9.9896109e-01]\n",
    " [9.9785280e-01]\n",
    " [9.9829048e-01]\n",
    " [2.9750664e-03]\n",
    " [5.2361798e-01]]\n",
    "[0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1]\n",
    "[0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0]\n",
    "RNN-LSTM, Word Embeddings 0.8376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 23s 3ms/step - loss: 0.6119\n",
      "RNN-GRU, Word Embeddings 0.5\n"
     ]
    }
   ],
   "source": [
    "#3.7.3 Recurrent Neural Network â€“ GRU\n",
    "def create_rnn_gru():\n",
    "        # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_gru()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"RNN-GRU, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/1\n",
    "7500/7500 [==============================] - 93s 12ms/step - loss: 0.6193\n",
    "RNN-GRU, Word Embeddings 0.5016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 40s 5ms/step - loss: 0.5891\n",
      "RNN-Bidirectional, Word Embeddings 0.5\n"
     ]
    }
   ],
   "source": [
    "#3.7.4 Bidirectional RNN\n",
    "def create_bidirectional_rnn():\n",
    "        # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_bidirectional_rnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"RNN-Bidirectional, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/1\n",
    "7500/7500 [==============================] - 63s 8ms/step - loss: 0.6233\n",
    "RNN-Bidirectional, Word Embeddings 0.5016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 15s 2ms/step - loss: 0.5856\n",
      "CNN, Word Embeddings 0.5\n"
     ]
    }
   ],
   "source": [
    "#3.7.5 Recurrent Convolutional Neural Network\n",
    "def create_rcnn():\n",
    "        # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rcnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/1\n",
    "7500/7500 [==============================] - 16s 2ms/step - loss: 0.5710\n",
    "CNN, Word Embeddings 0.5016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

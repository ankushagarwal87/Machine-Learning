{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. Dataset preparation3. Model Building\n",
    "# load the dataset\n",
    "data = open('data/corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.1 Count Vectors as features\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.2 TF-IDF Vectors as features\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.3 Word Embeddings\n",
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.0897      0.016      -0.0571     ...  0.1559     -0.0254\n",
      "  -0.0259    ]\n",
      " [-0.0314      0.0149     -0.0205     ...  0.098       0.0893\n",
      "   0.0148    ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.0897     -0.0682      0.0726     ...  0.206       0.0839\n",
      "   0.1557    ]\n",
      " [-0.0857      0.0475      0.14489999 ...  0.0586      0.11\n",
      "   0.0743    ]]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.4 Text / NLP based features\n",
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.5 Topic Models as features\n",
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. Model Building\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.8368\n",
      "NB, WordLevel TF-IDF:  0.8364\n",
      "NB, N-Gram Vectors:  0.8284\n",
      "NB, CharLevel Vectors:  0.802\n"
     ]
    }
   ],
   "source": [
    "#3.1 Naive Bayes\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-166b17a58bdd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-166b17a58bdd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    NB, Count Vectors:  0.8488\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "NB, Count Vectors:  0.8488\n",
    "NB, WordLevel TF-IDF:  0.8524\n",
    "NB, N-Gram Vectors:  0.8448\n",
    "NB, CharLevel Vectors:  0.8108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.8504\n",
      "LR, WordLevel TF-IDF:  0.8516\n",
      "LR, N-Gram Vectors:  0.8284\n",
      "LR, CharLevel Vectors:  0.8264\n"
     ]
    }
   ],
   "source": [
    "#3.2 Linear Classifier\n",
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR, Count Vectors:  0.8688\n",
    "LR, WordLevel TF-IDF:  0.872\n",
    "LR, N-Gram Vectors:  0.8364\n",
    "LR, CharLevel Vectors:  0.8448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.5144\n"
     ]
    }
   ],
   "source": [
    "#3.3 Implementing a SVM Model\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"SVM, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM,Count Vectors:  0.6908\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"SVM,Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM,WordLevel TF-IDF:  0.5144\n",
      "SVM,CharLevel Vectors:  0.5144\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"SVM,WordLevel TF-IDF: \", accuracy)\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"SVM,CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM, N-Gram Vectors:  0.5016\n",
    "SVM,Count Vectors:  0.6908\n",
    "SVM,WordLevel TF-IDF:  0.5144\n",
    "SVM,CharLevel Vectors:  0.5144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.7292\n",
      "RF, WordLevel TF-IDF:  0.7668\n"
     ]
    }
   ],
   "source": [
    "#3.4 Bagging Model\n",
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"RF, Count Vectors: \", accuracy)\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF, Count Vectors:  0.7592\n",
    "RF, WordLevel TF-IDF:  0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.7996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, WordLevel TF-IDF:  0.7948\n",
      "Xgb, CharLevel Vectors:  0.8024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#3.5 Boosting Model\n",
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xgb, Count Vectors:  0.8004\n",
    "Xgb, WordLevel TF-IDF:  0.7972\n",
    "Xgb, CharLevel Vectors:  0.8152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Model Building\n",
    "def train_model_new(classifier, feature_vector_train, label, feature_vector_valid, epoch, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label,epochs=epoch)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    print(predictions.shape)\n",
    "    print(predictions[0:20])\n",
    "   \n",
    "    predictions = [1 if x >= 0.2 else 0 for x in predictions[:, 0]]\n",
    "    #if is_neural_net:\n",
    "        #predictions = predictions.argmax(axis=1)\n",
    "    \n",
    "    print(predictions[0:20])\n",
    "    print(valid_y[0:20])\n",
    "    #print(\"test auc:\", metrics.roc_auc_score(valid_y,predictions))\n",
    "    return metrics.roc_auc_score(valid_y,predictions) #metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7500/7500 [==============================] - 5s 643us/step - loss: 0.5229 - acc: 0.7864\n",
      "Epoch 2/20\n",
      "7500/7500 [==============================] - 3s 461us/step - loss: 0.2760 - acc: 0.8963\n",
      "Epoch 3/20\n",
      "7500/7500 [==============================] - 3s 463us/step - loss: 0.1925 - acc: 0.9324\n",
      "Epoch 4/20\n",
      "7500/7500 [==============================] - 3s 466us/step - loss: 0.1417 - acc: 0.9536\n",
      "Epoch 5/20\n",
      "7500/7500 [==============================] - 4s 467us/step - loss: 0.1041 - acc: 0.9711\n",
      "Epoch 6/20\n",
      "7500/7500 [==============================] - 4s 473us/step - loss: 0.0733 - acc: 0.9837\n",
      "Epoch 7/20\n",
      "7500/7500 [==============================] - 4s 473us/step - loss: 0.0516 - acc: 0.9929\n",
      "Epoch 8/20\n",
      "7500/7500 [==============================] - 4s 472us/step - loss: 0.0364 - acc: 0.9969\n",
      "Epoch 9/20\n",
      "7500/7500 [==============================] - 4s 468us/step - loss: 0.0254 - acc: 0.9981\n",
      "Epoch 10/20\n",
      "7500/7500 [==============================] - 4s 476us/step - loss: 0.0181 - acc: 0.9985\n",
      "Epoch 11/20\n",
      "7500/7500 [==============================] - 4s 490us/step - loss: 0.0137 - acc: 0.9987\n",
      "Epoch 12/20\n",
      "7500/7500 [==============================] - 4s 483us/step - loss: 0.0107 - acc: 0.9988\n",
      "Epoch 13/20\n",
      "7500/7500 [==============================] - 4s 490us/step - loss: 0.0087 - acc: 0.9988\n",
      "Epoch 14/20\n",
      "7500/7500 [==============================] - 4s 483us/step - loss: 0.0074 - acc: 0.9988\n",
      "Epoch 15/20\n",
      "7500/7500 [==============================] - 4s 495us/step - loss: 0.0063 - acc: 0.9988\n",
      "Epoch 16/20\n",
      "7500/7500 [==============================] - 4s 484us/step - loss: 0.0055 - acc: 0.9988\n",
      "Epoch 17/20\n",
      "7500/7500 [==============================] - 4s 483us/step - loss: 0.0049 - acc: 0.9988\n",
      "Epoch 18/20\n",
      "7500/7500 [==============================] - 4s 524us/step - loss: 0.0045 - acc: 0.9988\n",
      "Epoch 19/20\n",
      "7500/7500 [==============================] - 4s 492us/step - loss: 0.0041 - acc: 0.9988\n",
      "Epoch 20/20\n",
      "7500/7500 [==============================] - 4s 491us/step - loss: 0.0038 - acc: 0.9987\n",
      "(2500, 1)\n",
      "[[9.9999559e-01]\n",
      " [9.9994469e-01]\n",
      " [1.8518373e-08]\n",
      " [9.9960500e-01]\n",
      " [1.0000000e+00]\n",
      " [1.5213782e-01]\n",
      " [9.9817085e-01]\n",
      " [9.9994373e-01]\n",
      " [4.9368147e-04]\n",
      " [9.9550545e-01]\n",
      " [9.9999940e-01]\n",
      " [6.4435807e-07]\n",
      " [9.9999964e-01]\n",
      " [3.2644314e-09]\n",
      " [9.9999952e-01]\n",
      " [5.4405012e-07]\n",
      " [6.3147211e-01]\n",
      " [6.5051464e-10]\n",
      " [9.0418273e-01]\n",
      " [3.4284156e-10]]\n",
      "[1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "[1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 0]\n",
      "NN, Ngram Level TF IDF Vectors 0.8126725340258742\n"
     ]
    }
   ],
   "source": [
    "#3.6 Shallow Neural Networks\n",
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['acc'])\n",
    "    return classifier \n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy = train_model_new(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, 20, is_neural_net=True)\n",
    "print (\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/1\n",
    "7500/7500 [==============================] - 8s 1ms/step - loss: 0.5251\n",
    "NN, Ngram Level TF IDF Vectors 0.5016\n",
    "\n",
    "Epoch 1/20\n",
    "7500/7500 [==============================] - 5s 709us/step - loss: 0.5273 - acc: 0.7760\n",
    "Epoch 2/20\n",
    "7500/7500 [==============================] - 4s 470us/step - loss: 0.2785 - acc: 0.8925\n",
    "Epoch 3/20\n",
    "7500/7500 [==============================] - 4s 487us/step - loss: 0.1936 - acc: 0.9320\n",
    "Epoch 4/20\n",
    "7500/7500 [==============================] - 3s 390us/step - loss: 0.1423 - acc: 0.9543\n",
    "Epoch 5/20\n",
    "7500/7500 [==============================] - 3s 385us/step - loss: 0.1046 - acc: 0.9719\n",
    "Epoch 6/20\n",
    "7500/7500 [==============================] - 3s 382us/step - loss: 0.0752 - acc: 0.9845\n",
    "Epoch 7/20\n",
    "7500/7500 [==============================] - 3s 385us/step - loss: 0.0527 - acc: 0.9931\n",
    "Epoch 8/20\n",
    "7500/7500 [==============================] - 3s 382us/step - loss: 0.0368 - acc: 0.9968\n",
    "Epoch 9/20\n",
    "7500/7500 [==============================] - 3s 381us/step - loss: 0.0259 - acc: 0.9979\n",
    "Epoch 10/20\n",
    "7500/7500 [==============================] - 3s 447us/step - loss: 0.0188 - acc: 0.9987\n",
    "Epoch 11/20\n",
    "7500/7500 [==============================] - 3s 466us/step - loss: 0.0141 - acc: 0.9987\n",
    "Epoch 12/20\n",
    "7500/7500 [==============================] - 4s 517us/step - loss: 0.0111 - acc: 0.9988\n",
    "Epoch 13/20\n",
    "7500/7500 [==============================] - 5s 624us/step - loss: 0.0090 - acc: 0.9988\n",
    "Epoch 14/20\n",
    "7500/7500 [==============================] - 4s 472us/step - loss: 0.0075 - acc: 0.9988\n",
    "Epoch 15/20\n",
    "7500/7500 [==============================] - 4s 473us/step - loss: 0.0064 - acc: 0.9988\n",
    "Epoch 16/20\n",
    "7500/7500 [==============================] - 4s 474us/step - loss: 0.0056 - acc: 0.9988\n",
    "Epoch 17/20\n",
    "7500/7500 [==============================] - 4s 480us/step - loss: 0.0050 - acc: 0.9988\n",
    "Epoch 18/20\n",
    "7500/7500 [==============================] - 4s 476us/step - loss: 0.0045 - acc: 0.9987\n",
    "Epoch 19/20\n",
    "7500/7500 [==============================] - 4s 478us/step - loss: 0.0041 - acc: 0.9987\n",
    "Epoch 20/20\n",
    "7500/7500 [==============================] - 4s 478us/step - loss: 0.0038 - acc: 0.9987\n",
    "NN, Ngram Level TF IDF Vectors 0.8967110726366244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Epoch 1/20\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.5549\n",
      "Epoch 2/20\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.3796\n",
      "Epoch 3/20\n",
      "7500/7500 [==============================] - 13s 2ms/step - loss: 0.3206\n",
      "Epoch 4/20\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.2778\n",
      "Epoch 5/20\n",
      "7500/7500 [==============================] - 13s 2ms/step - loss: 0.2384\n",
      "Epoch 6/20\n",
      "7500/7500 [==============================] - 13s 2ms/step - loss: 0.1917\n",
      "Epoch 7/20\n",
      "7500/7500 [==============================] - 13s 2ms/step - loss: 0.1575\n",
      "Epoch 8/20\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.1238\n",
      "Epoch 9/20\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.1015\n",
      "Epoch 10/20\n",
      "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0889\n",
      "Epoch 11/20\n",
      "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0688\n",
      "Epoch 12/20\n",
      "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0716\n",
      "Epoch 13/20\n",
      "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0612\n",
      "Epoch 14/20\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.0563\n",
      "Epoch 15/20\n",
      "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0523\n",
      "Epoch 16/20\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.0483\n",
      "Epoch 17/20\n",
      "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0537\n",
      "Epoch 18/20\n",
      "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0442\n",
      "Epoch 19/20\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.0456\n",
      "Epoch 20/20\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.0425\n",
      "(2500, 1)\n",
      "[7.8318059e-01 9.9925035e-01 6.1134610e-04 9.9998665e-01 9.9999905e-01\n",
      " 9.9962318e-01 9.9859709e-01 9.9917930e-01 7.0752937e-01 9.9864143e-01\n",
      " 9.6448255e-01 1.1390647e-04 9.6624172e-01 5.3794348e-01 9.9877435e-01\n",
      " 6.1611372e-01 9.9974352e-01 1.3516644e-06 8.7371105e-01 7.7482120e-08]\n",
      "[1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 0]\n",
      "CNN, Word Embeddings 0.9373635358053192\n"
     ]
    }
   ],
   "source": [
    "#3.7 Deep Neural Networks\n",
    "#3.7.1 Convolutional Neural Network\n",
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "accuracy = train_model_new(classifier, train_seq_x, train_y, valid_seq_x, 20, is_neural_net=True)\n",
    "print (\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/1\n",
    "7500/7500 [==============================] - 17s 2ms/step - loss: 0.5841\n",
    "CNN, Word Embeddings 0.5016\n",
    "\n",
    "Epoch 1/20\n",
    "7500/7500 [==============================] - 14s 2ms/step - loss: 0.5549\n",
    "Epoch 2/20\n",
    "7500/7500 [==============================] - 12s 2ms/step - loss: 0.3796\n",
    "Epoch 3/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.3206\n",
    "Epoch 4/20\n",
    "7500/7500 [==============================] - 12s 2ms/step - loss: 0.2778\n",
    "Epoch 5/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.2384\n",
    "Epoch 6/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.1917\n",
    "Epoch 7/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.1575\n",
    "Epoch 8/20\n",
    "7500/7500 [==============================] - 14s 2ms/step - loss: 0.1238\n",
    "Epoch 9/20\n",
    "7500/7500 [==============================] - 14s 2ms/step - loss: 0.1015\n",
    "Epoch 10/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0889\n",
    "Epoch 11/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0688\n",
    "Epoch 12/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0716\n",
    "Epoch 13/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0612\n",
    "Epoch 14/20\n",
    "7500/7500 [==============================] - 14s 2ms/step - loss: 0.0563\n",
    "Epoch 15/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0523\n",
    "Epoch 16/20\n",
    "7500/7500 [==============================] - 14s 2ms/step - loss: 0.0483\n",
    "Epoch 17/20\n",
    "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0537\n",
    "Epoch 18/20\n",
    "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0442\n",
    "Epoch 19/20\n",
    "7500/7500 [==============================] - 14s 2ms/step - loss: 0.0456\n",
    "Epoch 20/20\n",
    "7500/7500 [==============================] - 12s 2ms/step - loss: 0.0425\n",
    "(2500, 1)\n",
    "[7.8318059e-01 9.9925035e-01 6.1134610e-04 9.9998665e-01 9.9999905e-01\n",
    " 9.9962318e-01 9.9859709e-01 9.9917930e-01 7.0752937e-01 9.9864143e-01\n",
    " 9.6448255e-01 1.1390647e-04 9.6624172e-01 5.3794348e-01 9.9877435e-01\n",
    " 6.1611372e-01 9.9974352e-01 1.3516644e-06 8.7371105e-01 7.7482120e-08]\n",
    "[1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 0]\n",
    "CNN, Word Embeddings 0.9373635358053192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 78s 10ms/step - loss: 0.6023\n",
      "RNN-LSTM, Word Embeddings 0.5144\n"
     ]
    }
   ],
   "source": [
    "#3.7.2 Recurrent Neural Network – LSTM\n",
    "# Add an Input Layer\n",
    "def create_rnn_lstm():\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "accuracy = train_model_new(classifier, train_seq_x, train_y, valid_seq_x, 20, is_neural_net=True)\n",
    "print (\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/1\n",
    "7500/7500 [==============================] - 42s 6ms/step - loss: 0.5914\n",
    "RNN-LSTM, Word Embeddings 0.5016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 54s 7ms/step - loss: 0.5964\n",
      "RNN-GRU, Word Embeddings 0.5144\n"
     ]
    }
   ],
   "source": [
    "#3.7.3 Recurrent Neural Network – GRU\n",
    "def create_rnn_gru():\n",
    "        # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_gru()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"RNN-GRU, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/1\n",
    "7500/7500 [==============================] - 93s 12ms/step - loss: 0.6193\n",
    "RNN-GRU, Word Embeddings 0.5016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 49s 7ms/step - loss: 0.5892\n",
      "RNN-Bidirectional, Word Embeddings 0.5144\n"
     ]
    }
   ],
   "source": [
    "#3.7.4 Bidirectional RNN\n",
    "def create_bidirectional_rnn():\n",
    "        # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_bidirectional_rnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"RNN-Bidirectional, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/1\n",
    "7500/7500 [==============================] - 63s 8ms/step - loss: 0.6233\n",
    "RNN-Bidirectional, Word Embeddings 0.5016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 15s 2ms/step - loss: 0.5750\n",
      "CNN, Word Embeddings 0.5144\n"
     ]
    }
   ],
   "source": [
    "#3.7.5 Recurrent Convolutional Neural Network\n",
    "def create_rcnn():\n",
    "        # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rcnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoch 1/1\n",
    "7500/7500 [==============================] - 16s 2ms/step - loss: 0.5710\n",
    "CNN, Word Embeddings 0.5016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
